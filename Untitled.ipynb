{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=data/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=data/rt-polarity.pos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"data/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"data/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 2973  y: 2973\n",
      "TIME\n",
      "PERSON\n",
      "LOCATION\n",
      "MEASURE\n",
      "DEFINITION\n",
      "ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "def correct_class(t, c):\n",
    "    if c == 'COUNT':\n",
    "        return 'MEASURE'\n",
    "    if c == 'D' or c == 'DEFINITION':\n",
    "        return 'DEFINITION'\n",
    "    if c == 'F' or c == 'FACTOID':\n",
    "        if t == 'COUNT':\n",
    "            return 'MEASURE'\n",
    "        else:\n",
    "            return t\n",
    "    if c == 'L' or c == 'LIST':\n",
    "        if t == 'COUNT':\n",
    "            return 'MEASURE'\n",
    "        else:\n",
    "            return t\n",
    "    if c == 'LOCATION':\n",
    "        return 'LOCATION'\n",
    "    if c == 'MEASURE':\n",
    "        return 'MEASURE'\n",
    "    if c == 'OBJECT':\n",
    "        return 'DEFINITION'\n",
    "    if c == 'ORGANIZATION':\n",
    "        return 'ORGANIZATION'\n",
    "    if c == 'OTHER' and (t == 'FACTOID' or t == 'LIST'):\n",
    "        return 'OTHER'\n",
    "    if c == 'OTHER' and not (t == 'FACTOID' or t == 'LIST'):\n",
    "        return t\n",
    "    if c == 'PERSON' and t == 'DEFINITION':\n",
    "        return 'DEFINITION'\n",
    "    if c == 'PERSON' and not t == 'DEFINITION':\n",
    "        return 'PERSON'\n",
    "    if c == 'TIME':\n",
    "        return 'TIME'\n",
    "    return c\n",
    "\n",
    "def pre_processing(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    ret = text.replace('\\n', '').replace('\\r', '')\n",
    "    ret = ret.replace('?', '').replace('.', '').replace(',', '')\n",
    "    ret = ret.replace(':', '').replace(';', '')\n",
    "    ret = ret.replace('\\'', '').replace('\\\"', '')\n",
    "    ret = ret.replace(u'«', '').replace(u'»', '')\n",
    "    ret = ret.lower()\n",
    "    \n",
    "    return ret\n",
    "\n",
    "import xml.etree.ElementTree as et\n",
    "import qa_system\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "path_questions = \"data/questions.xml\"\n",
    "STOPWORDS = False\n",
    "\n",
    "if STOPWORDS:\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "X, y = [], []\n",
    "tree = et.parse(open(path_questions, 'r'))\n",
    "for question in tree.getroot():\n",
    "    \n",
    "    t = question.attrib['tipo']\n",
    "    c = question.attrib['categoria']\n",
    "    clss = correct_class(t, c)\n",
    "    \n",
    "    if clss == None:\n",
    "        continue\n",
    "    if clss == 'X':\n",
    "        continue\n",
    "    if clss == 'MANNER':\n",
    "        continue\n",
    "    if clss == 'OBJECT':\n",
    "        continue\n",
    "    if clss == 'OTHER':\n",
    "        continue\n",
    "\n",
    "    for t in question:\n",
    "        if t.tag == 'texto':\n",
    "            text = pre_processing(t.text)\n",
    "            if text is not None:\n",
    "                if type(text) == type(''): text = text.decode('utf-8')\n",
    "                X.append(text)\n",
    "                y.append(clss)\n",
    "                break\n",
    "                \n",
    "print 'X:',len(X),' y:',len(y)\n",
    "types = []\n",
    "for i in y:\n",
    "    if i not in types:\n",
    "        types.append(i)\n",
    "        print i\n",
    "ret = []\n",
    "for i in y:\n",
    "    a = [0, 0, 0, 0, 0, 0]\n",
    "    a[types.index(i)] = 1\n",
    "    ret.append(a)\n",
    "x_text = X\n",
    "y = np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 5011\n",
      "Train/Dev split: 2676/297\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087\n",
      "\n",
      "2017-09-26T14:14:49.182801: step 1, loss 4.86473, acc 0.265625\n",
      "2017-09-26T14:14:49.426067: step 2, loss 5.33441, acc 0.1875\n",
      "2017-09-26T14:14:49.681681: step 3, loss 3.66094, acc 0.171875\n",
      "2017-09-26T14:14:49.919460: step 4, loss 3.17204, acc 0.21875\n",
      "2017-09-26T14:14:50.166965: step 5, loss 3.51057, acc 0.15625\n",
      "2017-09-26T14:14:50.404730: step 6, loss 3.41182, acc 0.1875\n",
      "2017-09-26T14:14:50.642976: step 7, loss 3.81638, acc 0.1875\n",
      "2017-09-26T14:14:50.881823: step 8, loss 3.66386, acc 0.125\n",
      "2017-09-26T14:14:51.140511: step 9, loss 3.54023, acc 0.234375\n",
      "2017-09-26T14:14:51.447530: step 10, loss 2.45857, acc 0.328125\n",
      "2017-09-26T14:14:51.769475: step 11, loss 3.09146, acc 0.28125\n",
      "2017-09-26T14:14:52.052459: step 12, loss 3.05885, acc 0.25\n",
      "2017-09-26T14:14:52.292252: step 13, loss 2.35709, acc 0.328125\n",
      "2017-09-26T14:14:52.541699: step 14, loss 2.7727, acc 0.34375\n",
      "2017-09-26T14:14:52.797860: step 15, loss 2.92245, acc 0.28125\n",
      "2017-09-26T14:14:53.034798: step 16, loss 2.37648, acc 0.5\n",
      "2017-09-26T14:14:53.276461: step 17, loss 3.33942, acc 0.25\n",
      "2017-09-26T14:14:53.553707: step 18, loss 2.91255, acc 0.359375\n",
      "2017-09-26T14:14:53.794735: step 19, loss 2.37734, acc 0.40625\n",
      "2017-09-26T14:14:54.036881: step 20, loss 2.32169, acc 0.3125\n",
      "2017-09-26T14:14:54.274012: step 21, loss 2.49694, acc 0.421875\n",
      "2017-09-26T14:14:54.524284: step 22, loss 2.55543, acc 0.296875\n",
      "2017-09-26T14:14:54.768571: step 23, loss 2.48506, acc 0.359375\n",
      "2017-09-26T14:14:55.014108: step 24, loss 1.72644, acc 0.484375\n",
      "2017-09-26T14:14:55.259473: step 25, loss 1.7858, acc 0.515625\n",
      "2017-09-26T14:14:55.502235: step 26, loss 1.79546, acc 0.5\n",
      "2017-09-26T14:14:55.749162: step 27, loss 1.59412, acc 0.5625\n",
      "2017-09-26T14:14:56.008737: step 28, loss 2.32148, acc 0.421875\n",
      "2017-09-26T14:14:56.265730: step 29, loss 2.80494, acc 0.328125\n",
      "2017-09-26T14:14:56.530020: step 30, loss 1.74414, acc 0.4375\n",
      "2017-09-26T14:14:56.772439: step 31, loss 1.98133, acc 0.46875\n",
      "2017-09-26T14:14:57.013189: step 32, loss 1.87695, acc 0.546875\n",
      "2017-09-26T14:14:57.254375: step 33, loss 1.92861, acc 0.46875\n",
      "2017-09-26T14:14:57.489749: step 34, loss 2.19117, acc 0.515625\n",
      "2017-09-26T14:14:57.763191: step 35, loss 1.66052, acc 0.546875\n",
      "2017-09-26T14:14:58.005589: step 36, loss 2.0104, acc 0.453125\n",
      "2017-09-26T14:14:58.248504: step 37, loss 1.96591, acc 0.53125\n",
      "2017-09-26T14:14:58.492031: step 38, loss 1.61189, acc 0.578125\n",
      "2017-09-26T14:14:58.806730: step 39, loss 1.89933, acc 0.453125\n",
      "2017-09-26T14:14:59.112877: step 40, loss 1.44084, acc 0.5625\n",
      "2017-09-26T14:14:59.419922: step 41, loss 1.59595, acc 0.53125\n",
      "2017-09-26T14:14:59.645401: step 42, loss 1.69201, acc 0.576923\n",
      "2017-09-26T14:14:59.932237: step 43, loss 1.80461, acc 0.515625\n",
      "2017-09-26T14:15:00.215478: step 44, loss 1.23394, acc 0.625\n",
      "2017-09-26T14:15:00.524561: step 45, loss 1.45078, acc 0.53125\n",
      "2017-09-26T14:15:00.837236: step 46, loss 1.26407, acc 0.609375\n",
      "2017-09-26T14:15:01.133815: step 47, loss 1.59016, acc 0.578125\n",
      "2017-09-26T14:15:01.397426: step 48, loss 1.72784, acc 0.515625\n",
      "2017-09-26T14:15:01.653106: step 49, loss 1.76466, acc 0.65625\n",
      "2017-09-26T14:15:01.907717: step 50, loss 1.33019, acc 0.5625\n",
      "2017-09-26T14:15:02.167336: step 51, loss 1.27652, acc 0.609375\n",
      "2017-09-26T14:15:02.431049: step 52, loss 1.37495, acc 0.515625\n",
      "2017-09-26T14:15:02.687469: step 53, loss 1.86095, acc 0.484375\n",
      "2017-09-26T14:15:02.957355: step 54, loss 1.41221, acc 0.5625\n",
      "2017-09-26T14:15:03.207415: step 55, loss 1.56483, acc 0.53125\n",
      "2017-09-26T14:15:03.448022: step 56, loss 1.12018, acc 0.625\n",
      "2017-09-26T14:15:03.693619: step 57, loss 0.956043, acc 0.765625\n",
      "2017-09-26T14:15:03.939621: step 58, loss 1.41061, acc 0.5625\n",
      "2017-09-26T14:15:04.189490: step 59, loss 1.52811, acc 0.546875\n",
      "2017-09-26T14:15:04.441607: step 60, loss 1.31631, acc 0.65625\n",
      "2017-09-26T14:15:04.688309: step 61, loss 1.37075, acc 0.59375\n",
      "2017-09-26T14:15:04.928573: step 62, loss 1.17483, acc 0.640625\n",
      "2017-09-26T14:15:05.195376: step 63, loss 1.13768, acc 0.609375\n",
      "2017-09-26T14:15:05.441294: step 64, loss 1.23048, acc 0.59375\n",
      "2017-09-26T14:15:05.704290: step 65, loss 1.06338, acc 0.640625\n",
      "2017-09-26T14:15:05.958256: step 66, loss 1.1744, acc 0.640625\n",
      "2017-09-26T14:15:06.217445: step 67, loss 1.22782, acc 0.65625\n",
      "2017-09-26T14:15:06.463137: step 68, loss 1.44631, acc 0.625\n",
      "2017-09-26T14:15:06.716902: step 69, loss 1.14042, acc 0.65625\n",
      "2017-09-26T14:15:06.955479: step 70, loss 0.998434, acc 0.671875\n",
      "2017-09-26T14:15:07.226739: step 71, loss 1.15494, acc 0.6875\n",
      "2017-09-26T14:15:07.486415: step 72, loss 0.89318, acc 0.71875\n",
      "2017-09-26T14:15:07.733365: step 73, loss 0.82274, acc 0.703125\n",
      "2017-09-26T14:15:08.014167: step 74, loss 1.2283, acc 0.5625\n",
      "2017-09-26T14:15:08.292931: step 75, loss 1.8305, acc 0.515625\n",
      "2017-09-26T14:15:08.572353: step 76, loss 0.839655, acc 0.6875\n",
      "2017-09-26T14:15:08.857046: step 77, loss 1.29718, acc 0.6875\n",
      "2017-09-26T14:15:09.118974: step 78, loss 1.05763, acc 0.71875\n",
      "2017-09-26T14:15:09.382065: step 79, loss 0.953027, acc 0.625\n",
      "2017-09-26T14:15:09.631603: step 80, loss 0.85139, acc 0.765625\n",
      "2017-09-26T14:15:09.914616: step 81, loss 1.16042, acc 0.625\n",
      "2017-09-26T14:15:10.222770: step 82, loss 0.890156, acc 0.71875\n",
      "2017-09-26T14:15:10.522119: step 83, loss 0.948003, acc 0.671875\n",
      "2017-09-26T14:15:10.729864: step 84, loss 1.44295, acc 0.653846\n",
      "2017-09-26T14:15:10.998673: step 85, loss 0.594594, acc 0.8125\n",
      "2017-09-26T14:15:11.242609: step 86, loss 1.10487, acc 0.734375\n",
      "2017-09-26T14:15:11.485796: step 87, loss 0.848721, acc 0.75\n",
      "2017-09-26T14:15:11.724150: step 88, loss 0.963125, acc 0.6875\n",
      "2017-09-26T14:15:11.968792: step 89, loss 0.705229, acc 0.765625\n",
      "2017-09-26T14:15:12.252857: step 90, loss 0.8671, acc 0.6875\n",
      "2017-09-26T14:15:12.495034: step 91, loss 0.847554, acc 0.78125\n",
      "2017-09-26T14:15:12.750262: step 92, loss 0.676773, acc 0.828125\n",
      "2017-09-26T14:15:13.025754: step 93, loss 0.66246, acc 0.75\n",
      "2017-09-26T14:15:13.264772: step 94, loss 0.993738, acc 0.65625\n",
      "2017-09-26T14:15:13.514304: step 95, loss 1.01999, acc 0.640625\n",
      "2017-09-26T14:15:13.771024: step 96, loss 0.818778, acc 0.71875\n",
      "2017-09-26T14:15:14.023465: step 97, loss 0.944777, acc 0.6875\n",
      "2017-09-26T14:15:14.269114: step 98, loss 0.893648, acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:15:14.515222: step 99, loss 0.681021, acc 0.78125\n",
      "2017-09-26T14:15:14.760744: step 100, loss 0.546302, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:15:15.083865: step 100, loss 0.546874, acc 0.811448\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-100\n",
      "\n",
      "2017-09-26T14:15:15.659288: step 101, loss 0.733213, acc 0.71875\n",
      "2017-09-26T14:15:15.909376: step 102, loss 0.732655, acc 0.78125\n",
      "2017-09-26T14:15:16.161182: step 103, loss 1.06559, acc 0.734375\n",
      "2017-09-26T14:15:16.467942: step 104, loss 0.967589, acc 0.65625\n",
      "2017-09-26T14:15:16.728975: step 105, loss 1.04565, acc 0.6875\n",
      "2017-09-26T14:15:16.978255: step 106, loss 0.705659, acc 0.78125\n",
      "2017-09-26T14:15:17.213479: step 107, loss 0.824609, acc 0.78125\n",
      "2017-09-26T14:15:17.444997: step 108, loss 0.692981, acc 0.765625\n",
      "2017-09-26T14:15:17.688300: step 109, loss 1.05895, acc 0.78125\n",
      "2017-09-26T14:15:17.921184: step 110, loss 0.368904, acc 0.84375\n",
      "2017-09-26T14:15:18.151733: step 111, loss 0.991526, acc 0.625\n",
      "2017-09-26T14:15:18.422471: step 112, loss 1.1831, acc 0.65625\n",
      "2017-09-26T14:15:18.722641: step 113, loss 1.12368, acc 0.625\n",
      "2017-09-26T14:15:19.020360: step 114, loss 1.11366, acc 0.671875\n",
      "2017-09-26T14:15:19.343656: step 115, loss 0.905724, acc 0.65625\n",
      "2017-09-26T14:15:19.649758: step 116, loss 0.82777, acc 0.765625\n",
      "2017-09-26T14:15:19.901682: step 117, loss 0.95793, acc 0.6875\n",
      "2017-09-26T14:15:20.162862: step 118, loss 1.05056, acc 0.734375\n",
      "2017-09-26T14:15:20.426804: step 119, loss 0.303419, acc 0.90625\n",
      "2017-09-26T14:15:20.694684: step 120, loss 0.607663, acc 0.765625\n",
      "2017-09-26T14:15:20.972470: step 121, loss 0.60039, acc 0.828125\n",
      "2017-09-26T14:15:21.228763: step 122, loss 0.727195, acc 0.796875\n",
      "2017-09-26T14:15:21.514673: step 123, loss 0.56035, acc 0.828125\n",
      "2017-09-26T14:15:21.761402: step 124, loss 0.89276, acc 0.796875\n",
      "2017-09-26T14:15:22.017790: step 125, loss 0.760561, acc 0.734375\n",
      "2017-09-26T14:15:22.234075: step 126, loss 1.12533, acc 0.653846\n",
      "2017-09-26T14:15:22.484033: step 127, loss 0.438125, acc 0.859375\n",
      "2017-09-26T14:15:22.735724: step 128, loss 0.548131, acc 0.796875\n",
      "2017-09-26T14:15:23.020047: step 129, loss 0.564687, acc 0.78125\n",
      "2017-09-26T14:15:23.297514: step 130, loss 0.663158, acc 0.78125\n",
      "2017-09-26T14:15:23.573364: step 131, loss 0.581198, acc 0.828125\n",
      "2017-09-26T14:15:23.799444: step 132, loss 0.488375, acc 0.859375\n",
      "2017-09-26T14:15:24.033579: step 133, loss 0.768453, acc 0.734375\n",
      "2017-09-26T14:15:24.282192: step 134, loss 0.569696, acc 0.828125\n",
      "2017-09-26T14:15:24.517309: step 135, loss 1.13679, acc 0.609375\n",
      "2017-09-26T14:15:24.769831: step 136, loss 0.475215, acc 0.78125\n",
      "2017-09-26T14:15:25.000660: step 137, loss 0.460447, acc 0.84375\n",
      "2017-09-26T14:15:25.255287: step 138, loss 0.560773, acc 0.8125\n",
      "2017-09-26T14:15:25.502931: step 139, loss 0.538556, acc 0.765625\n",
      "2017-09-26T14:15:25.734350: step 140, loss 0.661266, acc 0.78125\n",
      "2017-09-26T14:15:25.968015: step 141, loss 0.460574, acc 0.828125\n",
      "2017-09-26T14:15:26.198384: step 142, loss 0.725747, acc 0.765625\n",
      "2017-09-26T14:15:26.432499: step 143, loss 0.849768, acc 0.765625\n",
      "2017-09-26T14:15:26.667708: step 144, loss 0.354632, acc 0.859375\n",
      "2017-09-26T14:15:26.920379: step 145, loss 0.730174, acc 0.734375\n",
      "2017-09-26T14:15:27.167121: step 146, loss 0.499693, acc 0.859375\n",
      "2017-09-26T14:15:27.414122: step 147, loss 0.422476, acc 0.875\n",
      "2017-09-26T14:15:27.681745: step 148, loss 0.560324, acc 0.875\n",
      "2017-09-26T14:15:27.912212: step 149, loss 0.478843, acc 0.828125\n",
      "2017-09-26T14:15:28.150925: step 150, loss 0.464953, acc 0.8125\n",
      "2017-09-26T14:15:28.382702: step 151, loss 0.557151, acc 0.84375\n",
      "2017-09-26T14:15:28.629360: step 152, loss 0.644104, acc 0.71875\n",
      "2017-09-26T14:15:28.950837: step 153, loss 0.51915, acc 0.796875\n",
      "2017-09-26T14:15:29.236737: step 154, loss 0.422348, acc 0.84375\n",
      "2017-09-26T14:15:29.491742: step 155, loss 0.652511, acc 0.78125\n",
      "2017-09-26T14:15:29.730668: step 156, loss 0.781384, acc 0.75\n",
      "2017-09-26T14:15:29.998353: step 157, loss 0.883425, acc 0.796875\n",
      "2017-09-26T14:15:30.280624: step 158, loss 0.512091, acc 0.765625\n",
      "2017-09-26T14:15:30.548372: step 159, loss 0.554602, acc 0.828125\n",
      "2017-09-26T14:15:30.835092: step 160, loss 0.314203, acc 0.859375\n",
      "2017-09-26T14:15:31.067255: step 161, loss 0.630027, acc 0.796875\n",
      "2017-09-26T14:15:31.305035: step 162, loss 0.626815, acc 0.828125\n",
      "2017-09-26T14:15:31.539204: step 163, loss 0.876622, acc 0.75\n",
      "2017-09-26T14:15:31.782988: step 164, loss 0.91507, acc 0.71875\n",
      "2017-09-26T14:15:32.022560: step 165, loss 0.551044, acc 0.84375\n",
      "2017-09-26T14:15:32.259610: step 166, loss 0.828626, acc 0.71875\n",
      "2017-09-26T14:15:32.501167: step 167, loss 0.53536, acc 0.84375\n",
      "2017-09-26T14:15:32.709944: step 168, loss 0.353581, acc 0.846154\n",
      "2017-09-26T14:15:32.954701: step 169, loss 0.4939, acc 0.875\n",
      "2017-09-26T14:15:33.194016: step 170, loss 0.566798, acc 0.859375\n",
      "2017-09-26T14:15:33.435526: step 171, loss 0.287789, acc 0.890625\n",
      "2017-09-26T14:15:33.712328: step 172, loss 0.55516, acc 0.78125\n",
      "2017-09-26T14:15:33.967493: step 173, loss 0.606791, acc 0.828125\n",
      "2017-09-26T14:15:34.222605: step 174, loss 0.62689, acc 0.828125\n",
      "2017-09-26T14:15:34.481930: step 175, loss 0.270535, acc 0.953125\n",
      "2017-09-26T14:15:34.740612: step 176, loss 0.636107, acc 0.8125\n",
      "2017-09-26T14:15:34.972927: step 177, loss 0.481025, acc 0.859375\n",
      "2017-09-26T14:15:35.205989: step 178, loss 0.569509, acc 0.78125\n",
      "2017-09-26T14:15:35.446518: step 179, loss 0.452222, acc 0.859375\n",
      "2017-09-26T14:15:35.680203: step 180, loss 0.255014, acc 0.875\n",
      "2017-09-26T14:15:35.933857: step 181, loss 0.384547, acc 0.90625\n",
      "2017-09-26T14:15:36.173222: step 182, loss 0.334888, acc 0.875\n",
      "2017-09-26T14:15:36.435715: step 183, loss 0.620275, acc 0.8125\n",
      "2017-09-26T14:15:36.670507: step 184, loss 0.405036, acc 0.875\n",
      "2017-09-26T14:15:36.920721: step 185, loss 0.587657, acc 0.78125\n",
      "2017-09-26T14:15:37.169146: step 186, loss 0.529168, acc 0.765625\n",
      "2017-09-26T14:15:37.422786: step 187, loss 0.515915, acc 0.8125\n",
      "2017-09-26T14:15:37.666942: step 188, loss 0.425988, acc 0.859375\n",
      "2017-09-26T14:15:37.932570: step 189, loss 0.413571, acc 0.875\n",
      "2017-09-26T14:15:38.217160: step 190, loss 0.537101, acc 0.796875\n",
      "2017-09-26T14:15:38.456066: step 191, loss 0.658987, acc 0.765625\n",
      "2017-09-26T14:15:38.695452: step 192, loss 0.412102, acc 0.859375\n",
      "2017-09-26T14:15:38.934951: step 193, loss 0.325386, acc 0.890625\n",
      "2017-09-26T14:15:39.199547: step 194, loss 0.778772, acc 0.765625\n",
      "2017-09-26T14:15:39.509602: step 195, loss 0.430049, acc 0.828125\n",
      "2017-09-26T14:15:39.841173: step 196, loss 0.610639, acc 0.875\n",
      "2017-09-26T14:15:40.148899: step 197, loss 0.564087, acc 0.828125\n",
      "2017-09-26T14:15:40.464948: step 198, loss 0.393725, acc 0.875\n",
      "2017-09-26T14:15:40.745096: step 199, loss 0.417859, acc 0.859375\n",
      "2017-09-26T14:15:41.050381: step 200, loss 0.350108, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:15:41.327123: step 200, loss 0.445105, acc 0.841751\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-200\n",
      "\n",
      "2017-09-26T14:15:41.916094: step 201, loss 0.582173, acc 0.75\n",
      "2017-09-26T14:15:42.178580: step 202, loss 0.336454, acc 0.890625\n",
      "2017-09-26T14:15:42.453919: step 203, loss 0.646828, acc 0.8125\n",
      "2017-09-26T14:15:42.729046: step 204, loss 0.635318, acc 0.859375\n",
      "2017-09-26T14:15:43.010724: step 205, loss 0.610808, acc 0.8125\n",
      "2017-09-26T14:15:43.310157: step 206, loss 0.324154, acc 0.90625\n",
      "2017-09-26T14:15:43.579738: step 207, loss 0.483216, acc 0.84375\n",
      "2017-09-26T14:15:43.817116: step 208, loss 0.360926, acc 0.875\n",
      "2017-09-26T14:15:44.064487: step 209, loss 0.395975, acc 0.859375\n",
      "2017-09-26T14:15:44.289470: step 210, loss 0.243122, acc 0.903846\n",
      "2017-09-26T14:15:44.565336: step 211, loss 0.447662, acc 0.90625\n",
      "2017-09-26T14:15:44.830673: step 212, loss 0.416151, acc 0.859375\n",
      "2017-09-26T14:15:45.107107: step 213, loss 0.311594, acc 0.84375\n",
      "2017-09-26T14:15:45.366387: step 214, loss 0.679034, acc 0.734375\n",
      "2017-09-26T14:15:45.611539: step 215, loss 0.329503, acc 0.890625\n",
      "2017-09-26T14:15:45.854717: step 216, loss 0.370321, acc 0.890625\n",
      "2017-09-26T14:15:46.093667: step 217, loss 0.432989, acc 0.890625\n",
      "2017-09-26T14:15:46.381022: step 218, loss 0.287723, acc 0.953125\n",
      "2017-09-26T14:15:46.640638: step 219, loss 0.688965, acc 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:15:46.888428: step 220, loss 0.385371, acc 0.890625\n",
      "2017-09-26T14:15:47.139315: step 221, loss 0.363012, acc 0.859375\n",
      "2017-09-26T14:15:47.371218: step 222, loss 0.298872, acc 0.921875\n",
      "2017-09-26T14:15:47.608064: step 223, loss 0.292495, acc 0.890625\n",
      "2017-09-26T14:15:47.849644: step 224, loss 0.249553, acc 0.890625\n",
      "2017-09-26T14:15:48.092405: step 225, loss 0.327636, acc 0.859375\n",
      "2017-09-26T14:15:48.351032: step 226, loss 0.501224, acc 0.859375\n",
      "2017-09-26T14:15:48.596141: step 227, loss 0.361091, acc 0.859375\n",
      "2017-09-26T14:15:48.835055: step 228, loss 0.344155, acc 0.90625\n",
      "2017-09-26T14:15:49.085633: step 229, loss 0.413305, acc 0.859375\n",
      "2017-09-26T14:15:49.330151: step 230, loss 0.231818, acc 0.90625\n",
      "2017-09-26T14:15:49.571807: step 231, loss 0.356455, acc 0.84375\n",
      "2017-09-26T14:15:49.816746: step 232, loss 0.519426, acc 0.859375\n",
      "2017-09-26T14:15:50.050942: step 233, loss 0.221793, acc 0.9375\n",
      "2017-09-26T14:15:50.301923: step 234, loss 0.318384, acc 0.9375\n",
      "2017-09-26T14:15:50.545706: step 235, loss 0.572783, acc 0.84375\n",
      "2017-09-26T14:15:50.793667: step 236, loss 0.290575, acc 0.921875\n",
      "2017-09-26T14:15:51.036602: step 237, loss 0.25085, acc 0.90625\n",
      "2017-09-26T14:15:51.272327: step 238, loss 0.411217, acc 0.890625\n",
      "2017-09-26T14:15:51.524098: step 239, loss 0.284931, acc 0.9375\n",
      "2017-09-26T14:15:51.776714: step 240, loss 0.334783, acc 0.875\n",
      "2017-09-26T14:15:52.014168: step 241, loss 0.227009, acc 0.90625\n",
      "2017-09-26T14:15:52.266903: step 242, loss 0.448779, acc 0.8125\n",
      "2017-09-26T14:15:52.514248: step 243, loss 0.400335, acc 0.890625\n",
      "2017-09-26T14:15:52.752221: step 244, loss 0.414067, acc 0.875\n",
      "2017-09-26T14:15:52.995128: step 245, loss 0.428469, acc 0.84375\n",
      "2017-09-26T14:15:53.236175: step 246, loss 0.280326, acc 0.890625\n",
      "2017-09-26T14:15:53.496014: step 247, loss 0.187298, acc 0.953125\n",
      "2017-09-26T14:15:53.728643: step 248, loss 0.238728, acc 0.890625\n",
      "2017-09-26T14:15:53.974919: step 249, loss 0.453777, acc 0.859375\n",
      "2017-09-26T14:15:54.212411: step 250, loss 0.416019, acc 0.859375\n",
      "2017-09-26T14:15:54.493313: step 251, loss 0.366566, acc 0.84375\n",
      "2017-09-26T14:15:54.724162: step 252, loss 0.223173, acc 0.903846\n",
      "2017-09-26T14:15:54.967189: step 253, loss 0.205671, acc 0.96875\n",
      "2017-09-26T14:15:55.217995: step 254, loss 0.386274, acc 0.859375\n",
      "2017-09-26T14:15:55.475559: step 255, loss 0.354042, acc 0.90625\n",
      "2017-09-26T14:15:55.748229: step 256, loss 0.302606, acc 0.875\n",
      "2017-09-26T14:15:56.022276: step 257, loss 0.17744, acc 0.9375\n",
      "2017-09-26T14:15:56.281884: step 258, loss 0.439954, acc 0.859375\n",
      "2017-09-26T14:15:56.561091: step 259, loss 0.313616, acc 0.90625\n",
      "2017-09-26T14:15:56.822212: step 260, loss 0.295425, acc 0.859375\n",
      "2017-09-26T14:15:57.076011: step 261, loss 0.215829, acc 0.96875\n",
      "2017-09-26T14:15:57.334557: step 262, loss 0.394073, acc 0.90625\n",
      "2017-09-26T14:15:57.594746: step 263, loss 0.169222, acc 0.9375\n",
      "2017-09-26T14:15:57.871187: step 264, loss 0.250139, acc 0.890625\n",
      "2017-09-26T14:15:58.119369: step 265, loss 0.235315, acc 0.921875\n",
      "2017-09-26T14:15:58.362944: step 266, loss 0.173123, acc 0.953125\n",
      "2017-09-26T14:15:58.613921: step 267, loss 0.326307, acc 0.859375\n",
      "2017-09-26T14:15:58.858569: step 268, loss 0.313714, acc 0.875\n",
      "2017-09-26T14:15:59.090948: step 269, loss 0.390276, acc 0.890625\n",
      "2017-09-26T14:15:59.335466: step 270, loss 0.411469, acc 0.890625\n",
      "2017-09-26T14:15:59.635405: step 271, loss 0.255725, acc 0.921875\n",
      "2017-09-26T14:15:59.902278: step 272, loss 0.344742, acc 0.90625\n",
      "2017-09-26T14:16:00.159936: step 273, loss 0.403271, acc 0.875\n",
      "2017-09-26T14:16:00.418106: step 274, loss 0.270194, acc 0.921875\n",
      "2017-09-26T14:16:00.678625: step 275, loss 0.366468, acc 0.828125\n",
      "2017-09-26T14:16:00.958100: step 276, loss 0.105196, acc 0.984375\n",
      "2017-09-26T14:16:01.223370: step 277, loss 0.412898, acc 0.84375\n",
      "2017-09-26T14:16:01.484370: step 278, loss 0.523025, acc 0.890625\n",
      "2017-09-26T14:16:01.727155: step 279, loss 0.559077, acc 0.84375\n",
      "2017-09-26T14:16:01.975094: step 280, loss 0.15071, acc 0.96875\n",
      "2017-09-26T14:16:02.229351: step 281, loss 0.359724, acc 0.84375\n",
      "2017-09-26T14:16:02.470540: step 282, loss 0.368894, acc 0.890625\n",
      "2017-09-26T14:16:02.726115: step 283, loss 0.224006, acc 0.90625\n",
      "2017-09-26T14:16:03.006943: step 284, loss 0.213344, acc 0.890625\n",
      "2017-09-26T14:16:03.307759: step 285, loss 0.556104, acc 0.875\n",
      "2017-09-26T14:16:03.572309: step 286, loss 0.34314, acc 0.921875\n",
      "2017-09-26T14:16:03.845500: step 287, loss 0.281742, acc 0.875\n",
      "2017-09-26T14:16:04.095161: step 288, loss 0.216858, acc 0.921875\n",
      "2017-09-26T14:16:04.384278: step 289, loss 0.208623, acc 0.890625\n",
      "2017-09-26T14:16:04.629018: step 290, loss 0.176233, acc 0.953125\n",
      "2017-09-26T14:16:04.867215: step 291, loss 0.325013, acc 0.875\n",
      "2017-09-26T14:16:05.123842: step 292, loss 0.15431, acc 0.9375\n",
      "2017-09-26T14:16:05.363143: step 293, loss 0.291609, acc 0.9375\n",
      "2017-09-26T14:16:05.576634: step 294, loss 0.104771, acc 0.961538\n",
      "2017-09-26T14:16:05.831684: step 295, loss 0.177699, acc 0.9375\n",
      "2017-09-26T14:16:06.103228: step 296, loss 0.209102, acc 0.9375\n",
      "2017-09-26T14:16:06.362496: step 297, loss 0.188207, acc 0.953125\n",
      "2017-09-26T14:16:06.616484: step 298, loss 0.134253, acc 0.9375\n",
      "2017-09-26T14:16:06.878979: step 299, loss 0.200681, acc 0.9375\n",
      "2017-09-26T14:16:07.129302: step 300, loss 0.175752, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:16:07.396243: step 300, loss 0.443131, acc 0.868687\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-300\n",
      "\n",
      "2017-09-26T14:16:08.101526: step 301, loss 0.165757, acc 0.953125\n",
      "2017-09-26T14:16:08.349605: step 302, loss 0.233564, acc 0.90625\n",
      "2017-09-26T14:16:08.628667: step 303, loss 0.143205, acc 0.953125\n",
      "2017-09-26T14:16:08.942826: step 304, loss 0.335944, acc 0.90625\n",
      "2017-09-26T14:16:09.202792: step 305, loss 0.411242, acc 0.875\n",
      "2017-09-26T14:16:09.463863: step 306, loss 0.446926, acc 0.890625\n",
      "2017-09-26T14:16:09.724859: step 307, loss 0.321711, acc 0.84375\n",
      "2017-09-26T14:16:09.973144: step 308, loss 0.221953, acc 0.921875\n",
      "2017-09-26T14:16:10.263762: step 309, loss 0.192599, acc 0.953125\n",
      "2017-09-26T14:16:10.500182: step 310, loss 0.168541, acc 0.9375\n",
      "2017-09-26T14:16:10.761308: step 311, loss 0.331692, acc 0.875\n",
      "2017-09-26T14:16:10.991631: step 312, loss 0.277208, acc 0.890625\n",
      "2017-09-26T14:16:11.235009: step 313, loss 0.182834, acc 0.921875\n",
      "2017-09-26T14:16:11.491155: step 314, loss 0.257729, acc 0.875\n",
      "2017-09-26T14:16:11.755842: step 315, loss 0.192253, acc 0.921875\n",
      "2017-09-26T14:16:12.056574: step 316, loss 0.232724, acc 0.890625\n",
      "2017-09-26T14:16:12.321544: step 317, loss 0.359411, acc 0.890625\n",
      "2017-09-26T14:16:12.590428: step 318, loss 0.243257, acc 0.90625\n",
      "2017-09-26T14:16:12.888771: step 319, loss 0.263506, acc 0.921875\n",
      "2017-09-26T14:16:13.155693: step 320, loss 0.143931, acc 0.9375\n",
      "2017-09-26T14:16:13.427677: step 321, loss 0.374103, acc 0.84375\n",
      "2017-09-26T14:16:13.671610: step 322, loss 0.380682, acc 0.875\n",
      "2017-09-26T14:16:13.923961: step 323, loss 0.268322, acc 0.875\n",
      "2017-09-26T14:16:14.167749: step 324, loss 0.192028, acc 0.9375\n",
      "2017-09-26T14:16:14.408008: step 325, loss 0.198298, acc 0.9375\n",
      "2017-09-26T14:16:14.664826: step 326, loss 0.322536, acc 0.890625\n",
      "2017-09-26T14:16:14.925697: step 327, loss 0.122068, acc 0.96875\n",
      "2017-09-26T14:16:15.194734: step 328, loss 0.388125, acc 0.859375\n",
      "2017-09-26T14:16:15.548075: step 329, loss 0.237067, acc 0.9375\n",
      "2017-09-26T14:16:15.830936: step 330, loss 0.164109, acc 0.9375\n",
      "2017-09-26T14:16:16.140249: step 331, loss 0.160923, acc 0.9375\n",
      "2017-09-26T14:16:16.433539: step 332, loss 0.15776, acc 0.90625\n",
      "2017-09-26T14:16:16.752685: step 333, loss 0.142842, acc 0.9375\n",
      "2017-09-26T14:16:17.024137: step 334, loss 0.284903, acc 0.921875\n",
      "2017-09-26T14:16:17.293345: step 335, loss 0.187173, acc 0.921875\n",
      "2017-09-26T14:16:17.535843: step 336, loss 0.316274, acc 0.903846\n",
      "2017-09-26T14:16:17.829767: step 337, loss 0.358724, acc 0.890625\n",
      "2017-09-26T14:16:18.091514: step 338, loss 0.265286, acc 0.890625\n",
      "2017-09-26T14:16:18.382744: step 339, loss 0.25804, acc 0.921875\n",
      "2017-09-26T14:16:18.664319: step 340, loss 0.250852, acc 0.953125\n",
      "2017-09-26T14:16:18.972497: step 341, loss 0.224438, acc 0.953125\n",
      "2017-09-26T14:16:19.245668: step 342, loss 0.243971, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:16:19.524104: step 343, loss 0.129791, acc 0.984375\n",
      "2017-09-26T14:16:19.878026: step 344, loss 0.268322, acc 0.9375\n",
      "2017-09-26T14:16:20.161238: step 345, loss 0.369288, acc 0.875\n",
      "2017-09-26T14:16:20.442864: step 346, loss 0.261567, acc 0.921875\n",
      "2017-09-26T14:16:20.712604: step 347, loss 0.199824, acc 0.921875\n",
      "2017-09-26T14:16:20.981745: step 348, loss 0.185444, acc 0.9375\n",
      "2017-09-26T14:16:21.218289: step 349, loss 0.145868, acc 0.953125\n",
      "2017-09-26T14:16:21.451224: step 350, loss 0.131047, acc 0.96875\n",
      "2017-09-26T14:16:21.689177: step 351, loss 0.201317, acc 0.921875\n",
      "2017-09-26T14:16:21.921951: step 352, loss 0.148008, acc 0.96875\n",
      "2017-09-26T14:16:22.167525: step 353, loss 0.324459, acc 0.90625\n",
      "2017-09-26T14:16:22.418078: step 354, loss 0.311312, acc 0.859375\n",
      "2017-09-26T14:16:22.677975: step 355, loss 0.157296, acc 0.9375\n",
      "2017-09-26T14:16:22.937331: step 356, loss 0.143107, acc 0.9375\n",
      "2017-09-26T14:16:23.186981: step 357, loss 0.104015, acc 0.984375\n",
      "2017-09-26T14:16:23.434734: step 358, loss 0.28153, acc 0.90625\n",
      "2017-09-26T14:16:23.695847: step 359, loss 0.224145, acc 0.9375\n",
      "2017-09-26T14:16:23.978675: step 360, loss 0.109739, acc 0.96875\n",
      "2017-09-26T14:16:24.243114: step 361, loss 0.426388, acc 0.890625\n",
      "2017-09-26T14:16:24.527151: step 362, loss 0.175828, acc 0.96875\n",
      "2017-09-26T14:16:24.811980: step 363, loss 0.141743, acc 0.96875\n",
      "2017-09-26T14:16:25.087934: step 364, loss 0.199012, acc 0.9375\n",
      "2017-09-26T14:16:25.364084: step 365, loss 0.241952, acc 0.90625\n",
      "2017-09-26T14:16:25.628726: step 366, loss 0.109847, acc 0.984375\n",
      "2017-09-26T14:16:25.896727: step 367, loss 0.385825, acc 0.875\n",
      "2017-09-26T14:16:26.167592: step 368, loss 0.0924542, acc 0.96875\n",
      "2017-09-26T14:16:26.411037: step 369, loss 0.280477, acc 0.90625\n",
      "2017-09-26T14:16:26.682605: step 370, loss 0.139261, acc 0.9375\n",
      "2017-09-26T14:16:26.951688: step 371, loss 0.133614, acc 0.96875\n",
      "2017-09-26T14:16:27.218625: step 372, loss 0.170188, acc 0.953125\n",
      "2017-09-26T14:16:27.478534: step 373, loss 0.213936, acc 0.921875\n",
      "2017-09-26T14:16:27.756289: step 374, loss 0.31264, acc 0.859375\n",
      "2017-09-26T14:16:28.051570: step 375, loss 0.167826, acc 0.96875\n",
      "2017-09-26T14:16:28.336318: step 376, loss 0.221185, acc 0.953125\n",
      "2017-09-26T14:16:28.646232: step 377, loss 0.202218, acc 0.953125\n",
      "2017-09-26T14:16:28.892473: step 378, loss 0.266579, acc 0.903846\n",
      "2017-09-26T14:16:29.152668: step 379, loss 0.151824, acc 0.984375\n",
      "2017-09-26T14:16:29.429202: step 380, loss 0.167133, acc 0.96875\n",
      "2017-09-26T14:16:29.696259: step 381, loss 0.185845, acc 0.90625\n",
      "2017-09-26T14:16:29.967530: step 382, loss 0.408814, acc 0.859375\n",
      "2017-09-26T14:16:30.251532: step 383, loss 0.205252, acc 0.921875\n",
      "2017-09-26T14:16:30.527166: step 384, loss 0.244909, acc 0.921875\n",
      "2017-09-26T14:16:30.818012: step 385, loss 0.125642, acc 0.953125\n",
      "2017-09-26T14:16:31.095752: step 386, loss 0.0962657, acc 0.984375\n",
      "2017-09-26T14:16:31.364658: step 387, loss 0.180997, acc 0.9375\n",
      "2017-09-26T14:16:31.662125: step 388, loss 0.096668, acc 0.984375\n",
      "2017-09-26T14:16:31.945438: step 389, loss 0.128337, acc 0.984375\n",
      "2017-09-26T14:16:32.222489: step 390, loss 0.0538293, acc 0.984375\n",
      "2017-09-26T14:16:32.504039: step 391, loss 0.192889, acc 0.921875\n",
      "2017-09-26T14:16:32.792353: step 392, loss 0.105612, acc 0.96875\n",
      "2017-09-26T14:16:33.080630: step 393, loss 0.245728, acc 0.921875\n",
      "2017-09-26T14:16:33.420295: step 394, loss 0.365841, acc 0.90625\n",
      "2017-09-26T14:16:33.742358: step 395, loss 0.1605, acc 0.984375\n",
      "2017-09-26T14:16:34.035055: step 396, loss 0.0866148, acc 0.984375\n",
      "2017-09-26T14:16:34.317820: step 397, loss 0.10765, acc 0.953125\n",
      "2017-09-26T14:16:34.604934: step 398, loss 0.265461, acc 0.953125\n",
      "2017-09-26T14:16:34.888834: step 399, loss 0.189428, acc 0.921875\n",
      "2017-09-26T14:16:35.171533: step 400, loss 0.128161, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:16:35.447435: step 400, loss 0.40265, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-400\n",
      "\n",
      "2017-09-26T14:16:36.126518: step 401, loss 0.22327, acc 0.921875\n",
      "2017-09-26T14:16:36.389030: step 402, loss 0.184225, acc 0.921875\n",
      "2017-09-26T14:16:36.652070: step 403, loss 0.100058, acc 0.96875\n",
      "2017-09-26T14:16:36.900141: step 404, loss 0.340643, acc 0.90625\n",
      "2017-09-26T14:16:37.146436: step 405, loss 0.296002, acc 0.90625\n",
      "2017-09-26T14:16:37.386259: step 406, loss 0.215387, acc 0.9375\n",
      "2017-09-26T14:16:37.628856: step 407, loss 0.108737, acc 0.9375\n",
      "2017-09-26T14:16:37.871490: step 408, loss 0.0897008, acc 0.96875\n",
      "2017-09-26T14:16:38.109095: step 409, loss 0.111283, acc 0.96875\n",
      "2017-09-26T14:16:38.374607: step 410, loss 0.335856, acc 0.921875\n",
      "2017-09-26T14:16:38.665197: step 411, loss 0.0790298, acc 0.984375\n",
      "2017-09-26T14:16:38.922539: step 412, loss 0.138847, acc 0.953125\n",
      "2017-09-26T14:16:39.215140: step 413, loss 0.265397, acc 0.90625\n",
      "2017-09-26T14:16:39.477232: step 414, loss 0.3511, acc 0.90625\n",
      "2017-09-26T14:16:39.771543: step 415, loss 0.117046, acc 0.9375\n",
      "2017-09-26T14:16:40.074693: step 416, loss 0.171223, acc 0.9375\n",
      "2017-09-26T14:16:40.381734: step 417, loss 0.118637, acc 0.953125\n",
      "2017-09-26T14:16:40.679445: step 418, loss 0.189233, acc 0.953125\n",
      "2017-09-26T14:16:40.983176: step 419, loss 0.184611, acc 0.9375\n",
      "2017-09-26T14:16:41.237600: step 420, loss 0.184007, acc 0.903846\n",
      "2017-09-26T14:16:41.529754: step 421, loss 0.0388813, acc 1\n",
      "2017-09-26T14:16:41.840813: step 422, loss 0.202598, acc 0.953125\n",
      "2017-09-26T14:16:42.128602: step 423, loss 0.0888386, acc 1\n",
      "2017-09-26T14:16:42.420187: step 424, loss 0.148037, acc 0.953125\n",
      "2017-09-26T14:16:42.726556: step 425, loss 0.0927632, acc 0.953125\n",
      "2017-09-26T14:16:43.058678: step 426, loss 0.128039, acc 0.953125\n",
      "2017-09-26T14:16:43.366825: step 427, loss 0.27898, acc 0.921875\n",
      "2017-09-26T14:16:43.672278: step 428, loss 0.164552, acc 0.953125\n",
      "2017-09-26T14:16:43.952547: step 429, loss 0.19944, acc 0.921875\n",
      "2017-09-26T14:16:44.260741: step 430, loss 0.124644, acc 0.96875\n",
      "2017-09-26T14:16:44.553207: step 431, loss 0.303158, acc 0.890625\n",
      "2017-09-26T14:16:44.848236: step 432, loss 0.107291, acc 0.96875\n",
      "2017-09-26T14:16:45.155717: step 433, loss 0.195487, acc 0.953125\n",
      "2017-09-26T14:16:45.476604: step 434, loss 0.144146, acc 0.953125\n",
      "2017-09-26T14:16:45.787401: step 435, loss 0.159014, acc 0.921875\n",
      "2017-09-26T14:16:46.087126: step 436, loss 0.125397, acc 0.96875\n",
      "2017-09-26T14:16:46.368816: step 437, loss 0.117316, acc 0.984375\n",
      "2017-09-26T14:16:46.658490: step 438, loss 0.139184, acc 0.953125\n",
      "2017-09-26T14:16:46.970544: step 439, loss 0.267523, acc 0.953125\n",
      "2017-09-26T14:16:47.268001: step 440, loss 0.0622413, acc 1\n",
      "2017-09-26T14:16:47.579195: step 441, loss 0.0885471, acc 0.984375\n",
      "2017-09-26T14:16:47.871719: step 442, loss 0.117391, acc 0.96875\n",
      "2017-09-26T14:16:48.178557: step 443, loss 0.103462, acc 0.953125\n",
      "2017-09-26T14:16:48.492027: step 444, loss 0.202234, acc 0.953125\n",
      "2017-09-26T14:16:48.806616: step 445, loss 0.199166, acc 0.90625\n",
      "2017-09-26T14:16:49.140277: step 446, loss 0.125027, acc 0.953125\n",
      "2017-09-26T14:16:49.445055: step 447, loss 0.151574, acc 0.921875\n",
      "2017-09-26T14:16:49.759827: step 448, loss 0.182344, acc 0.953125\n",
      "2017-09-26T14:16:50.077773: step 449, loss 0.0844105, acc 0.984375\n",
      "2017-09-26T14:16:50.391952: step 450, loss 0.105261, acc 0.984375\n",
      "2017-09-26T14:16:50.672593: step 451, loss 0.0809348, acc 0.984375\n",
      "2017-09-26T14:16:50.958045: step 452, loss 0.172774, acc 0.953125\n",
      "2017-09-26T14:16:51.249612: step 453, loss 0.0813167, acc 0.96875\n",
      "2017-09-26T14:16:51.543799: step 454, loss 0.146964, acc 0.953125\n",
      "2017-09-26T14:16:51.834054: step 455, loss 0.361287, acc 0.90625\n",
      "2017-09-26T14:16:52.119093: step 456, loss 0.0951621, acc 0.96875\n",
      "2017-09-26T14:16:52.400441: step 457, loss 0.0748455, acc 0.984375\n",
      "2017-09-26T14:16:52.685686: step 458, loss 0.156674, acc 0.953125\n",
      "2017-09-26T14:16:52.938301: step 459, loss 0.0985268, acc 0.953125\n",
      "2017-09-26T14:16:53.216567: step 460, loss 0.171466, acc 0.96875\n",
      "2017-09-26T14:16:53.505857: step 461, loss 0.14839, acc 0.9375\n",
      "2017-09-26T14:16:53.736857: step 462, loss 0.105682, acc 0.942308\n",
      "2017-09-26T14:16:54.003825: step 463, loss 0.0848252, acc 1\n",
      "2017-09-26T14:16:54.265747: step 464, loss 0.24555, acc 0.90625\n",
      "2017-09-26T14:16:54.522961: step 465, loss 0.23581, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:16:54.782637: step 466, loss 0.0838815, acc 0.984375\n",
      "2017-09-26T14:16:55.042504: step 467, loss 0.11221, acc 0.953125\n",
      "2017-09-26T14:16:55.309241: step 468, loss 0.0877698, acc 0.984375\n",
      "2017-09-26T14:16:55.563165: step 469, loss 0.0414352, acc 0.984375\n",
      "2017-09-26T14:16:55.816061: step 470, loss 0.105741, acc 0.953125\n",
      "2017-09-26T14:16:56.064549: step 471, loss 0.0952424, acc 1\n",
      "2017-09-26T14:16:56.321998: step 472, loss 0.190775, acc 0.9375\n",
      "2017-09-26T14:16:56.575133: step 473, loss 0.0937542, acc 0.984375\n",
      "2017-09-26T14:16:56.826640: step 474, loss 0.117702, acc 0.953125\n",
      "2017-09-26T14:16:57.079050: step 475, loss 0.109169, acc 0.953125\n",
      "2017-09-26T14:16:57.331839: step 476, loss 0.142343, acc 0.953125\n",
      "2017-09-26T14:16:57.581893: step 477, loss 0.179963, acc 0.921875\n",
      "2017-09-26T14:16:57.834403: step 478, loss 0.0932268, acc 0.984375\n",
      "2017-09-26T14:16:58.082319: step 479, loss 0.0942976, acc 0.953125\n",
      "2017-09-26T14:16:58.344808: step 480, loss 0.127079, acc 0.984375\n",
      "2017-09-26T14:16:58.605743: step 481, loss 0.120528, acc 0.953125\n",
      "2017-09-26T14:16:58.863526: step 482, loss 0.126659, acc 0.984375\n",
      "2017-09-26T14:16:59.132566: step 483, loss 0.107811, acc 0.984375\n",
      "2017-09-26T14:16:59.410398: step 484, loss 0.122811, acc 0.96875\n",
      "2017-09-26T14:16:59.677864: step 485, loss 0.0747693, acc 0.984375\n",
      "2017-09-26T14:16:59.938027: step 486, loss 0.15998, acc 0.9375\n",
      "2017-09-26T14:17:00.214895: step 487, loss 0.0926736, acc 0.984375\n",
      "2017-09-26T14:17:00.488433: step 488, loss 0.206845, acc 0.921875\n",
      "2017-09-26T14:17:00.772101: step 489, loss 0.242706, acc 0.90625\n",
      "2017-09-26T14:17:01.034886: step 490, loss 0.204445, acc 0.9375\n",
      "2017-09-26T14:17:01.308395: step 491, loss 0.222807, acc 0.9375\n",
      "2017-09-26T14:17:01.572183: step 492, loss 0.218203, acc 0.921875\n",
      "2017-09-26T14:17:01.851287: step 493, loss 0.234476, acc 0.90625\n",
      "2017-09-26T14:17:02.134893: step 494, loss 0.187298, acc 0.953125\n",
      "2017-09-26T14:17:02.412024: step 495, loss 0.0783003, acc 0.984375\n",
      "2017-09-26T14:17:02.698089: step 496, loss 0.217059, acc 0.953125\n",
      "2017-09-26T14:17:03.007491: step 497, loss 0.195994, acc 0.90625\n",
      "2017-09-26T14:17:03.380384: step 498, loss 0.149192, acc 0.953125\n",
      "2017-09-26T14:17:03.730318: step 499, loss 0.0742788, acc 0.984375\n",
      "2017-09-26T14:17:04.110816: step 500, loss 0.129633, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:17:04.382498: step 500, loss 0.406326, acc 0.875421\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-500\n",
      "\n",
      "2017-09-26T14:17:04.911753: step 501, loss 0.206937, acc 0.921875\n",
      "2017-09-26T14:17:05.180093: step 502, loss 0.13204, acc 0.9375\n",
      "2017-09-26T14:17:05.425669: step 503, loss 0.136136, acc 0.96875\n",
      "2017-09-26T14:17:05.648017: step 504, loss 0.0900452, acc 0.980769\n",
      "2017-09-26T14:17:05.977380: step 505, loss 0.143953, acc 0.9375\n",
      "2017-09-26T14:17:06.276643: step 506, loss 0.0660502, acc 0.96875\n",
      "2017-09-26T14:17:06.555045: step 507, loss 0.0705778, acc 0.96875\n",
      "2017-09-26T14:17:06.868719: step 508, loss 0.0673448, acc 0.984375\n",
      "2017-09-26T14:17:07.105388: step 509, loss 0.042454, acc 1\n",
      "2017-09-26T14:17:07.362050: step 510, loss 0.12156, acc 0.953125\n",
      "2017-09-26T14:17:07.625951: step 511, loss 0.131085, acc 0.96875\n",
      "2017-09-26T14:17:07.923149: step 512, loss 0.204316, acc 0.9375\n",
      "2017-09-26T14:17:08.176797: step 513, loss 0.0912481, acc 0.96875\n",
      "2017-09-26T14:17:08.444706: step 514, loss 0.0988833, acc 0.96875\n",
      "2017-09-26T14:17:08.731842: step 515, loss 0.0507179, acc 1\n",
      "2017-09-26T14:17:09.004910: step 516, loss 0.121186, acc 0.9375\n",
      "2017-09-26T14:17:09.316675: step 517, loss 0.0858939, acc 0.96875\n",
      "2017-09-26T14:17:09.654344: step 518, loss 0.0783507, acc 1\n",
      "2017-09-26T14:17:10.003023: step 519, loss 0.215559, acc 0.96875\n",
      "2017-09-26T14:17:10.327836: step 520, loss 0.11048, acc 0.984375\n",
      "2017-09-26T14:17:10.671901: step 521, loss 0.0619112, acc 0.984375\n",
      "2017-09-26T14:17:10.946717: step 522, loss 0.0716579, acc 0.984375\n",
      "2017-09-26T14:17:11.205225: step 523, loss 0.120708, acc 0.953125\n",
      "2017-09-26T14:17:11.550131: step 524, loss 0.0713744, acc 0.984375\n",
      "2017-09-26T14:17:11.926387: step 525, loss 0.142862, acc 0.953125\n",
      "2017-09-26T14:17:12.262132: step 526, loss 0.144814, acc 0.953125\n",
      "2017-09-26T14:17:12.681636: step 527, loss 0.0641185, acc 1\n",
      "2017-09-26T14:17:12.996558: step 528, loss 0.0710593, acc 0.96875\n",
      "2017-09-26T14:17:13.281254: step 529, loss 0.0435819, acc 1\n",
      "2017-09-26T14:17:13.565167: step 530, loss 0.0922802, acc 0.984375\n",
      "2017-09-26T14:17:13.842181: step 531, loss 0.0621434, acc 0.96875\n",
      "2017-09-26T14:17:14.104316: step 532, loss 0.138672, acc 0.9375\n",
      "2017-09-26T14:17:14.359527: step 533, loss 0.130022, acc 0.96875\n",
      "2017-09-26T14:17:14.610045: step 534, loss 0.0632275, acc 0.984375\n",
      "2017-09-26T14:17:14.894728: step 535, loss 0.127697, acc 0.96875\n",
      "2017-09-26T14:17:15.149112: step 536, loss 0.124209, acc 0.9375\n",
      "2017-09-26T14:17:15.403870: step 537, loss 0.0673116, acc 0.984375\n",
      "2017-09-26T14:17:15.651153: step 538, loss 0.0625816, acc 0.984375\n",
      "2017-09-26T14:17:15.913658: step 539, loss 0.0996422, acc 0.953125\n",
      "2017-09-26T14:17:16.185946: step 540, loss 0.0538629, acc 0.984375\n",
      "2017-09-26T14:17:16.434971: step 541, loss 0.0466371, acc 1\n",
      "2017-09-26T14:17:16.734250: step 542, loss 0.11358, acc 0.96875\n",
      "2017-09-26T14:17:17.002038: step 543, loss 0.115132, acc 0.96875\n",
      "2017-09-26T14:17:17.275045: step 544, loss 0.106879, acc 0.96875\n",
      "2017-09-26T14:17:17.541189: step 545, loss 0.152048, acc 0.921875\n",
      "2017-09-26T14:17:17.778338: step 546, loss 0.0372167, acc 1\n",
      "2017-09-26T14:17:18.051329: step 547, loss 0.146142, acc 0.9375\n",
      "2017-09-26T14:17:18.310961: step 548, loss 0.0371467, acc 1\n",
      "2017-09-26T14:17:18.581431: step 549, loss 0.183339, acc 0.953125\n",
      "2017-09-26T14:17:18.867830: step 550, loss 0.16772, acc 0.9375\n",
      "2017-09-26T14:17:19.135831: step 551, loss 0.0931822, acc 0.984375\n",
      "2017-09-26T14:17:19.424022: step 552, loss 0.0588087, acc 0.984375\n",
      "2017-09-26T14:17:19.678891: step 553, loss 0.051057, acc 0.984375\n",
      "2017-09-26T14:17:19.972245: step 554, loss 0.0374642, acc 1\n",
      "2017-09-26T14:17:20.246651: step 555, loss 0.105095, acc 0.953125\n",
      "2017-09-26T14:17:20.516219: step 556, loss 0.100492, acc 0.953125\n",
      "2017-09-26T14:17:20.786612: step 557, loss 0.262002, acc 0.9375\n",
      "2017-09-26T14:17:21.054383: step 558, loss 0.0938336, acc 0.984375\n",
      "2017-09-26T14:17:21.319094: step 559, loss 0.0658799, acc 0.96875\n",
      "2017-09-26T14:17:21.616159: step 560, loss 0.0429995, acc 1\n",
      "2017-09-26T14:17:21.907353: step 561, loss 0.0896713, acc 0.96875\n",
      "2017-09-26T14:17:22.168499: step 562, loss 0.0994463, acc 0.953125\n",
      "2017-09-26T14:17:22.449328: step 563, loss 0.0980109, acc 0.953125\n",
      "2017-09-26T14:17:22.688316: step 564, loss 0.0889064, acc 0.96875\n",
      "2017-09-26T14:17:22.941070: step 565, loss 0.117086, acc 0.96875\n",
      "2017-09-26T14:17:23.184363: step 566, loss 0.183387, acc 0.921875\n",
      "2017-09-26T14:17:23.430411: step 567, loss 0.164054, acc 0.9375\n",
      "2017-09-26T14:17:23.698297: step 568, loss 0.0910775, acc 0.96875\n",
      "2017-09-26T14:17:23.977355: step 569, loss 0.0623891, acc 0.96875\n",
      "2017-09-26T14:17:24.229388: step 570, loss 0.179394, acc 0.9375\n",
      "2017-09-26T14:17:24.510793: step 571, loss 0.0637341, acc 0.984375\n",
      "2017-09-26T14:17:24.823174: step 572, loss 0.157138, acc 0.953125\n",
      "2017-09-26T14:17:25.149139: step 573, loss 0.0801784, acc 0.96875\n",
      "2017-09-26T14:17:25.423263: step 574, loss 0.167068, acc 0.90625\n",
      "2017-09-26T14:17:25.735880: step 575, loss 0.0905993, acc 0.953125\n",
      "2017-09-26T14:17:25.993912: step 576, loss 0.032412, acc 1\n",
      "2017-09-26T14:17:26.271033: step 577, loss 0.150592, acc 0.96875\n",
      "2017-09-26T14:17:26.538102: step 578, loss 0.155585, acc 0.953125\n",
      "2017-09-26T14:17:26.806240: step 579, loss 0.101082, acc 0.984375\n",
      "2017-09-26T14:17:27.085212: step 580, loss 0.0646298, acc 1\n",
      "2017-09-26T14:17:27.324198: step 581, loss 0.192533, acc 0.953125\n",
      "2017-09-26T14:17:27.571592: step 582, loss 0.209936, acc 0.90625\n",
      "2017-09-26T14:17:27.822359: step 583, loss 0.0448929, acc 1\n",
      "2017-09-26T14:17:28.070611: step 584, loss 0.121449, acc 0.953125\n",
      "2017-09-26T14:17:28.332009: step 585, loss 0.121057, acc 0.953125\n",
      "2017-09-26T14:17:28.578574: step 586, loss 0.12688, acc 0.9375\n",
      "2017-09-26T14:17:28.867204: step 587, loss 0.143785, acc 0.953125\n",
      "2017-09-26T14:17:29.097713: step 588, loss 0.0963069, acc 0.961538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:17:29.376008: step 589, loss 0.087057, acc 0.96875\n",
      "2017-09-26T14:17:29.626096: step 590, loss 0.113697, acc 0.96875\n",
      "2017-09-26T14:17:29.892970: step 591, loss 0.0307216, acc 0.984375\n",
      "2017-09-26T14:17:30.150632: step 592, loss 0.064799, acc 0.984375\n",
      "2017-09-26T14:17:30.419963: step 593, loss 0.0722256, acc 0.984375\n",
      "2017-09-26T14:17:30.675680: step 594, loss 0.159646, acc 0.953125\n",
      "2017-09-26T14:17:30.924989: step 595, loss 0.156768, acc 0.9375\n",
      "2017-09-26T14:17:31.187565: step 596, loss 0.102811, acc 0.984375\n",
      "2017-09-26T14:17:31.459532: step 597, loss 0.0472105, acc 1\n",
      "2017-09-26T14:17:31.752214: step 598, loss 0.0287409, acc 1\n",
      "2017-09-26T14:17:31.990066: step 599, loss 0.107791, acc 0.953125\n",
      "2017-09-26T14:17:32.253651: step 600, loss 0.0558608, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:17:32.544867: step 600, loss 0.424079, acc 0.875421\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-600\n",
      "\n",
      "2017-09-26T14:17:33.072299: step 601, loss 0.0835255, acc 0.96875\n",
      "2017-09-26T14:17:33.339281: step 602, loss 0.050472, acc 1\n",
      "2017-09-26T14:17:33.605764: step 603, loss 0.195082, acc 0.9375\n",
      "2017-09-26T14:17:33.872080: step 604, loss 0.0423646, acc 1\n",
      "2017-09-26T14:17:34.192793: step 605, loss 0.228246, acc 0.9375\n",
      "2017-09-26T14:17:34.512546: step 606, loss 0.0621835, acc 0.96875\n",
      "2017-09-26T14:17:34.804389: step 607, loss 0.101876, acc 0.9375\n",
      "2017-09-26T14:17:35.183998: step 608, loss 0.0862666, acc 0.953125\n",
      "2017-09-26T14:17:35.522828: step 609, loss 0.0522891, acc 0.984375\n",
      "2017-09-26T14:17:35.810943: step 610, loss 0.0529051, acc 0.984375\n",
      "2017-09-26T14:17:36.055140: step 611, loss 0.0861175, acc 0.96875\n",
      "2017-09-26T14:17:36.319786: step 612, loss 0.0862704, acc 0.984375\n",
      "2017-09-26T14:17:36.617359: step 613, loss 0.0563953, acc 0.984375\n",
      "2017-09-26T14:17:36.908769: step 614, loss 0.132673, acc 0.953125\n",
      "2017-09-26T14:17:37.183151: step 615, loss 0.112438, acc 0.9375\n",
      "2017-09-26T14:17:37.476434: step 616, loss 0.0938405, acc 0.96875\n",
      "2017-09-26T14:17:37.756065: step 617, loss 0.0356544, acc 1\n",
      "2017-09-26T14:17:38.058271: step 618, loss 0.0330164, acc 1\n",
      "2017-09-26T14:17:38.331529: step 619, loss 0.124667, acc 0.9375\n",
      "2017-09-26T14:17:38.619527: step 620, loss 0.048881, acc 1\n",
      "2017-09-26T14:17:38.943801: step 621, loss 0.167485, acc 0.9375\n",
      "2017-09-26T14:17:39.236929: step 622, loss 0.0528431, acc 0.984375\n",
      "2017-09-26T14:17:39.494012: step 623, loss 0.0656298, acc 0.984375\n",
      "2017-09-26T14:17:39.728664: step 624, loss 0.0688716, acc 0.984375\n",
      "2017-09-26T14:17:39.970871: step 625, loss 0.0475147, acc 0.984375\n",
      "2017-09-26T14:17:40.234853: step 626, loss 0.127975, acc 0.96875\n",
      "2017-09-26T14:17:40.482432: step 627, loss 0.0946657, acc 0.984375\n",
      "2017-09-26T14:17:40.712406: step 628, loss 0.0819254, acc 0.984375\n",
      "2017-09-26T14:17:40.955823: step 629, loss 0.109169, acc 0.96875\n",
      "2017-09-26T14:17:41.166089: step 630, loss 0.0627837, acc 1\n",
      "2017-09-26T14:17:41.426090: step 631, loss 0.0581427, acc 1\n",
      "2017-09-26T14:17:41.657194: step 632, loss 0.0711905, acc 0.984375\n",
      "2017-09-26T14:17:41.899132: step 633, loss 0.0608434, acc 0.96875\n",
      "2017-09-26T14:17:42.145044: step 634, loss 0.0688033, acc 0.984375\n",
      "2017-09-26T14:17:42.394294: step 635, loss 0.0588617, acc 0.984375\n",
      "2017-09-26T14:17:42.625811: step 636, loss 0.144382, acc 0.96875\n",
      "2017-09-26T14:17:42.862794: step 637, loss 0.0679497, acc 0.96875\n",
      "2017-09-26T14:17:43.105138: step 638, loss 0.0731343, acc 0.984375\n",
      "2017-09-26T14:17:43.342694: step 639, loss 0.107273, acc 0.96875\n",
      "2017-09-26T14:17:43.576585: step 640, loss 0.134488, acc 0.96875\n",
      "2017-09-26T14:17:43.819724: step 641, loss 0.12945, acc 0.953125\n",
      "2017-09-26T14:17:44.064642: step 642, loss 0.1281, acc 0.96875\n",
      "2017-09-26T14:17:44.307961: step 643, loss 0.0305825, acc 0.984375\n",
      "2017-09-26T14:17:44.534751: step 644, loss 0.0727638, acc 0.984375\n",
      "2017-09-26T14:17:44.769923: step 645, loss 0.0883975, acc 0.96875\n",
      "2017-09-26T14:17:45.006106: step 646, loss 0.0501992, acc 0.984375\n",
      "2017-09-26T14:17:45.272271: step 647, loss 0.068172, acc 0.96875\n",
      "2017-09-26T14:17:45.506876: step 648, loss 0.109296, acc 0.9375\n",
      "2017-09-26T14:17:45.757625: step 649, loss 0.0710575, acc 0.96875\n",
      "2017-09-26T14:17:46.003246: step 650, loss 0.127013, acc 0.984375\n",
      "2017-09-26T14:17:46.269445: step 651, loss 0.0557436, acc 0.984375\n",
      "2017-09-26T14:17:46.517839: step 652, loss 0.0535724, acc 0.984375\n",
      "2017-09-26T14:17:46.818776: step 653, loss 0.0901264, acc 0.96875\n",
      "2017-09-26T14:17:47.071262: step 654, loss 0.0388719, acc 1\n",
      "2017-09-26T14:17:47.318013: step 655, loss 0.107161, acc 0.953125\n",
      "2017-09-26T14:17:47.592006: step 656, loss 0.0822852, acc 0.984375\n",
      "2017-09-26T14:17:47.877320: step 657, loss 0.301285, acc 0.9375\n",
      "2017-09-26T14:17:48.143474: step 658, loss 0.226941, acc 0.9375\n",
      "2017-09-26T14:17:48.376251: step 659, loss 0.0481082, acc 0.984375\n",
      "2017-09-26T14:17:48.621370: step 660, loss 0.0407825, acc 1\n",
      "2017-09-26T14:17:48.858381: step 661, loss 0.0698598, acc 0.984375\n",
      "2017-09-26T14:17:49.092250: step 662, loss 0.0510927, acc 0.984375\n",
      "2017-09-26T14:17:49.345095: step 663, loss 0.0987501, acc 0.9375\n",
      "2017-09-26T14:17:49.576895: step 664, loss 0.0957429, acc 0.96875\n",
      "2017-09-26T14:17:49.812429: step 665, loss 0.0560767, acc 0.984375\n",
      "2017-09-26T14:17:50.045313: step 666, loss 0.132651, acc 0.953125\n",
      "2017-09-26T14:17:50.299511: step 667, loss 0.0442953, acc 1\n",
      "2017-09-26T14:17:50.535430: step 668, loss 0.0892569, acc 0.96875\n",
      "2017-09-26T14:17:50.770564: step 669, loss 0.034212, acc 1\n",
      "2017-09-26T14:17:51.012234: step 670, loss 0.135527, acc 0.953125\n",
      "2017-09-26T14:17:51.246749: step 671, loss 0.0379399, acc 0.96875\n",
      "2017-09-26T14:17:51.448075: step 672, loss 0.0499506, acc 1\n",
      "2017-09-26T14:17:51.691630: step 673, loss 0.0689628, acc 0.984375\n",
      "2017-09-26T14:17:51.919247: step 674, loss 0.0900389, acc 0.96875\n",
      "2017-09-26T14:17:52.158159: step 675, loss 0.0657934, acc 0.96875\n",
      "2017-09-26T14:17:52.435769: step 676, loss 0.0957176, acc 0.984375\n",
      "2017-09-26T14:17:52.732070: step 677, loss 0.0553863, acc 0.984375\n",
      "2017-09-26T14:17:53.002894: step 678, loss 0.132658, acc 0.921875\n",
      "2017-09-26T14:17:53.236276: step 679, loss 0.11933, acc 0.9375\n",
      "2017-09-26T14:17:53.523229: step 680, loss 0.0270973, acc 1\n",
      "2017-09-26T14:17:53.829977: step 681, loss 0.185148, acc 0.921875\n",
      "2017-09-26T14:17:54.100694: step 682, loss 0.101216, acc 0.953125\n",
      "2017-09-26T14:17:54.351586: step 683, loss 0.0753821, acc 0.96875\n",
      "2017-09-26T14:17:54.602051: step 684, loss 0.041926, acc 1\n",
      "2017-09-26T14:17:54.834631: step 685, loss 0.0619155, acc 0.984375\n",
      "2017-09-26T14:17:55.073777: step 686, loss 0.0615138, acc 0.984375\n",
      "2017-09-26T14:17:55.311071: step 687, loss 0.181211, acc 0.953125\n",
      "2017-09-26T14:17:55.545867: step 688, loss 0.0686007, acc 0.96875\n",
      "2017-09-26T14:17:55.782139: step 689, loss 0.0497936, acc 0.984375\n",
      "2017-09-26T14:17:56.024142: step 690, loss 0.0453935, acc 1\n",
      "2017-09-26T14:17:56.259399: step 691, loss 0.0273683, acc 1\n",
      "2017-09-26T14:17:56.542994: step 692, loss 0.160736, acc 0.953125\n",
      "2017-09-26T14:17:56.834654: step 693, loss 0.0755772, acc 0.953125\n",
      "2017-09-26T14:17:57.134861: step 694, loss 0.108995, acc 0.984375\n",
      "2017-09-26T14:17:57.367586: step 695, loss 0.0384283, acc 1\n",
      "2017-09-26T14:17:57.608403: step 696, loss 0.125031, acc 0.984375\n",
      "2017-09-26T14:17:57.845262: step 697, loss 0.0460567, acc 0.984375\n",
      "2017-09-26T14:17:58.080807: step 698, loss 0.0502603, acc 1\n",
      "2017-09-26T14:17:58.320830: step 699, loss 0.123772, acc 0.96875\n",
      "2017-09-26T14:17:58.555611: step 700, loss 0.0189394, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:17:58.809170: step 700, loss 0.400732, acc 0.875421\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-700\n",
      "\n",
      "2017-09-26T14:17:59.409443: step 701, loss 0.0848455, acc 0.984375\n",
      "2017-09-26T14:17:59.676439: step 702, loss 0.0487633, acc 0.984375\n",
      "2017-09-26T14:17:59.920080: step 703, loss 0.0545096, acc 0.96875\n",
      "2017-09-26T14:18:00.193134: step 704, loss 0.111618, acc 0.9375\n",
      "2017-09-26T14:18:00.472036: step 705, loss 0.0699671, acc 0.984375\n",
      "2017-09-26T14:18:00.758055: step 706, loss 0.0524762, acc 1\n",
      "2017-09-26T14:18:01.062100: step 707, loss 0.105318, acc 0.9375\n",
      "2017-09-26T14:18:01.318871: step 708, loss 0.168627, acc 0.9375\n",
      "2017-09-26T14:18:01.550010: step 709, loss 0.0641992, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:18:01.796400: step 710, loss 0.0730654, acc 0.984375\n",
      "2017-09-26T14:18:02.043787: step 711, loss 0.0388091, acc 0.96875\n",
      "2017-09-26T14:18:02.282382: step 712, loss 0.0750161, acc 0.984375\n",
      "2017-09-26T14:18:02.519148: step 713, loss 0.163859, acc 0.953125\n",
      "2017-09-26T14:18:02.732587: step 714, loss 0.033391, acc 1\n",
      "2017-09-26T14:18:02.973717: step 715, loss 0.158146, acc 0.953125\n",
      "2017-09-26T14:18:03.204020: step 716, loss 0.0539293, acc 0.984375\n",
      "2017-09-26T14:18:03.434580: step 717, loss 0.0773441, acc 0.96875\n",
      "2017-09-26T14:18:03.665326: step 718, loss 0.0729783, acc 0.984375\n",
      "2017-09-26T14:18:03.922350: step 719, loss 0.0305772, acc 1\n",
      "2017-09-26T14:18:04.154264: step 720, loss 0.0259593, acc 1\n",
      "2017-09-26T14:18:04.415602: step 721, loss 0.0518306, acc 0.984375\n",
      "2017-09-26T14:18:04.661294: step 722, loss 0.033749, acc 0.984375\n",
      "2017-09-26T14:18:04.901779: step 723, loss 0.0604072, acc 0.96875\n",
      "2017-09-26T14:18:05.147726: step 724, loss 0.0445898, acc 1\n",
      "2017-09-26T14:18:05.396683: step 725, loss 0.0346129, acc 0.984375\n",
      "2017-09-26T14:18:05.632091: step 726, loss 0.0684859, acc 0.984375\n",
      "2017-09-26T14:18:05.868228: step 727, loss 0.0663645, acc 0.984375\n",
      "2017-09-26T14:18:06.147138: step 728, loss 0.0356258, acc 0.984375\n",
      "2017-09-26T14:18:06.395652: step 729, loss 0.0509282, acc 0.984375\n",
      "2017-09-26T14:18:06.640107: step 730, loss 0.111908, acc 0.953125\n",
      "2017-09-26T14:18:06.877863: step 731, loss 0.0698276, acc 0.984375\n",
      "2017-09-26T14:18:07.122707: step 732, loss 0.0132538, acc 1\n",
      "2017-09-26T14:18:07.357149: step 733, loss 0.0674637, acc 0.96875\n",
      "2017-09-26T14:18:07.591259: step 734, loss 0.0283388, acc 1\n",
      "2017-09-26T14:18:07.836752: step 735, loss 0.0233518, acc 1\n",
      "2017-09-26T14:18:08.071397: step 736, loss 0.0404557, acc 0.984375\n",
      "2017-09-26T14:18:08.318375: step 737, loss 0.0140563, acc 1\n",
      "2017-09-26T14:18:08.554158: step 738, loss 0.0468746, acc 1\n",
      "2017-09-26T14:18:08.802475: step 739, loss 0.0264437, acc 1\n",
      "2017-09-26T14:18:09.046608: step 740, loss 0.0296356, acc 0.984375\n",
      "2017-09-26T14:18:09.289834: step 741, loss 0.0292132, acc 1\n",
      "2017-09-26T14:18:09.522538: step 742, loss 0.108806, acc 0.984375\n",
      "2017-09-26T14:18:09.767185: step 743, loss 0.0634031, acc 0.96875\n",
      "2017-09-26T14:18:10.000378: step 744, loss 0.0494068, acc 1\n",
      "2017-09-26T14:18:10.239911: step 745, loss 0.105832, acc 0.96875\n",
      "2017-09-26T14:18:10.473757: step 746, loss 0.0960221, acc 0.984375\n",
      "2017-09-26T14:18:10.721433: step 747, loss 0.0355133, acc 0.984375\n",
      "2017-09-26T14:18:10.974551: step 748, loss 0.0629132, acc 0.984375\n",
      "2017-09-26T14:18:11.211521: step 749, loss 0.0213074, acc 1\n",
      "2017-09-26T14:18:11.446940: step 750, loss 0.189765, acc 0.9375\n",
      "2017-09-26T14:18:11.706710: step 751, loss 0.0389178, acc 1\n",
      "2017-09-26T14:18:11.950486: step 752, loss 0.0407064, acc 1\n",
      "2017-09-26T14:18:12.203778: step 753, loss 0.018093, acc 1\n",
      "2017-09-26T14:18:12.473270: step 754, loss 0.0700523, acc 0.984375\n",
      "2017-09-26T14:18:12.741709: step 755, loss 0.0548109, acc 0.984375\n",
      "2017-09-26T14:18:12.976591: step 756, loss 0.145043, acc 0.942308\n",
      "2017-09-26T14:18:13.247375: step 757, loss 0.031785, acc 1\n",
      "2017-09-26T14:18:13.488655: step 758, loss 0.162708, acc 0.96875\n",
      "2017-09-26T14:18:13.734122: step 759, loss 0.0407153, acc 0.984375\n",
      "2017-09-26T14:18:13.969961: step 760, loss 0.0768083, acc 0.96875\n",
      "2017-09-26T14:18:14.212842: step 761, loss 0.0329583, acc 1\n",
      "2017-09-26T14:18:14.454834: step 762, loss 0.067125, acc 0.984375\n",
      "2017-09-26T14:18:14.687056: step 763, loss 0.0300541, acc 1\n",
      "2017-09-26T14:18:14.920217: step 764, loss 0.108835, acc 0.96875\n",
      "2017-09-26T14:18:15.169140: step 765, loss 0.154575, acc 0.984375\n",
      "2017-09-26T14:18:15.402709: step 766, loss 0.0665685, acc 0.984375\n",
      "2017-09-26T14:18:15.635235: step 767, loss 0.030504, acc 1\n",
      "2017-09-26T14:18:15.867873: step 768, loss 0.035367, acc 0.984375\n",
      "2017-09-26T14:18:16.107974: step 769, loss 0.0549109, acc 0.984375\n",
      "2017-09-26T14:18:16.348976: step 770, loss 0.0385966, acc 0.984375\n",
      "2017-09-26T14:18:16.582001: step 771, loss 0.0374579, acc 0.984375\n",
      "2017-09-26T14:18:16.816472: step 772, loss 0.0444319, acc 0.984375\n",
      "2017-09-26T14:18:17.060737: step 773, loss 0.0395018, acc 0.984375\n",
      "2017-09-26T14:18:17.313098: step 774, loss 0.045368, acc 0.984375\n",
      "2017-09-26T14:18:17.545884: step 775, loss 0.0582875, acc 0.96875\n",
      "2017-09-26T14:18:17.783135: step 776, loss 0.118502, acc 0.953125\n",
      "2017-09-26T14:18:18.022001: step 777, loss 0.0303692, acc 1\n",
      "2017-09-26T14:18:18.286718: step 778, loss 0.0700105, acc 0.953125\n",
      "2017-09-26T14:18:18.522279: step 779, loss 0.0475531, acc 1\n",
      "2017-09-26T14:18:18.758511: step 780, loss 0.0339249, acc 1\n",
      "2017-09-26T14:18:19.003328: step 781, loss 0.0335619, acc 1\n",
      "2017-09-26T14:18:19.247279: step 782, loss 0.120677, acc 0.96875\n",
      "2017-09-26T14:18:19.484886: step 783, loss 0.0209128, acc 1\n",
      "2017-09-26T14:18:19.735595: step 784, loss 0.0452171, acc 0.984375\n",
      "2017-09-26T14:18:19.970820: step 785, loss 0.0369309, acc 1\n",
      "2017-09-26T14:18:20.215993: step 786, loss 0.111958, acc 0.953125\n",
      "2017-09-26T14:18:20.486932: step 787, loss 0.0361618, acc 0.984375\n",
      "2017-09-26T14:18:20.748614: step 788, loss 0.0115678, acc 1\n",
      "2017-09-26T14:18:21.007526: step 789, loss 0.0896406, acc 0.96875\n",
      "2017-09-26T14:18:21.241710: step 790, loss 0.00908754, acc 1\n",
      "2017-09-26T14:18:21.481111: step 791, loss 0.0787483, acc 0.96875\n",
      "2017-09-26T14:18:21.719999: step 792, loss 0.0464911, acc 0.96875\n",
      "2017-09-26T14:18:21.958247: step 793, loss 0.0305226, acc 0.984375\n",
      "2017-09-26T14:18:22.193058: step 794, loss 0.0340286, acc 1\n",
      "2017-09-26T14:18:22.428819: step 795, loss 0.0180766, acc 1\n",
      "2017-09-26T14:18:22.667236: step 796, loss 0.0324609, acc 1\n",
      "2017-09-26T14:18:22.913272: step 797, loss 0.0527564, acc 0.984375\n",
      "2017-09-26T14:18:23.122108: step 798, loss 0.257965, acc 0.903846\n",
      "2017-09-26T14:18:23.368169: step 799, loss 0.0101017, acc 1\n",
      "2017-09-26T14:18:23.601506: step 800, loss 0.0560729, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:18:23.833357: step 800, loss 0.428497, acc 0.872054\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-800\n",
      "\n",
      "2017-09-26T14:18:24.336921: step 801, loss 0.0131972, acc 1\n",
      "2017-09-26T14:18:24.572708: step 802, loss 0.0506651, acc 1\n",
      "2017-09-26T14:18:24.824975: step 803, loss 0.0171733, acc 1\n",
      "2017-09-26T14:18:25.058484: step 804, loss 0.0286449, acc 1\n",
      "2017-09-26T14:18:25.300020: step 805, loss 0.0478137, acc 0.984375\n",
      "2017-09-26T14:18:25.543971: step 806, loss 0.0488581, acc 0.984375\n",
      "2017-09-26T14:18:25.786979: step 807, loss 0.0737866, acc 0.96875\n",
      "2017-09-26T14:18:26.023836: step 808, loss 0.0752648, acc 0.984375\n",
      "2017-09-26T14:18:26.267596: step 809, loss 0.141545, acc 0.96875\n",
      "2017-09-26T14:18:26.509358: step 810, loss 0.029656, acc 0.984375\n",
      "2017-09-26T14:18:26.754585: step 811, loss 0.0445623, acc 0.984375\n",
      "2017-09-26T14:18:26.989077: step 812, loss 0.0678885, acc 0.984375\n",
      "2017-09-26T14:18:27.258568: step 813, loss 0.0695821, acc 0.984375\n",
      "2017-09-26T14:18:27.494685: step 814, loss 0.0548362, acc 0.984375\n",
      "2017-09-26T14:18:27.737293: step 815, loss 0.140167, acc 0.953125\n",
      "2017-09-26T14:18:27.976281: step 816, loss 0.0352801, acc 0.984375\n",
      "2017-09-26T14:18:28.218708: step 817, loss 0.0713863, acc 0.96875\n",
      "2017-09-26T14:18:28.458541: step 818, loss 0.0387845, acc 1\n",
      "2017-09-26T14:18:28.731806: step 819, loss 0.0898142, acc 0.984375\n",
      "2017-09-26T14:18:28.987913: step 820, loss 0.0147975, acc 1\n",
      "2017-09-26T14:18:29.232989: step 821, loss 0.0178693, acc 1\n",
      "2017-09-26T14:18:29.476577: step 822, loss 0.0270868, acc 1\n",
      "2017-09-26T14:18:29.720085: step 823, loss 0.00967319, acc 1\n",
      "2017-09-26T14:18:29.957962: step 824, loss 0.0531543, acc 0.96875\n",
      "2017-09-26T14:18:30.200664: step 825, loss 0.0871038, acc 0.953125\n",
      "2017-09-26T14:18:30.442383: step 826, loss 0.0517081, acc 0.96875\n",
      "2017-09-26T14:18:30.680753: step 827, loss 0.0591353, acc 0.984375\n",
      "2017-09-26T14:18:30.920221: step 828, loss 0.0302552, acc 0.984375\n",
      "2017-09-26T14:18:31.160648: step 829, loss 0.0329642, acc 1\n",
      "2017-09-26T14:18:31.403410: step 830, loss 0.0280007, acc 1\n",
      "2017-09-26T14:18:31.649697: step 831, loss 0.0215373, acc 1\n",
      "2017-09-26T14:18:31.890338: step 832, loss 0.0424131, acc 0.984375\n",
      "2017-09-26T14:18:32.157732: step 833, loss 0.0340359, acc 1\n",
      "2017-09-26T14:18:32.393528: step 834, loss 0.0358445, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:18:32.633128: step 835, loss 0.0596812, acc 0.984375\n",
      "2017-09-26T14:18:32.869869: step 836, loss 0.210789, acc 0.9375\n",
      "2017-09-26T14:18:33.109654: step 837, loss 0.0353117, acc 1\n",
      "2017-09-26T14:18:33.345507: step 838, loss 0.0292589, acc 1\n",
      "2017-09-26T14:18:33.581326: step 839, loss 0.0454391, acc 0.984375\n",
      "2017-09-26T14:18:33.785800: step 840, loss 0.0909565, acc 0.961538\n",
      "2017-09-26T14:18:34.025238: step 841, loss 0.0365, acc 1\n",
      "2017-09-26T14:18:34.259943: step 842, loss 0.0375059, acc 0.984375\n",
      "2017-09-26T14:18:34.500337: step 843, loss 0.043175, acc 0.984375\n",
      "2017-09-26T14:18:34.826742: step 844, loss 0.0759864, acc 0.96875\n",
      "2017-09-26T14:18:35.057810: step 845, loss 0.0280459, acc 1\n",
      "2017-09-26T14:18:35.300812: step 846, loss 0.0533848, acc 0.984375\n",
      "2017-09-26T14:18:35.539575: step 847, loss 0.0429581, acc 0.984375\n",
      "2017-09-26T14:18:35.786122: step 848, loss 0.0312262, acc 0.984375\n",
      "2017-09-26T14:18:36.015912: step 849, loss 0.0324574, acc 1\n",
      "2017-09-26T14:18:36.260590: step 850, loss 0.0324439, acc 1\n",
      "2017-09-26T14:18:36.506642: step 851, loss 0.0140132, acc 1\n",
      "2017-09-26T14:18:36.737796: step 852, loss 0.0678143, acc 0.984375\n",
      "2017-09-26T14:18:36.972661: step 853, loss 0.013832, acc 1\n",
      "2017-09-26T14:18:37.205121: step 854, loss 0.0155692, acc 1\n",
      "2017-09-26T14:18:37.441486: step 855, loss 0.0152608, acc 1\n",
      "2017-09-26T14:18:37.688719: step 856, loss 0.0336218, acc 0.984375\n",
      "2017-09-26T14:18:37.922915: step 857, loss 0.0991073, acc 0.984375\n",
      "2017-09-26T14:18:38.192496: step 858, loss 0.0172245, acc 1\n",
      "2017-09-26T14:18:38.439044: step 859, loss 0.0169733, acc 1\n",
      "2017-09-26T14:18:38.679055: step 860, loss 0.0254013, acc 1\n",
      "2017-09-26T14:18:38.923824: step 861, loss 0.0598054, acc 0.984375\n",
      "2017-09-26T14:18:39.156054: step 862, loss 0.0243319, acc 1\n",
      "2017-09-26T14:18:39.393434: step 863, loss 0.0180818, acc 1\n",
      "2017-09-26T14:18:39.625732: step 864, loss 0.0348831, acc 1\n",
      "2017-09-26T14:18:39.858041: step 865, loss 0.0671239, acc 0.984375\n",
      "2017-09-26T14:18:40.090576: step 866, loss 0.0116566, acc 1\n",
      "2017-09-26T14:18:40.333635: step 867, loss 0.0840309, acc 0.96875\n",
      "2017-09-26T14:18:40.571033: step 868, loss 0.0447207, acc 0.984375\n",
      "2017-09-26T14:18:40.813484: step 869, loss 0.0602636, acc 0.96875\n",
      "2017-09-26T14:18:41.049109: step 870, loss 0.0303721, acc 1\n",
      "2017-09-26T14:18:41.282418: step 871, loss 0.0155609, acc 1\n",
      "2017-09-26T14:18:41.521483: step 872, loss 0.0326448, acc 1\n",
      "2017-09-26T14:18:41.754493: step 873, loss 0.067145, acc 0.984375\n",
      "2017-09-26T14:18:41.985950: step 874, loss 0.050037, acc 0.984375\n",
      "2017-09-26T14:18:42.234370: step 875, loss 0.00919501, acc 1\n",
      "2017-09-26T14:18:42.474285: step 876, loss 0.0902917, acc 0.953125\n",
      "2017-09-26T14:18:42.712965: step 877, loss 0.0410028, acc 1\n",
      "2017-09-26T14:18:42.952857: step 878, loss 0.0614163, acc 0.96875\n",
      "2017-09-26T14:18:43.189083: step 879, loss 0.048168, acc 0.984375\n",
      "2017-09-26T14:18:43.434822: step 880, loss 0.064136, acc 0.984375\n",
      "2017-09-26T14:18:43.668925: step 881, loss 0.0323547, acc 0.984375\n",
      "2017-09-26T14:18:43.883149: step 882, loss 0.0340886, acc 1\n",
      "2017-09-26T14:18:44.136484: step 883, loss 0.0485918, acc 0.984375\n",
      "2017-09-26T14:18:44.379904: step 884, loss 0.0239588, acc 1\n",
      "2017-09-26T14:18:44.624002: step 885, loss 0.0320812, acc 1\n",
      "2017-09-26T14:18:44.870904: step 886, loss 0.0285164, acc 0.984375\n",
      "2017-09-26T14:18:45.133466: step 887, loss 0.0606391, acc 0.96875\n",
      "2017-09-26T14:18:45.371958: step 888, loss 0.074074, acc 0.96875\n",
      "2017-09-26T14:18:45.612787: step 889, loss 0.0292688, acc 0.984375\n",
      "2017-09-26T14:18:45.849330: step 890, loss 0.0194768, acc 1\n",
      "2017-09-26T14:18:46.087160: step 891, loss 0.0175089, acc 1\n",
      "2017-09-26T14:18:46.340438: step 892, loss 0.0313748, acc 1\n",
      "2017-09-26T14:18:46.584458: step 893, loss 0.058755, acc 1\n",
      "2017-09-26T14:18:46.827276: step 894, loss 0.0175676, acc 1\n",
      "2017-09-26T14:18:47.075871: step 895, loss 0.0204526, acc 1\n",
      "2017-09-26T14:18:47.312663: step 896, loss 0.0103498, acc 1\n",
      "2017-09-26T14:18:47.568298: step 897, loss 0.0595202, acc 0.984375\n",
      "2017-09-26T14:18:47.819231: step 898, loss 0.0402577, acc 0.984375\n",
      "2017-09-26T14:18:48.058293: step 899, loss 0.0322499, acc 1\n",
      "2017-09-26T14:18:48.302504: step 900, loss 0.0676351, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:18:48.532628: step 900, loss 0.432687, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-900\n",
      "\n",
      "2017-09-26T14:18:49.035793: step 901, loss 0.0116746, acc 1\n",
      "2017-09-26T14:18:49.312016: step 902, loss 0.0111916, acc 1\n",
      "2017-09-26T14:18:49.553286: step 903, loss 0.0678935, acc 0.96875\n",
      "2017-09-26T14:18:49.806847: step 904, loss 0.0742155, acc 0.953125\n",
      "2017-09-26T14:18:50.049391: step 905, loss 0.0271213, acc 1\n",
      "2017-09-26T14:18:50.299124: step 906, loss 0.0708871, acc 0.953125\n",
      "2017-09-26T14:18:50.537988: step 907, loss 0.102427, acc 0.96875\n",
      "2017-09-26T14:18:50.775768: step 908, loss 0.0284104, acc 1\n",
      "2017-09-26T14:18:51.012252: step 909, loss 0.0233323, acc 0.984375\n",
      "2017-09-26T14:18:51.257603: step 910, loss 0.0595965, acc 0.96875\n",
      "2017-09-26T14:18:51.495789: step 911, loss 0.0119026, acc 1\n",
      "2017-09-26T14:18:51.735355: step 912, loss 0.0139107, acc 1\n",
      "2017-09-26T14:18:51.975979: step 913, loss 0.0370331, acc 0.984375\n",
      "2017-09-26T14:18:52.219445: step 914, loss 0.0652082, acc 0.984375\n",
      "2017-09-26T14:18:52.466557: step 915, loss 0.0315727, acc 1\n",
      "2017-09-26T14:18:52.702034: step 916, loss 0.0379482, acc 0.984375\n",
      "2017-09-26T14:18:52.976328: step 917, loss 0.0200979, acc 1\n",
      "2017-09-26T14:18:53.206948: step 918, loss 0.0288591, acc 1\n",
      "2017-09-26T14:18:53.442081: step 919, loss 0.0121698, acc 1\n",
      "2017-09-26T14:18:53.676395: step 920, loss 0.0120034, acc 1\n",
      "2017-09-26T14:18:53.909337: step 921, loss 0.0385558, acc 1\n",
      "2017-09-26T14:18:54.159681: step 922, loss 0.0288174, acc 1\n",
      "2017-09-26T14:18:54.399372: step 923, loss 0.0508895, acc 0.984375\n",
      "2017-09-26T14:18:54.610401: step 924, loss 0.0513298, acc 0.980769\n",
      "2017-09-26T14:18:54.852312: step 925, loss 0.0592637, acc 0.984375\n",
      "2017-09-26T14:18:55.104106: step 926, loss 0.0902204, acc 0.96875\n",
      "2017-09-26T14:18:55.355375: step 927, loss 0.00500335, acc 1\n",
      "2017-09-26T14:18:55.598326: step 928, loss 0.0261513, acc 1\n",
      "2017-09-26T14:18:55.843378: step 929, loss 0.0488948, acc 0.96875\n",
      "2017-09-26T14:18:56.093019: step 930, loss 0.0156048, acc 1\n",
      "2017-09-26T14:18:56.329478: step 931, loss 0.0586572, acc 0.984375\n",
      "2017-09-26T14:18:56.587719: step 932, loss 0.100331, acc 0.984375\n",
      "2017-09-26T14:18:56.887918: step 933, loss 0.038839, acc 1\n",
      "2017-09-26T14:18:57.167639: step 934, loss 0.026169, acc 1\n",
      "2017-09-26T14:18:57.402671: step 935, loss 0.0281076, acc 0.984375\n",
      "2017-09-26T14:18:57.676834: step 936, loss 0.110694, acc 0.96875\n",
      "2017-09-26T14:18:57.917615: step 937, loss 0.0808549, acc 0.984375\n",
      "2017-09-26T14:18:58.197340: step 938, loss 0.0581624, acc 0.984375\n",
      "2017-09-26T14:18:58.435418: step 939, loss 0.0990728, acc 0.953125\n",
      "2017-09-26T14:18:58.702765: step 940, loss 0.0459741, acc 0.984375\n",
      "2017-09-26T14:18:58.989474: step 941, loss 0.0328079, acc 1\n",
      "2017-09-26T14:18:59.257281: step 942, loss 0.0264225, acc 1\n",
      "2017-09-26T14:18:59.494735: step 943, loss 0.0296814, acc 1\n",
      "2017-09-26T14:18:59.745223: step 944, loss 0.0184061, acc 1\n",
      "2017-09-26T14:19:00.007197: step 945, loss 0.0279157, acc 1\n",
      "2017-09-26T14:19:00.252712: step 946, loss 0.0186061, acc 1\n",
      "2017-09-26T14:19:00.504918: step 947, loss 0.0326176, acc 1\n",
      "2017-09-26T14:19:00.746831: step 948, loss 0.127657, acc 0.953125\n",
      "2017-09-26T14:19:00.998274: step 949, loss 0.0141442, acc 1\n",
      "2017-09-26T14:19:01.261874: step 950, loss 0.0211202, acc 1\n",
      "2017-09-26T14:19:01.501980: step 951, loss 0.0180841, acc 1\n",
      "2017-09-26T14:19:01.747107: step 952, loss 0.023532, acc 1\n",
      "2017-09-26T14:19:01.999499: step 953, loss 0.0121505, acc 1\n",
      "2017-09-26T14:19:02.251413: step 954, loss 0.0283071, acc 1\n",
      "2017-09-26T14:19:02.507451: step 955, loss 0.0218169, acc 1\n",
      "2017-09-26T14:19:02.746966: step 956, loss 0.11313, acc 0.96875\n",
      "2017-09-26T14:19:02.988492: step 957, loss 0.0568647, acc 0.96875\n",
      "2017-09-26T14:19:03.227595: step 958, loss 0.0248328, acc 1\n",
      "2017-09-26T14:19:03.475268: step 959, loss 0.0281061, acc 0.984375\n",
      "2017-09-26T14:19:03.720077: step 960, loss 0.0687414, acc 0.96875\n",
      "2017-09-26T14:19:03.958371: step 961, loss 0.0197964, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:19:04.233200: step 962, loss 0.0176882, acc 1\n",
      "2017-09-26T14:19:04.475774: step 963, loss 0.0115047, acc 1\n",
      "2017-09-26T14:19:04.723410: step 964, loss 0.118243, acc 0.96875\n",
      "2017-09-26T14:19:04.960794: step 965, loss 0.0161534, acc 1\n",
      "2017-09-26T14:19:05.165892: step 966, loss 0.0373314, acc 1\n",
      "2017-09-26T14:19:05.432908: step 967, loss 0.0148941, acc 1\n",
      "2017-09-26T14:19:05.687147: step 968, loss 0.0205347, acc 1\n",
      "2017-09-26T14:19:05.961485: step 969, loss 0.0859139, acc 0.953125\n",
      "2017-09-26T14:19:06.204705: step 970, loss 0.0341652, acc 0.96875\n",
      "2017-09-26T14:19:06.463801: step 971, loss 0.00770119, acc 1\n",
      "2017-09-26T14:19:06.706718: step 972, loss 0.0242494, acc 1\n",
      "2017-09-26T14:19:06.946289: step 973, loss 0.018503, acc 1\n",
      "2017-09-26T14:19:07.179851: step 974, loss 0.0480549, acc 0.984375\n",
      "2017-09-26T14:19:07.422118: step 975, loss 0.0345679, acc 1\n",
      "2017-09-26T14:19:07.672810: step 976, loss 0.015545, acc 1\n",
      "2017-09-26T14:19:07.909742: step 977, loss 0.0158446, acc 1\n",
      "2017-09-26T14:19:08.145817: step 978, loss 0.00663718, acc 1\n",
      "2017-09-26T14:19:08.379968: step 979, loss 0.0300393, acc 1\n",
      "2017-09-26T14:19:08.611743: step 980, loss 0.0672717, acc 0.984375\n",
      "2017-09-26T14:19:08.849529: step 981, loss 0.0525401, acc 0.984375\n",
      "2017-09-26T14:19:09.085166: step 982, loss 0.036818, acc 0.984375\n",
      "2017-09-26T14:19:09.321629: step 983, loss 0.0300773, acc 1\n",
      "2017-09-26T14:19:09.553376: step 984, loss 0.0335527, acc 0.984375\n",
      "2017-09-26T14:19:09.786317: step 985, loss 0.0430787, acc 0.984375\n",
      "2017-09-26T14:19:10.020229: step 986, loss 0.0464805, acc 0.96875\n",
      "2017-09-26T14:19:10.260769: step 987, loss 0.0632377, acc 0.96875\n",
      "2017-09-26T14:19:10.513927: step 988, loss 0.0450889, acc 0.984375\n",
      "2017-09-26T14:19:10.747489: step 989, loss 0.0227957, acc 1\n",
      "2017-09-26T14:19:10.977635: step 990, loss 0.0136558, acc 1\n",
      "2017-09-26T14:19:11.210143: step 991, loss 0.0408687, acc 1\n",
      "2017-09-26T14:19:11.472645: step 992, loss 0.0693694, acc 0.96875\n",
      "2017-09-26T14:19:11.713283: step 993, loss 0.0039262, acc 1\n",
      "2017-09-26T14:19:11.946234: step 994, loss 0.00686737, acc 1\n",
      "2017-09-26T14:19:12.196288: step 995, loss 0.0305448, acc 0.984375\n",
      "2017-09-26T14:19:12.432424: step 996, loss 0.0337127, acc 0.984375\n",
      "2017-09-26T14:19:12.663250: step 997, loss 0.0329954, acc 0.984375\n",
      "2017-09-26T14:19:12.897095: step 998, loss 0.00778744, acc 1\n",
      "2017-09-26T14:19:13.132358: step 999, loss 0.0332321, acc 0.984375\n",
      "2017-09-26T14:19:13.362452: step 1000, loss 0.00743486, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:19:13.591395: step 1000, loss 0.431657, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1000\n",
      "\n",
      "2017-09-26T14:19:14.092947: step 1001, loss 0.0114246, acc 1\n",
      "2017-09-26T14:19:14.325824: step 1002, loss 0.0174132, acc 1\n",
      "2017-09-26T14:19:14.609389: step 1003, loss 0.01388, acc 1\n",
      "2017-09-26T14:19:14.889543: step 1004, loss 0.0164058, acc 1\n",
      "2017-09-26T14:19:15.140809: step 1005, loss 0.00950701, acc 1\n",
      "2017-09-26T14:19:15.384734: step 1006, loss 0.0432594, acc 0.984375\n",
      "2017-09-26T14:19:15.623704: step 1007, loss 0.0522501, acc 0.984375\n",
      "2017-09-26T14:19:15.836039: step 1008, loss 0.0093407, acc 1\n",
      "2017-09-26T14:19:16.109496: step 1009, loss 0.0608772, acc 0.984375\n",
      "2017-09-26T14:19:16.345662: step 1010, loss 0.0209152, acc 1\n",
      "2017-09-26T14:19:16.578460: step 1011, loss 0.0373295, acc 0.984375\n",
      "2017-09-26T14:19:16.846800: step 1012, loss 0.0220257, acc 1\n",
      "2017-09-26T14:19:17.084035: step 1013, loss 0.0133641, acc 1\n",
      "2017-09-26T14:19:17.318904: step 1014, loss 0.0367381, acc 0.984375\n",
      "2017-09-26T14:19:17.589511: step 1015, loss 0.0340283, acc 1\n",
      "2017-09-26T14:19:17.859765: step 1016, loss 0.0538074, acc 0.984375\n",
      "2017-09-26T14:19:18.146297: step 1017, loss 0.0198308, acc 1\n",
      "2017-09-26T14:19:18.402410: step 1018, loss 0.0283061, acc 1\n",
      "2017-09-26T14:19:18.641860: step 1019, loss 0.0176472, acc 1\n",
      "2017-09-26T14:19:18.895996: step 1020, loss 0.0580256, acc 0.96875\n",
      "2017-09-26T14:19:19.144780: step 1021, loss 0.0361186, acc 0.984375\n",
      "2017-09-26T14:19:19.383310: step 1022, loss 0.0215123, acc 1\n",
      "2017-09-26T14:19:19.625941: step 1023, loss 0.00752435, acc 1\n",
      "2017-09-26T14:19:19.863005: step 1024, loss 0.0340812, acc 0.984375\n",
      "2017-09-26T14:19:20.109259: step 1025, loss 0.0406502, acc 0.96875\n",
      "2017-09-26T14:19:20.353901: step 1026, loss 0.0390436, acc 0.984375\n",
      "2017-09-26T14:19:20.590832: step 1027, loss 0.0511177, acc 0.96875\n",
      "2017-09-26T14:19:20.830484: step 1028, loss 0.00809753, acc 1\n",
      "2017-09-26T14:19:21.071407: step 1029, loss 0.00981491, acc 1\n",
      "2017-09-26T14:19:21.309267: step 1030, loss 0.0295905, acc 0.984375\n",
      "2017-09-26T14:19:21.561398: step 1031, loss 0.0693806, acc 0.96875\n",
      "2017-09-26T14:19:21.805864: step 1032, loss 0.00734183, acc 1\n",
      "2017-09-26T14:19:22.043421: step 1033, loss 0.0566135, acc 0.984375\n",
      "2017-09-26T14:19:22.277841: step 1034, loss 0.0386984, acc 0.984375\n",
      "2017-09-26T14:19:22.510983: step 1035, loss 0.00847824, acc 1\n",
      "2017-09-26T14:19:22.756906: step 1036, loss 0.0124339, acc 1\n",
      "2017-09-26T14:19:23.023019: step 1037, loss 0.0214266, acc 1\n",
      "2017-09-26T14:19:23.252737: step 1038, loss 0.0529295, acc 0.984375\n",
      "2017-09-26T14:19:23.497401: step 1039, loss 0.021808, acc 1\n",
      "2017-09-26T14:19:23.751700: step 1040, loss 0.0170607, acc 1\n",
      "2017-09-26T14:19:23.984110: step 1041, loss 0.00517835, acc 1\n",
      "2017-09-26T14:19:24.220912: step 1042, loss 0.0327938, acc 0.984375\n",
      "2017-09-26T14:19:24.455948: step 1043, loss 0.0511618, acc 0.984375\n",
      "2017-09-26T14:19:24.695305: step 1044, loss 0.0492984, acc 0.984375\n",
      "2017-09-26T14:19:24.929502: step 1045, loss 0.0172053, acc 1\n",
      "2017-09-26T14:19:25.167026: step 1046, loss 0.021151, acc 1\n",
      "2017-09-26T14:19:25.407477: step 1047, loss 0.031853, acc 0.984375\n",
      "2017-09-26T14:19:25.638551: step 1048, loss 0.0367511, acc 0.984375\n",
      "2017-09-26T14:19:25.875695: step 1049, loss 0.0101548, acc 1\n",
      "2017-09-26T14:19:26.093443: step 1050, loss 0.0284614, acc 1\n",
      "2017-09-26T14:19:26.332852: step 1051, loss 0.0164365, acc 1\n",
      "2017-09-26T14:19:26.574187: step 1052, loss 0.0299219, acc 1\n",
      "2017-09-26T14:19:26.813782: step 1053, loss 0.0323982, acc 0.984375\n",
      "2017-09-26T14:19:27.050639: step 1054, loss 0.00683843, acc 1\n",
      "2017-09-26T14:19:27.289526: step 1055, loss 0.0985393, acc 0.96875\n",
      "2017-09-26T14:19:27.524458: step 1056, loss 0.011614, acc 1\n",
      "2017-09-26T14:19:27.758441: step 1057, loss 0.0305639, acc 0.984375\n",
      "2017-09-26T14:19:27.994016: step 1058, loss 0.010741, acc 1\n",
      "2017-09-26T14:19:28.228759: step 1059, loss 0.0420222, acc 0.984375\n",
      "2017-09-26T14:19:28.481307: step 1060, loss 0.0299068, acc 1\n",
      "2017-09-26T14:19:28.714237: step 1061, loss 0.027936, acc 0.984375\n",
      "2017-09-26T14:19:28.960976: step 1062, loss 0.0164519, acc 1\n",
      "2017-09-26T14:19:29.214908: step 1063, loss 0.0780894, acc 0.953125\n",
      "2017-09-26T14:19:29.451019: step 1064, loss 0.0426636, acc 0.984375\n",
      "2017-09-26T14:19:29.688670: step 1065, loss 0.00414425, acc 1\n",
      "2017-09-26T14:19:29.946358: step 1066, loss 0.0234872, acc 0.984375\n",
      "2017-09-26T14:19:30.221021: step 1067, loss 0.0415894, acc 0.984375\n",
      "2017-09-26T14:19:30.480824: step 1068, loss 0.0138248, acc 1\n",
      "2017-09-26T14:19:30.719446: step 1069, loss 0.0269015, acc 0.984375\n",
      "2017-09-26T14:19:30.963000: step 1070, loss 0.0444591, acc 0.984375\n",
      "2017-09-26T14:19:31.210977: step 1071, loss 0.0144178, acc 1\n",
      "2017-09-26T14:19:31.453805: step 1072, loss 0.0714913, acc 0.96875\n",
      "2017-09-26T14:19:31.696728: step 1073, loss 0.0152986, acc 1\n",
      "2017-09-26T14:19:31.971398: step 1074, loss 0.0135596, acc 1\n",
      "2017-09-26T14:19:32.217961: step 1075, loss 0.0142423, acc 1\n",
      "2017-09-26T14:19:32.468389: step 1076, loss 0.0986879, acc 0.984375\n",
      "2017-09-26T14:19:32.753681: step 1077, loss 0.0311777, acc 0.984375\n",
      "2017-09-26T14:19:33.032031: step 1078, loss 0.0853034, acc 0.96875\n",
      "2017-09-26T14:19:33.295560: step 1079, loss 0.0171948, acc 1\n",
      "2017-09-26T14:19:33.545389: step 1080, loss 0.00800618, acc 1\n",
      "2017-09-26T14:19:33.797831: step 1081, loss 0.0185145, acc 1\n",
      "2017-09-26T14:19:34.038312: step 1082, loss 0.0305573, acc 1\n",
      "2017-09-26T14:19:34.280247: step 1083, loss 0.0108514, acc 1\n",
      "2017-09-26T14:19:34.523515: step 1084, loss 0.0520001, acc 0.984375\n",
      "2017-09-26T14:19:34.783320: step 1085, loss 0.0094302, acc 1\n",
      "2017-09-26T14:19:35.023913: step 1086, loss 0.0215577, acc 1\n",
      "2017-09-26T14:19:35.266449: step 1087, loss 0.0253958, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:19:35.505909: step 1088, loss 0.0532705, acc 0.96875\n",
      "2017-09-26T14:19:35.749952: step 1089, loss 0.0164007, acc 1\n",
      "2017-09-26T14:19:35.979247: step 1090, loss 0.0338879, acc 1\n",
      "2017-09-26T14:19:36.218243: step 1091, loss 0.0232241, acc 0.984375\n",
      "2017-09-26T14:19:36.419477: step 1092, loss 0.0120766, acc 1\n",
      "2017-09-26T14:19:36.658538: step 1093, loss 0.0429727, acc 0.984375\n",
      "2017-09-26T14:19:36.898504: step 1094, loss 0.00431266, acc 1\n",
      "2017-09-26T14:19:37.134234: step 1095, loss 0.0120808, acc 1\n",
      "2017-09-26T14:19:37.367999: step 1096, loss 0.00629404, acc 1\n",
      "2017-09-26T14:19:37.596063: step 1097, loss 0.015466, acc 1\n",
      "2017-09-26T14:19:37.841052: step 1098, loss 0.00463691, acc 1\n",
      "2017-09-26T14:19:38.079858: step 1099, loss 0.00966632, acc 1\n",
      "2017-09-26T14:19:38.312559: step 1100, loss 0.0464256, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:19:38.544855: step 1100, loss 0.423443, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1100\n",
      "\n",
      "2017-09-26T14:19:39.037739: step 1101, loss 0.0135448, acc 1\n",
      "2017-09-26T14:19:39.268205: step 1102, loss 0.0601563, acc 0.984375\n",
      "2017-09-26T14:19:39.500986: step 1103, loss 0.0163756, acc 1\n",
      "2017-09-26T14:19:39.742832: step 1104, loss 0.0185051, acc 1\n",
      "2017-09-26T14:19:40.010723: step 1105, loss 0.00778518, acc 1\n",
      "2017-09-26T14:19:40.246092: step 1106, loss 0.0377916, acc 1\n",
      "2017-09-26T14:19:40.478270: step 1107, loss 0.011837, acc 1\n",
      "2017-09-26T14:19:40.710033: step 1108, loss 0.0538144, acc 0.984375\n",
      "2017-09-26T14:19:40.943143: step 1109, loss 0.0374627, acc 0.984375\n",
      "2017-09-26T14:19:41.181404: step 1110, loss 0.0336225, acc 0.984375\n",
      "2017-09-26T14:19:41.415622: step 1111, loss 0.064799, acc 0.96875\n",
      "2017-09-26T14:19:41.650077: step 1112, loss 0.0145731, acc 1\n",
      "2017-09-26T14:19:41.892643: step 1113, loss 0.0702846, acc 0.953125\n",
      "2017-09-26T14:19:42.132170: step 1114, loss 0.00903708, acc 1\n",
      "2017-09-26T14:19:42.366199: step 1115, loss 0.0263252, acc 0.984375\n",
      "2017-09-26T14:19:42.598575: step 1116, loss 0.0111585, acc 1\n",
      "2017-09-26T14:19:42.841294: step 1117, loss 0.0201264, acc 1\n",
      "2017-09-26T14:19:43.075739: step 1118, loss 0.024218, acc 1\n",
      "2017-09-26T14:19:43.320754: step 1119, loss 0.0186346, acc 1\n",
      "2017-09-26T14:19:43.553166: step 1120, loss 0.0323207, acc 1\n",
      "2017-09-26T14:19:43.786700: step 1121, loss 0.0164653, acc 1\n",
      "2017-09-26T14:19:44.022211: step 1122, loss 0.0497817, acc 0.984375\n",
      "2017-09-26T14:19:44.263600: step 1123, loss 0.0326368, acc 0.984375\n",
      "2017-09-26T14:19:44.504014: step 1124, loss 0.0122585, acc 1\n",
      "2017-09-26T14:19:44.735408: step 1125, loss 0.0343788, acc 0.984375\n",
      "2017-09-26T14:19:44.973381: step 1126, loss 0.0286254, acc 1\n",
      "2017-09-26T14:19:45.230418: step 1127, loss 0.0856423, acc 0.984375\n",
      "2017-09-26T14:19:45.468764: step 1128, loss 0.0381904, acc 0.984375\n",
      "2017-09-26T14:19:45.701291: step 1129, loss 0.0407089, acc 0.984375\n",
      "2017-09-26T14:19:45.935372: step 1130, loss 0.047047, acc 0.984375\n",
      "2017-09-26T14:19:46.220921: step 1131, loss 0.0189739, acc 0.984375\n",
      "2017-09-26T14:19:46.457705: step 1132, loss 0.0165286, acc 1\n",
      "2017-09-26T14:19:46.691359: step 1133, loss 0.0227248, acc 0.984375\n",
      "2017-09-26T14:19:46.899877: step 1134, loss 0.0122059, acc 1\n",
      "2017-09-26T14:19:47.160167: step 1135, loss 0.0116998, acc 1\n",
      "2017-09-26T14:19:47.453751: step 1136, loss 0.011248, acc 1\n",
      "2017-09-26T14:19:47.713409: step 1137, loss 0.0121334, acc 1\n",
      "2017-09-26T14:19:47.951330: step 1138, loss 0.0722019, acc 0.953125\n",
      "2017-09-26T14:19:48.198039: step 1139, loss 0.0361738, acc 1\n",
      "2017-09-26T14:19:48.444133: step 1140, loss 0.0254183, acc 0.984375\n",
      "2017-09-26T14:19:48.696647: step 1141, loss 0.0482013, acc 0.96875\n",
      "2017-09-26T14:19:48.935516: step 1142, loss 0.0124402, acc 1\n",
      "2017-09-26T14:19:49.200045: step 1143, loss 0.00683174, acc 1\n",
      "2017-09-26T14:19:49.485465: step 1144, loss 0.0135277, acc 1\n",
      "2017-09-26T14:19:49.760175: step 1145, loss 0.0043088, acc 1\n",
      "2017-09-26T14:19:49.997938: step 1146, loss 0.0325693, acc 0.984375\n",
      "2017-09-26T14:19:50.259146: step 1147, loss 0.0175205, acc 0.984375\n",
      "2017-09-26T14:19:50.493121: step 1148, loss 0.00723295, acc 1\n",
      "2017-09-26T14:19:50.731636: step 1149, loss 0.0288109, acc 0.984375\n",
      "2017-09-26T14:19:50.984080: step 1150, loss 0.0148251, acc 1\n",
      "2017-09-26T14:19:51.231296: step 1151, loss 0.00482974, acc 1\n",
      "2017-09-26T14:19:51.477927: step 1152, loss 0.0349109, acc 0.984375\n",
      "2017-09-26T14:19:51.714219: step 1153, loss 0.00763941, acc 1\n",
      "2017-09-26T14:19:51.965155: step 1154, loss 0.0290451, acc 0.984375\n",
      "2017-09-26T14:19:52.200938: step 1155, loss 0.0769049, acc 0.953125\n",
      "2017-09-26T14:19:52.481855: step 1156, loss 0.0212522, acc 0.984375\n",
      "2017-09-26T14:19:52.716685: step 1157, loss 0.0128004, acc 1\n",
      "2017-09-26T14:19:52.966606: step 1158, loss 0.0325324, acc 0.984375\n",
      "2017-09-26T14:19:53.204206: step 1159, loss 0.0640739, acc 0.984375\n",
      "2017-09-26T14:19:53.457937: step 1160, loss 0.00818143, acc 1\n",
      "2017-09-26T14:19:53.694616: step 1161, loss 0.0228496, acc 0.984375\n",
      "2017-09-26T14:19:53.932266: step 1162, loss 0.0456445, acc 0.96875\n",
      "2017-09-26T14:19:54.222127: step 1163, loss 0.0328428, acc 1\n",
      "2017-09-26T14:19:54.487913: step 1164, loss 0.0089029, acc 1\n",
      "2017-09-26T14:19:54.753211: step 1165, loss 0.0057311, acc 1\n",
      "2017-09-26T14:19:54.995845: step 1166, loss 0.0199335, acc 1\n",
      "2017-09-26T14:19:55.233267: step 1167, loss 0.00942122, acc 1\n",
      "2017-09-26T14:19:55.482474: step 1168, loss 0.00815314, acc 1\n",
      "2017-09-26T14:19:55.716187: step 1169, loss 0.0216714, acc 1\n",
      "2017-09-26T14:19:55.963520: step 1170, loss 0.00782577, acc 1\n",
      "2017-09-26T14:19:56.227160: step 1171, loss 0.0152227, acc 1\n",
      "2017-09-26T14:19:56.466414: step 1172, loss 0.0149325, acc 1\n",
      "2017-09-26T14:19:56.769687: step 1173, loss 0.0124733, acc 1\n",
      "2017-09-26T14:19:57.011993: step 1174, loss 0.0759991, acc 0.984375\n",
      "2017-09-26T14:19:57.244205: step 1175, loss 0.098628, acc 0.984375\n",
      "2017-09-26T14:19:57.451026: step 1176, loss 0.0871046, acc 0.980769\n",
      "2017-09-26T14:19:57.694784: step 1177, loss 0.0224632, acc 1\n",
      "2017-09-26T14:19:57.941940: step 1178, loss 0.00457371, acc 1\n",
      "2017-09-26T14:19:58.249139: step 1179, loss 0.00499332, acc 1\n",
      "2017-09-26T14:19:58.507119: step 1180, loss 0.0178184, acc 1\n",
      "2017-09-26T14:19:58.752921: step 1181, loss 0.0382617, acc 1\n",
      "2017-09-26T14:19:58.998718: step 1182, loss 0.0303084, acc 1\n",
      "2017-09-26T14:19:59.242506: step 1183, loss 0.0060011, acc 1\n",
      "2017-09-26T14:19:59.482529: step 1184, loss 0.0108061, acc 1\n",
      "2017-09-26T14:19:59.727592: step 1185, loss 0.0372923, acc 0.984375\n",
      "2017-09-26T14:19:59.977614: step 1186, loss 0.0881266, acc 0.984375\n",
      "2017-09-26T14:20:00.235124: step 1187, loss 0.00732019, acc 1\n",
      "2017-09-26T14:20:00.467769: step 1188, loss 0.0139086, acc 1\n",
      "2017-09-26T14:20:00.719322: step 1189, loss 0.013536, acc 1\n",
      "2017-09-26T14:20:00.994255: step 1190, loss 0.0145622, acc 1\n",
      "2017-09-26T14:20:01.235621: step 1191, loss 0.00595548, acc 1\n",
      "2017-09-26T14:20:01.473957: step 1192, loss 0.0171022, acc 1\n",
      "2017-09-26T14:20:01.726600: step 1193, loss 0.0113469, acc 1\n",
      "2017-09-26T14:20:01.965816: step 1194, loss 0.0218952, acc 0.984375\n",
      "2017-09-26T14:20:02.219438: step 1195, loss 0.0204154, acc 1\n",
      "2017-09-26T14:20:02.469055: step 1196, loss 0.0198402, acc 1\n",
      "2017-09-26T14:20:02.722460: step 1197, loss 0.00998965, acc 1\n",
      "2017-09-26T14:20:02.962546: step 1198, loss 0.00925098, acc 1\n",
      "2017-09-26T14:20:03.207255: step 1199, loss 0.0133107, acc 1\n",
      "2017-09-26T14:20:03.444367: step 1200, loss 0.00567297, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:20:03.679837: step 1200, loss 0.413027, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1200\n",
      "\n",
      "2017-09-26T14:20:04.198752: step 1201, loss 0.0515763, acc 0.984375\n",
      "2017-09-26T14:20:04.449868: step 1202, loss 0.162688, acc 0.953125\n",
      "2017-09-26T14:20:04.695577: step 1203, loss 0.0186202, acc 1\n",
      "2017-09-26T14:20:04.943116: step 1204, loss 0.00997726, acc 1\n",
      "2017-09-26T14:20:05.182821: step 1205, loss 0.0427096, acc 0.984375\n",
      "2017-09-26T14:20:05.436831: step 1206, loss 0.0303553, acc 1\n",
      "2017-09-26T14:20:05.673512: step 1207, loss 0.0170067, acc 1\n",
      "2017-09-26T14:20:05.924304: step 1208, loss 0.00596903, acc 1\n",
      "2017-09-26T14:20:06.167093: step 1209, loss 0.00943974, acc 1\n",
      "2017-09-26T14:20:06.413444: step 1210, loss 0.0737774, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:20:06.651994: step 1211, loss 0.0149148, acc 1\n",
      "2017-09-26T14:20:06.893403: step 1212, loss 0.0337349, acc 0.984375\n",
      "2017-09-26T14:20:07.138838: step 1213, loss 0.0110776, acc 1\n",
      "2017-09-26T14:20:07.378756: step 1214, loss 0.0148725, acc 1\n",
      "2017-09-26T14:20:07.617734: step 1215, loss 0.0303644, acc 0.984375\n",
      "2017-09-26T14:20:07.886766: step 1216, loss 0.0743876, acc 0.96875\n",
      "2017-09-26T14:20:08.136151: step 1217, loss 0.0536559, acc 0.984375\n",
      "2017-09-26T14:20:08.342790: step 1218, loss 0.00463115, acc 1\n",
      "2017-09-26T14:20:08.580615: step 1219, loss 0.120816, acc 0.953125\n",
      "2017-09-26T14:20:08.826655: step 1220, loss 0.0148282, acc 1\n",
      "2017-09-26T14:20:09.066795: step 1221, loss 0.00399482, acc 1\n",
      "2017-09-26T14:20:09.312346: step 1222, loss 0.0118966, acc 1\n",
      "2017-09-26T14:20:09.542204: step 1223, loss 0.0605829, acc 0.96875\n",
      "2017-09-26T14:20:09.783077: step 1224, loss 0.0164395, acc 1\n",
      "2017-09-26T14:20:10.017118: step 1225, loss 0.01028, acc 1\n",
      "2017-09-26T14:20:10.261683: step 1226, loss 0.0188042, acc 1\n",
      "2017-09-26T14:20:10.496382: step 1227, loss 0.0222721, acc 0.984375\n",
      "2017-09-26T14:20:10.730176: step 1228, loss 0.0207692, acc 1\n",
      "2017-09-26T14:20:10.969427: step 1229, loss 0.0144331, acc 1\n",
      "2017-09-26T14:20:11.227039: step 1230, loss 0.014858, acc 1\n",
      "2017-09-26T14:20:11.507412: step 1231, loss 0.0270166, acc 0.984375\n",
      "2017-09-26T14:20:11.758360: step 1232, loss 0.00825668, acc 1\n",
      "2017-09-26T14:20:12.036629: step 1233, loss 0.0208328, acc 1\n",
      "2017-09-26T14:20:12.312510: step 1234, loss 0.0111154, acc 1\n",
      "2017-09-26T14:20:12.577248: step 1235, loss 0.0110116, acc 1\n",
      "2017-09-26T14:20:12.832340: step 1236, loss 0.00640223, acc 1\n",
      "2017-09-26T14:20:13.079678: step 1237, loss 0.00724111, acc 1\n",
      "2017-09-26T14:20:13.342680: step 1238, loss 0.0709486, acc 0.984375\n",
      "2017-09-26T14:20:13.587662: step 1239, loss 0.00763278, acc 1\n",
      "2017-09-26T14:20:13.851536: step 1240, loss 0.0126762, acc 1\n",
      "2017-09-26T14:20:14.113613: step 1241, loss 0.0134769, acc 1\n",
      "2017-09-26T14:20:14.390078: step 1242, loss 0.0457127, acc 0.984375\n",
      "2017-09-26T14:20:14.643375: step 1243, loss 0.0107068, acc 1\n",
      "2017-09-26T14:20:14.898840: step 1244, loss 0.00586034, acc 1\n",
      "2017-09-26T14:20:15.154814: step 1245, loss 0.0211021, acc 1\n",
      "2017-09-26T14:20:15.416449: step 1246, loss 0.0275622, acc 1\n",
      "2017-09-26T14:20:15.673057: step 1247, loss 0.00891999, acc 1\n",
      "2017-09-26T14:20:15.933323: step 1248, loss 0.0217677, acc 1\n",
      "2017-09-26T14:20:16.171034: step 1249, loss 0.0267312, acc 1\n",
      "2017-09-26T14:20:16.408645: step 1250, loss 0.00989904, acc 1\n",
      "2017-09-26T14:20:16.643833: step 1251, loss 0.0408448, acc 0.984375\n",
      "2017-09-26T14:20:16.902351: step 1252, loss 0.0206177, acc 1\n",
      "2017-09-26T14:20:17.142554: step 1253, loss 0.013212, acc 1\n",
      "2017-09-26T14:20:17.404441: step 1254, loss 0.0321215, acc 0.984375\n",
      "2017-09-26T14:20:17.669711: step 1255, loss 0.011576, acc 1\n",
      "2017-09-26T14:20:17.935921: step 1256, loss 0.00568981, acc 1\n",
      "2017-09-26T14:20:18.200174: step 1257, loss 0.0121103, acc 1\n",
      "2017-09-26T14:20:18.458465: step 1258, loss 0.00403924, acc 1\n",
      "2017-09-26T14:20:18.726318: step 1259, loss 0.014994, acc 1\n",
      "2017-09-26T14:20:18.948713: step 1260, loss 0.0307213, acc 0.980769\n",
      "2017-09-26T14:20:19.222423: step 1261, loss 0.00341487, acc 1\n",
      "2017-09-26T14:20:19.505474: step 1262, loss 0.00350824, acc 1\n",
      "2017-09-26T14:20:19.770579: step 1263, loss 0.0252886, acc 0.984375\n",
      "2017-09-26T14:20:20.018041: step 1264, loss 0.0384009, acc 1\n",
      "2017-09-26T14:20:20.281470: step 1265, loss 0.0249633, acc 1\n",
      "2017-09-26T14:20:20.521167: step 1266, loss 0.0309832, acc 0.984375\n",
      "2017-09-26T14:20:20.800930: step 1267, loss 0.0442625, acc 0.96875\n",
      "2017-09-26T14:20:21.035882: step 1268, loss 0.0204397, acc 1\n",
      "2017-09-26T14:20:21.282066: step 1269, loss 0.039532, acc 0.96875\n",
      "2017-09-26T14:20:21.531403: step 1270, loss 0.0273311, acc 0.984375\n",
      "2017-09-26T14:20:21.779872: step 1271, loss 0.00617706, acc 1\n",
      "2017-09-26T14:20:22.035516: step 1272, loss 0.0430147, acc 0.984375\n",
      "2017-09-26T14:20:22.295244: step 1273, loss 0.00712679, acc 1\n",
      "2017-09-26T14:20:22.563085: step 1274, loss 0.0127028, acc 1\n",
      "2017-09-26T14:20:22.812224: step 1275, loss 0.0102617, acc 1\n",
      "2017-09-26T14:20:23.080781: step 1276, loss 0.0187364, acc 0.984375\n",
      "2017-09-26T14:20:23.317097: step 1277, loss 0.0105797, acc 1\n",
      "2017-09-26T14:20:23.552054: step 1278, loss 0.0149924, acc 1\n",
      "2017-09-26T14:20:23.800341: step 1279, loss 0.00307537, acc 1\n",
      "2017-09-26T14:20:24.051652: step 1280, loss 0.00472546, acc 1\n",
      "2017-09-26T14:20:24.334674: step 1281, loss 0.0107797, acc 1\n",
      "2017-09-26T14:20:24.589648: step 1282, loss 0.0318673, acc 1\n",
      "2017-09-26T14:20:24.844317: step 1283, loss 0.0264998, acc 0.984375\n",
      "2017-09-26T14:20:25.097704: step 1284, loss 0.061287, acc 0.984375\n",
      "2017-09-26T14:20:25.366925: step 1285, loss 0.00292757, acc 1\n",
      "2017-09-26T14:20:25.625354: step 1286, loss 0.00665921, acc 1\n",
      "2017-09-26T14:20:25.873643: step 1287, loss 0.0495571, acc 0.984375\n",
      "2017-09-26T14:20:26.130940: step 1288, loss 0.0397953, acc 0.984375\n",
      "2017-09-26T14:20:26.388156: step 1289, loss 0.0258697, acc 1\n",
      "2017-09-26T14:20:26.621640: step 1290, loss 0.0163875, acc 1\n",
      "2017-09-26T14:20:26.862518: step 1291, loss 0.0231892, acc 1\n",
      "2017-09-26T14:20:27.121733: step 1292, loss 0.0208662, acc 0.984375\n",
      "2017-09-26T14:20:27.366000: step 1293, loss 0.0135758, acc 1\n",
      "2017-09-26T14:20:27.630455: step 1294, loss 0.00988146, acc 1\n",
      "2017-09-26T14:20:27.911616: step 1295, loss 0.0121716, acc 1\n",
      "2017-09-26T14:20:28.181521: step 1296, loss 0.0380229, acc 0.984375\n",
      "2017-09-26T14:20:28.439987: step 1297, loss 0.0262843, acc 0.984375\n",
      "2017-09-26T14:20:28.690464: step 1298, loss 0.00995851, acc 1\n",
      "2017-09-26T14:20:28.938838: step 1299, loss 0.0279484, acc 0.984375\n",
      "2017-09-26T14:20:29.223942: step 1300, loss 0.00380865, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:20:29.449073: step 1300, loss 0.463582, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1300\n",
      "\n",
      "2017-09-26T14:20:29.973633: step 1301, loss 0.00491266, acc 1\n",
      "2017-09-26T14:20:30.195521: step 1302, loss 0.0134118, acc 1\n",
      "2017-09-26T14:20:30.462445: step 1303, loss 0.0238247, acc 0.984375\n",
      "2017-09-26T14:20:30.719575: step 1304, loss 0.0110666, acc 1\n",
      "2017-09-26T14:20:30.973934: step 1305, loss 0.0252656, acc 0.984375\n",
      "2017-09-26T14:20:31.241117: step 1306, loss 0.00875447, acc 1\n",
      "2017-09-26T14:20:31.472658: step 1307, loss 0.00949613, acc 1\n",
      "2017-09-26T14:20:31.706897: step 1308, loss 0.0233935, acc 1\n",
      "2017-09-26T14:20:31.972122: step 1309, loss 0.00832682, acc 1\n",
      "2017-09-26T14:20:32.225914: step 1310, loss 0.0101312, acc 1\n",
      "2017-09-26T14:20:32.483271: step 1311, loss 0.0201821, acc 1\n",
      "2017-09-26T14:20:32.746046: step 1312, loss 0.0323842, acc 0.984375\n",
      "2017-09-26T14:20:33.031431: step 1313, loss 0.00362617, acc 1\n",
      "2017-09-26T14:20:33.287734: step 1314, loss 0.00454503, acc 1\n",
      "2017-09-26T14:20:33.559577: step 1315, loss 0.0208351, acc 0.984375\n",
      "2017-09-26T14:20:33.812294: step 1316, loss 0.00781599, acc 1\n",
      "2017-09-26T14:20:34.085150: step 1317, loss 0.0127216, acc 1\n",
      "2017-09-26T14:20:34.342177: step 1318, loss 0.0233679, acc 1\n",
      "2017-09-26T14:20:34.616730: step 1319, loss 0.0439595, acc 0.984375\n",
      "2017-09-26T14:20:34.879258: step 1320, loss 0.028138, acc 0.984375\n",
      "2017-09-26T14:20:35.147402: step 1321, loss 0.00770111, acc 1\n",
      "2017-09-26T14:20:35.412221: step 1322, loss 0.0110577, acc 1\n",
      "2017-09-26T14:20:35.672638: step 1323, loss 0.00942391, acc 1\n",
      "2017-09-26T14:20:35.959411: step 1324, loss 0.0751831, acc 0.984375\n",
      "2017-09-26T14:20:36.224702: step 1325, loss 0.0132377, acc 1\n",
      "2017-09-26T14:20:36.474921: step 1326, loss 0.0471025, acc 0.96875\n",
      "2017-09-26T14:20:36.730440: step 1327, loss 0.00913675, acc 1\n",
      "2017-09-26T14:20:36.979497: step 1328, loss 0.0464539, acc 0.984375\n",
      "2017-09-26T14:20:37.238790: step 1329, loss 0.00449965, acc 1\n",
      "2017-09-26T14:20:37.506973: step 1330, loss 0.00423765, acc 1\n",
      "2017-09-26T14:20:37.766107: step 1331, loss 0.00401939, acc 1\n",
      "2017-09-26T14:20:38.034840: step 1332, loss 0.00735716, acc 1\n",
      "2017-09-26T14:20:38.293457: step 1333, loss 0.00683939, acc 1\n",
      "2017-09-26T14:20:38.579202: step 1334, loss 0.0374673, acc 0.984375\n",
      "2017-09-26T14:20:38.872884: step 1335, loss 0.00953635, acc 1\n",
      "2017-09-26T14:20:39.122254: step 1336, loss 0.022343, acc 1\n",
      "2017-09-26T14:20:39.437026: step 1337, loss 0.0284641, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:20:39.766408: step 1338, loss 0.0171243, acc 1\n",
      "2017-09-26T14:20:40.121203: step 1339, loss 0.00693751, acc 1\n",
      "2017-09-26T14:20:40.455742: step 1340, loss 0.00717077, acc 1\n",
      "2017-09-26T14:20:40.732415: step 1341, loss 0.0399414, acc 0.96875\n",
      "2017-09-26T14:20:40.986194: step 1342, loss 0.0231314, acc 1\n",
      "2017-09-26T14:20:41.239239: step 1343, loss 0.0199423, acc 1\n",
      "2017-09-26T14:20:41.458089: step 1344, loss 0.0195255, acc 1\n",
      "2017-09-26T14:20:41.717189: step 1345, loss 0.0151494, acc 1\n",
      "2017-09-26T14:20:41.976659: step 1346, loss 0.0365747, acc 0.984375\n",
      "2017-09-26T14:20:42.243542: step 1347, loss 0.0188174, acc 1\n",
      "2017-09-26T14:20:42.496938: step 1348, loss 0.00759895, acc 1\n",
      "2017-09-26T14:20:42.737101: step 1349, loss 0.0111716, acc 1\n",
      "2017-09-26T14:20:42.998447: step 1350, loss 0.00662781, acc 1\n",
      "2017-09-26T14:20:43.228941: step 1351, loss 0.00639958, acc 1\n",
      "2017-09-26T14:20:43.459007: step 1352, loss 0.0550915, acc 0.984375\n",
      "2017-09-26T14:20:43.693926: step 1353, loss 0.0131477, acc 1\n",
      "2017-09-26T14:20:43.929259: step 1354, loss 0.0108403, acc 1\n",
      "2017-09-26T14:20:44.169265: step 1355, loss 0.0231253, acc 0.984375\n",
      "2017-09-26T14:20:44.413088: step 1356, loss 0.0150585, acc 1\n",
      "2017-09-26T14:20:44.650925: step 1357, loss 0.0258089, acc 0.984375\n",
      "2017-09-26T14:20:44.885259: step 1358, loss 0.0346822, acc 0.984375\n",
      "2017-09-26T14:20:45.117104: step 1359, loss 0.148928, acc 0.984375\n",
      "2017-09-26T14:20:45.363081: step 1360, loss 0.0609934, acc 0.96875\n",
      "2017-09-26T14:20:45.601736: step 1361, loss 0.0179327, acc 1\n",
      "2017-09-26T14:20:45.829627: step 1362, loss 0.00719074, acc 1\n",
      "2017-09-26T14:20:46.060043: step 1363, loss 0.0117134, acc 1\n",
      "2017-09-26T14:20:46.294767: step 1364, loss 0.0217662, acc 1\n",
      "2017-09-26T14:20:46.529404: step 1365, loss 0.044008, acc 0.984375\n",
      "2017-09-26T14:20:46.762165: step 1366, loss 0.0105064, acc 1\n",
      "2017-09-26T14:20:46.996679: step 1367, loss 0.00490785, acc 1\n",
      "2017-09-26T14:20:47.238641: step 1368, loss 0.0111857, acc 1\n",
      "2017-09-26T14:20:47.476585: step 1369, loss 0.0215652, acc 1\n",
      "2017-09-26T14:20:47.703669: step 1370, loss 0.00884224, acc 1\n",
      "2017-09-26T14:20:47.946989: step 1371, loss 0.0192821, acc 0.984375\n",
      "2017-09-26T14:20:48.185232: step 1372, loss 0.0640721, acc 0.984375\n",
      "2017-09-26T14:20:48.419635: step 1373, loss 0.0405742, acc 0.984375\n",
      "2017-09-26T14:20:48.653219: step 1374, loss 0.0317747, acc 0.984375\n",
      "2017-09-26T14:20:48.890442: step 1375, loss 0.0260915, acc 0.984375\n",
      "2017-09-26T14:20:49.125929: step 1376, loss 0.00590341, acc 1\n",
      "2017-09-26T14:20:49.358527: step 1377, loss 0.0130622, acc 1\n",
      "2017-09-26T14:20:49.591940: step 1378, loss 0.0136388, acc 1\n",
      "2017-09-26T14:20:49.831667: step 1379, loss 0.0168765, acc 1\n",
      "2017-09-26T14:20:50.063482: step 1380, loss 0.00894214, acc 1\n",
      "2017-09-26T14:20:50.297205: step 1381, loss 0.0112112, acc 1\n",
      "2017-09-26T14:20:50.554949: step 1382, loss 0.0192022, acc 1\n",
      "2017-09-26T14:20:50.793225: step 1383, loss 0.00675681, acc 1\n",
      "2017-09-26T14:20:51.029784: step 1384, loss 0.00549353, acc 1\n",
      "2017-09-26T14:20:51.264573: step 1385, loss 0.0302135, acc 0.984375\n",
      "2017-09-26T14:20:51.472754: step 1386, loss 0.00496726, acc 1\n",
      "2017-09-26T14:20:51.741738: step 1387, loss 0.0349266, acc 0.984375\n",
      "2017-09-26T14:20:51.984596: step 1388, loss 0.0218374, acc 1\n",
      "2017-09-26T14:20:52.225695: step 1389, loss 0.0134909, acc 1\n",
      "2017-09-26T14:20:52.469933: step 1390, loss 0.034461, acc 0.984375\n",
      "2017-09-26T14:20:52.737383: step 1391, loss 0.0146519, acc 1\n",
      "2017-09-26T14:20:52.974155: step 1392, loss 0.00938535, acc 1\n",
      "2017-09-26T14:20:53.208311: step 1393, loss 0.0139941, acc 1\n",
      "2017-09-26T14:20:53.443464: step 1394, loss 0.0541739, acc 0.984375\n",
      "2017-09-26T14:20:53.681635: step 1395, loss 0.0140614, acc 1\n",
      "2017-09-26T14:20:53.912583: step 1396, loss 0.00431476, acc 1\n",
      "2017-09-26T14:20:54.145414: step 1397, loss 0.00834429, acc 1\n",
      "2017-09-26T14:20:54.379732: step 1398, loss 0.0178665, acc 1\n",
      "2017-09-26T14:20:54.610778: step 1399, loss 0.0041154, acc 1\n",
      "2017-09-26T14:20:54.845660: step 1400, loss 0.0687119, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:20:55.076122: step 1400, loss 0.45048, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1400\n",
      "\n",
      "2017-09-26T14:20:55.574726: step 1401, loss 0.00928245, acc 1\n",
      "2017-09-26T14:20:55.819773: step 1402, loss 0.0257603, acc 0.984375\n",
      "2017-09-26T14:20:56.052462: step 1403, loss 0.00386593, acc 1\n",
      "2017-09-26T14:20:56.288668: step 1404, loss 0.0172516, acc 1\n",
      "2017-09-26T14:20:56.527006: step 1405, loss 0.0405178, acc 0.984375\n",
      "2017-09-26T14:20:56.757855: step 1406, loss 0.0171644, acc 0.984375\n",
      "2017-09-26T14:20:57.009060: step 1407, loss 0.00527205, acc 1\n",
      "2017-09-26T14:20:57.266087: step 1408, loss 0.0105202, acc 1\n",
      "2017-09-26T14:20:57.500050: step 1409, loss 0.00545385, acc 1\n",
      "2017-09-26T14:20:57.745485: step 1410, loss 0.00804786, acc 1\n",
      "2017-09-26T14:20:58.004004: step 1411, loss 0.00570362, acc 1\n",
      "2017-09-26T14:20:58.237390: step 1412, loss 0.0202718, acc 1\n",
      "2017-09-26T14:20:58.473122: step 1413, loss 0.0732793, acc 0.984375\n",
      "2017-09-26T14:20:58.729065: step 1414, loss 0.00457328, acc 1\n",
      "2017-09-26T14:20:58.975284: step 1415, loss 0.0198141, acc 1\n",
      "2017-09-26T14:20:59.220822: step 1416, loss 0.00644278, acc 1\n",
      "2017-09-26T14:20:59.475889: step 1417, loss 0.00649725, acc 1\n",
      "2017-09-26T14:20:59.720725: step 1418, loss 0.00952721, acc 1\n",
      "2017-09-26T14:20:59.966370: step 1419, loss 0.0274847, acc 0.984375\n",
      "2017-09-26T14:21:00.207638: step 1420, loss 0.0139914, acc 1\n",
      "2017-09-26T14:21:00.462213: step 1421, loss 0.00550883, acc 1\n",
      "2017-09-26T14:21:00.696603: step 1422, loss 0.0160295, acc 1\n",
      "2017-09-26T14:21:00.931902: step 1423, loss 0.078371, acc 0.984375\n",
      "2017-09-26T14:21:01.172418: step 1424, loss 0.00435703, acc 1\n",
      "2017-09-26T14:21:01.407293: step 1425, loss 0.0521115, acc 0.984375\n",
      "2017-09-26T14:21:01.639477: step 1426, loss 0.0103557, acc 1\n",
      "2017-09-26T14:21:01.883383: step 1427, loss 0.00875309, acc 1\n",
      "2017-09-26T14:21:02.118731: step 1428, loss 0.0128059, acc 1\n",
      "2017-09-26T14:21:02.361182: step 1429, loss 0.0558085, acc 0.984375\n",
      "2017-09-26T14:21:02.613525: step 1430, loss 0.0267182, acc 0.984375\n",
      "2017-09-26T14:21:02.850019: step 1431, loss 0.0100086, acc 1\n",
      "2017-09-26T14:21:03.089935: step 1432, loss 0.0204289, acc 1\n",
      "2017-09-26T14:21:03.343554: step 1433, loss 0.00603393, acc 1\n",
      "2017-09-26T14:21:03.576275: step 1434, loss 0.00499816, acc 1\n",
      "2017-09-26T14:21:03.814797: step 1435, loss 0.00724701, acc 1\n",
      "2017-09-26T14:21:04.054645: step 1436, loss 0.0139377, acc 1\n",
      "2017-09-26T14:21:04.296433: step 1437, loss 0.025252, acc 0.984375\n",
      "2017-09-26T14:21:04.535527: step 1438, loss 0.0116924, acc 1\n",
      "2017-09-26T14:21:04.785672: step 1439, loss 0.0117557, acc 1\n",
      "2017-09-26T14:21:05.029205: step 1440, loss 0.00564441, acc 1\n",
      "2017-09-26T14:21:05.270750: step 1441, loss 0.0132282, acc 1\n",
      "2017-09-26T14:21:05.505655: step 1442, loss 0.022573, acc 1\n",
      "2017-09-26T14:21:05.743974: step 1443, loss 0.035878, acc 0.984375\n",
      "2017-09-26T14:21:05.989399: step 1444, loss 0.0101101, acc 1\n",
      "2017-09-26T14:21:06.226520: step 1445, loss 0.0169268, acc 1\n",
      "2017-09-26T14:21:06.464021: step 1446, loss 0.0645935, acc 0.984375\n",
      "2017-09-26T14:21:06.703540: step 1447, loss 0.018472, acc 0.984375\n",
      "2017-09-26T14:21:06.942866: step 1448, loss 0.0122508, acc 1\n",
      "2017-09-26T14:21:07.180254: step 1449, loss 0.00721145, acc 1\n",
      "2017-09-26T14:21:07.422006: step 1450, loss 0.0113021, acc 1\n",
      "2017-09-26T14:21:07.673356: step 1451, loss 0.00721845, acc 1\n",
      "2017-09-26T14:21:07.909715: step 1452, loss 0.00995363, acc 1\n",
      "2017-09-26T14:21:08.149837: step 1453, loss 0.00396549, acc 1\n",
      "2017-09-26T14:21:08.389718: step 1454, loss 0.0316495, acc 0.984375\n",
      "2017-09-26T14:21:08.635206: step 1455, loss 0.0102904, acc 1\n",
      "2017-09-26T14:21:08.877220: step 1456, loss 0.0290508, acc 0.984375\n",
      "2017-09-26T14:21:09.112562: step 1457, loss 0.0194166, acc 1\n",
      "2017-09-26T14:21:09.355959: step 1458, loss 0.0122878, acc 1\n",
      "2017-09-26T14:21:09.594145: step 1459, loss 0.0161867, acc 0.984375\n",
      "2017-09-26T14:21:09.836335: step 1460, loss 0.0144763, acc 1\n",
      "2017-09-26T14:21:10.077988: step 1461, loss 0.0382602, acc 0.984375\n",
      "2017-09-26T14:21:10.315860: step 1462, loss 0.00521965, acc 1\n",
      "2017-09-26T14:21:10.558015: step 1463, loss 0.0118383, acc 1\n",
      "2017-09-26T14:21:10.801404: step 1464, loss 0.00997577, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:21:11.060408: step 1465, loss 0.00468373, acc 1\n",
      "2017-09-26T14:21:11.324608: step 1466, loss 0.00403595, acc 1\n",
      "2017-09-26T14:21:11.560272: step 1467, loss 0.0111866, acc 1\n",
      "2017-09-26T14:21:11.792877: step 1468, loss 0.00794535, acc 1\n",
      "2017-09-26T14:21:12.027585: step 1469, loss 0.00257967, acc 1\n",
      "2017-09-26T14:21:12.232249: step 1470, loss 0.015577, acc 1\n",
      "2017-09-26T14:21:12.477896: step 1471, loss 0.00191053, acc 1\n",
      "2017-09-26T14:21:12.713538: step 1472, loss 0.00717568, acc 1\n",
      "2017-09-26T14:21:12.944933: step 1473, loss 0.00128302, acc 1\n",
      "2017-09-26T14:21:13.179049: step 1474, loss 0.00884807, acc 1\n",
      "2017-09-26T14:21:13.413315: step 1475, loss 0.00620496, acc 1\n",
      "2017-09-26T14:21:13.647766: step 1476, loss 0.0278089, acc 0.984375\n",
      "2017-09-26T14:21:13.883846: step 1477, loss 0.00477297, acc 1\n",
      "2017-09-26T14:21:14.122561: step 1478, loss 0.00893035, acc 1\n",
      "2017-09-26T14:21:14.365831: step 1479, loss 0.0151662, acc 1\n",
      "2017-09-26T14:21:14.598759: step 1480, loss 0.00485652, acc 1\n",
      "2017-09-26T14:21:14.862339: step 1481, loss 0.0112973, acc 1\n",
      "2017-09-26T14:21:15.095911: step 1482, loss 0.00139513, acc 1\n",
      "2017-09-26T14:21:15.332092: step 1483, loss 0.00949101, acc 1\n",
      "2017-09-26T14:21:15.569929: step 1484, loss 0.072443, acc 0.96875\n",
      "2017-09-26T14:21:15.858259: step 1485, loss 0.0472546, acc 0.984375\n",
      "2017-09-26T14:21:16.147659: step 1486, loss 0.00904455, acc 1\n",
      "2017-09-26T14:21:16.389302: step 1487, loss 0.0072548, acc 1\n",
      "2017-09-26T14:21:16.627998: step 1488, loss 0.00782428, acc 1\n",
      "2017-09-26T14:21:16.868139: step 1489, loss 0.0034003, acc 1\n",
      "2017-09-26T14:21:17.103120: step 1490, loss 0.00410427, acc 1\n",
      "2017-09-26T14:21:17.345187: step 1491, loss 0.0059369, acc 1\n",
      "2017-09-26T14:21:17.586267: step 1492, loss 0.0400939, acc 0.984375\n",
      "2017-09-26T14:21:17.832885: step 1493, loss 0.00493883, acc 1\n",
      "2017-09-26T14:21:18.111503: step 1494, loss 0.00624206, acc 1\n",
      "2017-09-26T14:21:18.371326: step 1495, loss 0.0072115, acc 1\n",
      "2017-09-26T14:21:18.637339: step 1496, loss 0.00618799, acc 1\n",
      "2017-09-26T14:21:18.924932: step 1497, loss 0.0128927, acc 1\n",
      "2017-09-26T14:21:19.163980: step 1498, loss 0.0384661, acc 0.984375\n",
      "2017-09-26T14:21:19.416708: step 1499, loss 0.068234, acc 0.984375\n",
      "2017-09-26T14:21:19.671011: step 1500, loss 0.016524, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:21:19.916911: step 1500, loss 0.451742, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1500\n",
      "\n",
      "2017-09-26T14:21:20.440799: step 1501, loss 0.00814774, acc 1\n",
      "2017-09-26T14:21:20.704173: step 1502, loss 0.00490218, acc 1\n",
      "2017-09-26T14:21:20.950229: step 1503, loss 0.00494633, acc 1\n",
      "2017-09-26T14:21:21.181981: step 1504, loss 0.00629255, acc 1\n",
      "2017-09-26T14:21:21.411935: step 1505, loss 0.0182922, acc 1\n",
      "2017-09-26T14:21:21.640666: step 1506, loss 0.0172174, acc 1\n",
      "2017-09-26T14:21:21.918396: step 1507, loss 0.00529237, acc 1\n",
      "2017-09-26T14:21:22.178522: step 1508, loss 0.0107992, acc 1\n",
      "2017-09-26T14:21:22.421740: step 1509, loss 0.00601716, acc 1\n",
      "2017-09-26T14:21:22.661478: step 1510, loss 0.0635763, acc 0.984375\n",
      "2017-09-26T14:21:22.896991: step 1511, loss 0.0193657, acc 1\n",
      "2017-09-26T14:21:23.109489: step 1512, loss 0.00871242, acc 1\n",
      "2017-09-26T14:21:23.355133: step 1513, loss 0.00332888, acc 1\n",
      "2017-09-26T14:21:23.591374: step 1514, loss 0.00896852, acc 1\n",
      "2017-09-26T14:21:23.822665: step 1515, loss 0.00542343, acc 1\n",
      "2017-09-26T14:21:24.056050: step 1516, loss 0.0307005, acc 0.96875\n",
      "2017-09-26T14:21:24.294373: step 1517, loss 0.00796149, acc 1\n",
      "2017-09-26T14:21:24.527646: step 1518, loss 0.00486052, acc 1\n",
      "2017-09-26T14:21:24.785350: step 1519, loss 0.00515678, acc 1\n",
      "2017-09-26T14:21:25.017946: step 1520, loss 0.00424658, acc 1\n",
      "2017-09-26T14:21:25.268593: step 1521, loss 0.0104963, acc 1\n",
      "2017-09-26T14:21:25.503416: step 1522, loss 0.0136354, acc 1\n",
      "2017-09-26T14:21:25.738563: step 1523, loss 0.0107681, acc 1\n",
      "2017-09-26T14:21:25.982833: step 1524, loss 0.0054637, acc 1\n",
      "2017-09-26T14:21:26.251843: step 1525, loss 0.00484202, acc 1\n",
      "2017-09-26T14:21:26.487408: step 1526, loss 0.0129016, acc 1\n",
      "2017-09-26T14:21:26.722807: step 1527, loss 0.0138089, acc 1\n",
      "2017-09-26T14:21:26.960327: step 1528, loss 0.0213105, acc 0.984375\n",
      "2017-09-26T14:21:27.192491: step 1529, loss 0.00565092, acc 1\n",
      "2017-09-26T14:21:27.433409: step 1530, loss 0.00863238, acc 1\n",
      "2017-09-26T14:21:27.669474: step 1531, loss 0.00229866, acc 1\n",
      "2017-09-26T14:21:27.905022: step 1532, loss 0.0154259, acc 1\n",
      "2017-09-26T14:21:28.139685: step 1533, loss 0.0150031, acc 1\n",
      "2017-09-26T14:21:28.376348: step 1534, loss 0.00445201, acc 1\n",
      "2017-09-26T14:21:28.612372: step 1535, loss 0.00842445, acc 1\n",
      "2017-09-26T14:21:28.857388: step 1536, loss 0.00320258, acc 1\n",
      "2017-09-26T14:21:29.110660: step 1537, loss 0.0738454, acc 0.984375\n",
      "2017-09-26T14:21:29.381860: step 1538, loss 0.0112189, acc 1\n",
      "2017-09-26T14:21:29.677367: step 1539, loss 0.0055585, acc 1\n",
      "2017-09-26T14:21:29.927912: step 1540, loss 0.00431811, acc 1\n",
      "2017-09-26T14:21:30.168138: step 1541, loss 0.023195, acc 0.984375\n",
      "2017-09-26T14:21:30.410640: step 1542, loss 0.0132356, acc 1\n",
      "2017-09-26T14:21:30.650439: step 1543, loss 0.00246243, acc 1\n",
      "2017-09-26T14:21:30.886428: step 1544, loss 0.0128307, acc 1\n",
      "2017-09-26T14:21:31.129030: step 1545, loss 0.00970171, acc 1\n",
      "2017-09-26T14:21:31.369238: step 1546, loss 0.00419936, acc 1\n",
      "2017-09-26T14:21:31.622602: step 1547, loss 0.00476309, acc 1\n",
      "2017-09-26T14:21:31.878874: step 1548, loss 0.00992818, acc 1\n",
      "2017-09-26T14:21:32.110116: step 1549, loss 0.0125828, acc 1\n",
      "2017-09-26T14:21:32.358943: step 1550, loss 0.00773089, acc 1\n",
      "2017-09-26T14:21:32.593331: step 1551, loss 0.00862978, acc 1\n",
      "2017-09-26T14:21:32.856714: step 1552, loss 0.00689466, acc 1\n",
      "2017-09-26T14:21:33.093016: step 1553, loss 0.00307512, acc 1\n",
      "2017-09-26T14:21:33.320608: step 1554, loss 0.00620903, acc 1\n",
      "2017-09-26T14:21:33.561173: step 1555, loss 0.00753741, acc 1\n",
      "2017-09-26T14:21:33.800717: step 1556, loss 0.0160427, acc 1\n",
      "2017-09-26T14:21:34.043711: step 1557, loss 0.00212265, acc 1\n",
      "2017-09-26T14:21:34.281342: step 1558, loss 0.00931814, acc 1\n",
      "2017-09-26T14:21:34.536469: step 1559, loss 0.00600008, acc 1\n",
      "2017-09-26T14:21:34.778027: step 1560, loss 0.0121503, acc 1\n",
      "2017-09-26T14:21:35.025138: step 1561, loss 0.00427983, acc 1\n",
      "2017-09-26T14:21:35.273363: step 1562, loss 0.0257842, acc 1\n",
      "2017-09-26T14:21:35.512745: step 1563, loss 0.017537, acc 1\n",
      "2017-09-26T14:21:35.755068: step 1564, loss 0.0322978, acc 1\n",
      "2017-09-26T14:21:36.038283: step 1565, loss 0.0138752, acc 1\n",
      "2017-09-26T14:21:36.335452: step 1566, loss 0.00678128, acc 1\n",
      "2017-09-26T14:21:36.585943: step 1567, loss 0.00210825, acc 1\n",
      "2017-09-26T14:21:36.849199: step 1568, loss 0.0535922, acc 0.96875\n",
      "2017-09-26T14:21:37.111136: step 1569, loss 0.0211656, acc 1\n",
      "2017-09-26T14:21:37.368591: step 1570, loss 0.00405722, acc 1\n",
      "2017-09-26T14:21:37.631859: step 1571, loss 0.015026, acc 1\n",
      "2017-09-26T14:21:37.924874: step 1572, loss 0.00333674, acc 1\n",
      "2017-09-26T14:21:38.181349: step 1573, loss 0.085049, acc 0.96875\n",
      "2017-09-26T14:21:38.449758: step 1574, loss 0.0145225, acc 1\n",
      "2017-09-26T14:21:38.696159: step 1575, loss 0.00850943, acc 1\n",
      "2017-09-26T14:21:38.939315: step 1576, loss 0.0119365, acc 1\n",
      "2017-09-26T14:21:39.187674: step 1577, loss 0.0237837, acc 0.984375\n",
      "2017-09-26T14:21:39.446431: step 1578, loss 0.0118911, acc 1\n",
      "2017-09-26T14:21:39.693769: step 1579, loss 0.00400703, acc 1\n",
      "2017-09-26T14:21:39.937344: step 1580, loss 0.0056648, acc 1\n",
      "2017-09-26T14:21:40.179147: step 1581, loss 0.00693499, acc 1\n",
      "2017-09-26T14:21:40.452275: step 1582, loss 0.00523833, acc 1\n",
      "2017-09-26T14:21:40.696320: step 1583, loss 0.00734721, acc 1\n",
      "2017-09-26T14:21:40.944850: step 1584, loss 0.00754044, acc 1\n",
      "2017-09-26T14:21:41.189771: step 1585, loss 0.0248908, acc 0.984375\n",
      "2017-09-26T14:21:41.452498: step 1586, loss 0.0130801, acc 1\n",
      "2017-09-26T14:21:41.685913: step 1587, loss 0.00870796, acc 1\n",
      "2017-09-26T14:21:41.941799: step 1588, loss 0.00861137, acc 1\n",
      "2017-09-26T14:21:42.183411: step 1589, loss 0.0165052, acc 1\n",
      "2017-09-26T14:21:42.422919: step 1590, loss 0.00892145, acc 1\n",
      "2017-09-26T14:21:42.668827: step 1591, loss 0.00375548, acc 1\n",
      "2017-09-26T14:21:42.911911: step 1592, loss 0.0588606, acc 0.984375\n",
      "2017-09-26T14:21:43.153464: step 1593, loss 0.0239106, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:21:43.397209: step 1594, loss 0.00709031, acc 1\n",
      "2017-09-26T14:21:43.641494: step 1595, loss 0.00670417, acc 1\n",
      "2017-09-26T14:21:43.864845: step 1596, loss 0.0387468, acc 0.980769\n",
      "2017-09-26T14:21:44.118639: step 1597, loss 0.00657664, acc 1\n",
      "2017-09-26T14:21:44.361908: step 1598, loss 0.00718063, acc 1\n",
      "2017-09-26T14:21:44.599218: step 1599, loss 0.00431233, acc 1\n",
      "2017-09-26T14:21:44.826108: step 1600, loss 0.00400307, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:21:45.066373: step 1600, loss 0.45996, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1600\n",
      "\n",
      "2017-09-26T14:21:45.555799: step 1601, loss 0.0245357, acc 0.984375\n",
      "2017-09-26T14:21:45.787031: step 1602, loss 0.0050715, acc 1\n",
      "2017-09-26T14:21:46.028272: step 1603, loss 0.0188091, acc 0.984375\n",
      "2017-09-26T14:21:46.292535: step 1604, loss 0.00829271, acc 1\n",
      "2017-09-26T14:21:46.520430: step 1605, loss 0.00218937, acc 1\n",
      "2017-09-26T14:21:46.751789: step 1606, loss 0.00976632, acc 1\n",
      "2017-09-26T14:21:46.980076: step 1607, loss 0.00384966, acc 1\n",
      "2017-09-26T14:21:47.216058: step 1608, loss 0.00946066, acc 1\n",
      "2017-09-26T14:21:47.449678: step 1609, loss 0.00413339, acc 1\n",
      "2017-09-26T14:21:47.682188: step 1610, loss 0.00422724, acc 1\n",
      "2017-09-26T14:21:47.910390: step 1611, loss 0.021571, acc 0.984375\n",
      "2017-09-26T14:21:48.144660: step 1612, loss 0.00856063, acc 1\n",
      "2017-09-26T14:21:48.387900: step 1613, loss 0.00843826, acc 1\n",
      "2017-09-26T14:21:48.616652: step 1614, loss 0.0126437, acc 1\n",
      "2017-09-26T14:21:48.850933: step 1615, loss 0.0205513, acc 0.984375\n",
      "2017-09-26T14:21:49.082659: step 1616, loss 0.0223578, acc 0.984375\n",
      "2017-09-26T14:21:49.319256: step 1617, loss 0.00285033, acc 1\n",
      "2017-09-26T14:21:49.550613: step 1618, loss 0.059065, acc 0.984375\n",
      "2017-09-26T14:21:49.782793: step 1619, loss 0.0428379, acc 0.984375\n",
      "2017-09-26T14:21:50.020354: step 1620, loss 0.00923469, acc 1\n",
      "2017-09-26T14:21:50.254173: step 1621, loss 0.0224894, acc 0.984375\n",
      "2017-09-26T14:21:50.489906: step 1622, loss 0.0339116, acc 0.984375\n",
      "2017-09-26T14:21:50.736644: step 1623, loss 0.0240615, acc 0.984375\n",
      "2017-09-26T14:21:50.977000: step 1624, loss 0.0157584, acc 1\n",
      "2017-09-26T14:21:51.235244: step 1625, loss 0.00576814, acc 1\n",
      "2017-09-26T14:21:51.493414: step 1626, loss 0.00859263, acc 1\n",
      "2017-09-26T14:21:51.797484: step 1627, loss 0.00603803, acc 1\n",
      "2017-09-26T14:21:52.099765: step 1628, loss 0.0108969, acc 1\n",
      "2017-09-26T14:21:52.391472: step 1629, loss 0.00841115, acc 1\n",
      "2017-09-26T14:21:52.677542: step 1630, loss 0.0126213, acc 1\n",
      "2017-09-26T14:21:52.951654: step 1631, loss 0.0279235, acc 0.984375\n",
      "2017-09-26T14:21:53.253937: step 1632, loss 0.0494975, acc 0.984375\n",
      "2017-09-26T14:21:53.565511: step 1633, loss 0.00375523, acc 1\n",
      "2017-09-26T14:21:53.868723: step 1634, loss 0.00993245, acc 1\n",
      "2017-09-26T14:21:54.166407: step 1635, loss 0.00326394, acc 1\n",
      "2017-09-26T14:21:54.471359: step 1636, loss 0.00686811, acc 1\n",
      "2017-09-26T14:21:54.757694: step 1637, loss 0.020544, acc 1\n",
      "2017-09-26T14:21:55.019256: step 1638, loss 0.0283, acc 1\n",
      "2017-09-26T14:21:55.334224: step 1639, loss 0.00271498, acc 1\n",
      "2017-09-26T14:21:55.621898: step 1640, loss 0.0170579, acc 1\n",
      "2017-09-26T14:21:55.966520: step 1641, loss 0.0172648, acc 1\n",
      "2017-09-26T14:21:56.272251: step 1642, loss 0.00858121, acc 1\n",
      "2017-09-26T14:21:56.560398: step 1643, loss 0.0156113, acc 1\n",
      "2017-09-26T14:21:56.869799: step 1644, loss 0.05492, acc 0.984375\n",
      "2017-09-26T14:21:57.175023: step 1645, loss 0.0360311, acc 0.984375\n",
      "2017-09-26T14:21:57.469225: step 1646, loss 0.0246735, acc 0.984375\n",
      "2017-09-26T14:21:57.795522: step 1647, loss 0.0213695, acc 0.984375\n",
      "2017-09-26T14:21:58.083266: step 1648, loss 0.00430445, acc 1\n",
      "2017-09-26T14:21:58.382890: step 1649, loss 0.0111108, acc 1\n",
      "2017-09-26T14:21:58.680195: step 1650, loss 0.0110848, acc 1\n",
      "2017-09-26T14:21:58.958407: step 1651, loss 0.0212365, acc 1\n",
      "2017-09-26T14:21:59.241387: step 1652, loss 0.00573651, acc 1\n",
      "2017-09-26T14:21:59.529456: step 1653, loss 0.00375907, acc 1\n",
      "2017-09-26T14:21:59.836620: step 1654, loss 0.0155704, acc 1\n",
      "2017-09-26T14:22:00.126920: step 1655, loss 0.0142008, acc 1\n",
      "2017-09-26T14:22:00.394924: step 1656, loss 0.0294468, acc 1\n",
      "2017-09-26T14:22:00.701793: step 1657, loss 0.00330478, acc 1\n",
      "2017-09-26T14:22:00.995406: step 1658, loss 0.00520081, acc 1\n",
      "2017-09-26T14:22:01.280453: step 1659, loss 0.0325704, acc 1\n",
      "2017-09-26T14:22:01.559338: step 1660, loss 0.0318887, acc 0.984375\n",
      "2017-09-26T14:22:01.843865: step 1661, loss 0.00898922, acc 1\n",
      "2017-09-26T14:22:02.105982: step 1662, loss 0.0263791, acc 0.984375\n",
      "2017-09-26T14:22:02.386555: step 1663, loss 0.0161028, acc 1\n",
      "2017-09-26T14:22:02.684047: step 1664, loss 0.00456304, acc 1\n",
      "2017-09-26T14:22:02.986313: step 1665, loss 0.00779331, acc 1\n",
      "2017-09-26T14:22:03.273178: step 1666, loss 0.00703724, acc 1\n",
      "2017-09-26T14:22:03.560257: step 1667, loss 0.0227312, acc 0.984375\n",
      "2017-09-26T14:22:03.831932: step 1668, loss 0.00574225, acc 1\n",
      "2017-09-26T14:22:04.090573: step 1669, loss 0.00630816, acc 1\n",
      "2017-09-26T14:22:04.374611: step 1670, loss 0.00863092, acc 1\n",
      "2017-09-26T14:22:04.666001: step 1671, loss 0.00875447, acc 1\n",
      "2017-09-26T14:22:04.941911: step 1672, loss 0.00588053, acc 1\n",
      "2017-09-26T14:22:05.180636: step 1673, loss 0.00573491, acc 1\n",
      "2017-09-26T14:22:05.443888: step 1674, loss 0.00597053, acc 1\n",
      "2017-09-26T14:22:05.711695: step 1675, loss 0.00903735, acc 1\n",
      "2017-09-26T14:22:05.971894: step 1676, loss 0.0160807, acc 1\n",
      "2017-09-26T14:22:06.212644: step 1677, loss 0.00760505, acc 1\n",
      "2017-09-26T14:22:06.519142: step 1678, loss 0.0435274, acc 0.984375\n",
      "2017-09-26T14:22:06.807570: step 1679, loss 0.00512286, acc 1\n",
      "2017-09-26T14:22:07.049389: step 1680, loss 0.0244713, acc 1\n",
      "2017-09-26T14:22:07.347397: step 1681, loss 0.00376614, acc 1\n",
      "2017-09-26T14:22:07.649725: step 1682, loss 0.00953078, acc 1\n",
      "2017-09-26T14:22:07.945841: step 1683, loss 0.00602532, acc 1\n",
      "2017-09-26T14:22:08.252605: step 1684, loss 0.00391594, acc 1\n",
      "2017-09-26T14:22:08.545781: step 1685, loss 0.00541222, acc 1\n",
      "2017-09-26T14:22:08.845066: step 1686, loss 0.00892056, acc 1\n",
      "2017-09-26T14:22:09.143047: step 1687, loss 0.00421277, acc 1\n",
      "2017-09-26T14:22:09.425212: step 1688, loss 0.00639008, acc 1\n",
      "2017-09-26T14:22:09.736805: step 1689, loss 0.0532425, acc 0.984375\n",
      "2017-09-26T14:22:10.038183: step 1690, loss 0.00410969, acc 1\n",
      "2017-09-26T14:22:10.329283: step 1691, loss 0.00307239, acc 1\n",
      "2017-09-26T14:22:10.643359: step 1692, loss 0.0350145, acc 0.984375\n",
      "2017-09-26T14:22:10.982464: step 1693, loss 0.0248689, acc 0.984375\n",
      "2017-09-26T14:22:11.272023: step 1694, loss 0.00406895, acc 1\n",
      "2017-09-26T14:22:11.560238: step 1695, loss 0.00218285, acc 1\n",
      "2017-09-26T14:22:11.856619: step 1696, loss 0.00192846, acc 1\n",
      "2017-09-26T14:22:12.216145: step 1697, loss 0.00320831, acc 1\n",
      "2017-09-26T14:22:12.571691: step 1698, loss 0.00178145, acc 1\n",
      "2017-09-26T14:22:12.897843: step 1699, loss 0.0125579, acc 1\n",
      "2017-09-26T14:22:13.198805: step 1700, loss 0.00842395, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:22:13.478705: step 1700, loss 0.489697, acc 0.878788\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1700\n",
      "\n",
      "2017-09-26T14:22:14.089696: step 1701, loss 0.0182429, acc 0.984375\n",
      "2017-09-26T14:22:14.350343: step 1702, loss 0.00435982, acc 1\n",
      "2017-09-26T14:22:14.633868: step 1703, loss 0.01633, acc 0.984375\n",
      "2017-09-26T14:22:14.897825: step 1704, loss 0.00844135, acc 1\n",
      "2017-09-26T14:22:15.163105: step 1705, loss 0.0108023, acc 1\n",
      "2017-09-26T14:22:15.424594: step 1706, loss 0.00487373, acc 1\n",
      "2017-09-26T14:22:15.742451: step 1707, loss 0.00975368, acc 1\n",
      "2017-09-26T14:22:16.078390: step 1708, loss 0.0135806, acc 1\n",
      "2017-09-26T14:22:16.348659: step 1709, loss 0.0146165, acc 1\n",
      "2017-09-26T14:22:16.676086: step 1710, loss 0.0180744, acc 0.984375\n",
      "2017-09-26T14:22:16.979514: step 1711, loss 0.00624799, acc 1\n",
      "2017-09-26T14:22:17.258289: step 1712, loss 0.00308644, acc 1\n",
      "2017-09-26T14:22:17.600433: step 1713, loss 0.0198462, acc 1\n",
      "2017-09-26T14:22:17.894679: step 1714, loss 0.00660123, acc 1\n",
      "2017-09-26T14:22:18.190166: step 1715, loss 0.00850034, acc 1\n",
      "2017-09-26T14:22:18.487880: step 1716, loss 0.0115494, acc 1\n",
      "2017-09-26T14:22:18.793879: step 1717, loss 0.00716794, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:22:19.108422: step 1718, loss 0.0393215, acc 0.96875\n",
      "2017-09-26T14:22:19.450997: step 1719, loss 0.05836, acc 0.984375\n",
      "2017-09-26T14:22:19.748416: step 1720, loss 0.025703, acc 0.984375\n",
      "2017-09-26T14:22:20.104487: step 1721, loss 0.0119282, acc 1\n",
      "2017-09-26T14:22:20.393732: step 1722, loss 0.0103159, acc 1\n",
      "2017-09-26T14:22:20.714827: step 1723, loss 0.0250169, acc 0.984375\n",
      "2017-09-26T14:22:21.048828: step 1724, loss 0.00169814, acc 1\n",
      "2017-09-26T14:22:21.348431: step 1725, loss 0.0093073, acc 1\n",
      "2017-09-26T14:22:21.627971: step 1726, loss 0.00582948, acc 1\n",
      "2017-09-26T14:22:21.921909: step 1727, loss 0.00244116, acc 1\n",
      "2017-09-26T14:22:22.213168: step 1728, loss 0.00960699, acc 1\n",
      "2017-09-26T14:22:22.512702: step 1729, loss 0.00516315, acc 1\n",
      "2017-09-26T14:22:22.827380: step 1730, loss 0.0144971, acc 1\n",
      "2017-09-26T14:22:23.120339: step 1731, loss 0.00186906, acc 1\n",
      "2017-09-26T14:22:23.429885: step 1732, loss 0.0240358, acc 0.984375\n",
      "2017-09-26T14:22:23.730596: step 1733, loss 0.00624413, acc 1\n",
      "2017-09-26T14:22:24.064756: step 1734, loss 0.00459291, acc 1\n",
      "2017-09-26T14:22:24.510221: step 1735, loss 0.0177354, acc 1\n",
      "2017-09-26T14:22:24.830321: step 1736, loss 0.00369464, acc 1\n",
      "2017-09-26T14:22:25.206223: step 1737, loss 0.00544622, acc 1\n",
      "2017-09-26T14:22:25.525229: step 1738, loss 0.00241137, acc 1\n",
      "2017-09-26T14:22:25.806720: step 1739, loss 0.00792153, acc 1\n",
      "2017-09-26T14:22:26.079618: step 1740, loss 0.00932797, acc 1\n",
      "2017-09-26T14:22:26.369505: step 1741, loss 0.0185354, acc 1\n",
      "2017-09-26T14:22:26.670745: step 1742, loss 0.049636, acc 0.984375\n",
      "2017-09-26T14:22:26.971168: step 1743, loss 0.00267011, acc 1\n",
      "2017-09-26T14:22:27.253833: step 1744, loss 0.0108924, acc 1\n",
      "2017-09-26T14:22:27.547334: step 1745, loss 0.00205056, acc 1\n",
      "2017-09-26T14:22:27.856556: step 1746, loss 0.00670236, acc 1\n",
      "2017-09-26T14:22:28.136985: step 1747, loss 0.00357185, acc 1\n",
      "2017-09-26T14:22:28.408331: step 1748, loss 0.0192243, acc 0.984375\n",
      "2017-09-26T14:22:28.675519: step 1749, loss 0.0211061, acc 0.984375\n",
      "2017-09-26T14:22:28.956939: step 1750, loss 0.00783203, acc 1\n",
      "2017-09-26T14:22:29.252777: step 1751, loss 0.00187209, acc 1\n",
      "2017-09-26T14:22:29.521482: step 1752, loss 0.0201533, acc 0.984375\n",
      "2017-09-26T14:22:29.818380: step 1753, loss 0.00184321, acc 1\n",
      "2017-09-26T14:22:30.119746: step 1754, loss 0.00519756, acc 1\n",
      "2017-09-26T14:22:30.436691: step 1755, loss 0.0694635, acc 0.953125\n",
      "2017-09-26T14:22:30.749441: step 1756, loss 0.00443224, acc 1\n",
      "2017-09-26T14:22:31.054877: step 1757, loss 0.00459352, acc 1\n",
      "2017-09-26T14:22:31.341796: step 1758, loss 0.00554516, acc 1\n",
      "2017-09-26T14:22:31.630354: step 1759, loss 0.0100255, acc 1\n",
      "2017-09-26T14:22:31.933792: step 1760, loss 0.00338156, acc 1\n",
      "2017-09-26T14:22:32.208932: step 1761, loss 0.00879601, acc 1\n",
      "2017-09-26T14:22:32.466171: step 1762, loss 0.00806854, acc 1\n",
      "2017-09-26T14:22:32.779546: step 1763, loss 0.041819, acc 0.984375\n",
      "2017-09-26T14:22:33.034343: step 1764, loss 0.00777765, acc 1\n",
      "2017-09-26T14:22:33.356058: step 1765, loss 0.0492783, acc 0.984375\n",
      "2017-09-26T14:22:33.665846: step 1766, loss 0.0143138, acc 1\n",
      "2017-09-26T14:22:33.971764: step 1767, loss 0.00367384, acc 1\n",
      "2017-09-26T14:22:34.251512: step 1768, loss 0.00575572, acc 1\n",
      "2017-09-26T14:22:34.560524: step 1769, loss 0.00400443, acc 1\n",
      "2017-09-26T14:22:34.883753: step 1770, loss 0.00264688, acc 1\n",
      "2017-09-26T14:22:35.186156: step 1771, loss 0.00512882, acc 1\n",
      "2017-09-26T14:22:35.480657: step 1772, loss 0.00565814, acc 1\n",
      "2017-09-26T14:22:35.766310: step 1773, loss 0.0278597, acc 0.984375\n",
      "2017-09-26T14:22:36.024338: step 1774, loss 0.0180943, acc 0.984375\n",
      "2017-09-26T14:22:36.287789: step 1775, loss 0.00460668, acc 1\n",
      "2017-09-26T14:22:36.557792: step 1776, loss 0.0320832, acc 0.984375\n",
      "2017-09-26T14:22:36.826990: step 1777, loss 0.00498387, acc 1\n",
      "2017-09-26T14:22:37.090148: step 1778, loss 0.0360438, acc 0.984375\n",
      "2017-09-26T14:22:37.403907: step 1779, loss 0.0119628, acc 1\n",
      "2017-09-26T14:22:37.695460: step 1780, loss 0.0071824, acc 1\n",
      "2017-09-26T14:22:37.983191: step 1781, loss 0.00411696, acc 1\n",
      "2017-09-26T14:22:38.282059: step 1782, loss 0.00318156, acc 1\n",
      "2017-09-26T14:22:38.540180: step 1783, loss 0.00967128, acc 1\n",
      "2017-09-26T14:22:38.825713: step 1784, loss 0.0203415, acc 0.984375\n",
      "2017-09-26T14:22:39.097307: step 1785, loss 0.00433183, acc 1\n",
      "2017-09-26T14:22:39.381429: step 1786, loss 0.00586295, acc 1\n",
      "2017-09-26T14:22:39.656412: step 1787, loss 0.00359829, acc 1\n",
      "2017-09-26T14:22:39.943703: step 1788, loss 0.0194881, acc 1\n",
      "2017-09-26T14:22:40.208673: step 1789, loss 0.0160936, acc 0.984375\n",
      "2017-09-26T14:22:40.457235: step 1790, loss 0.0026536, acc 1\n",
      "2017-09-26T14:22:40.735937: step 1791, loss 0.0016763, acc 1\n",
      "2017-09-26T14:22:41.034119: step 1792, loss 0.0203678, acc 0.984375\n",
      "2017-09-26T14:22:41.306019: step 1793, loss 0.00490947, acc 1\n",
      "2017-09-26T14:22:41.561394: step 1794, loss 0.0522354, acc 0.984375\n",
      "2017-09-26T14:22:41.854743: step 1795, loss 0.00365306, acc 1\n",
      "2017-09-26T14:22:42.119806: step 1796, loss 0.0201357, acc 0.984375\n",
      "2017-09-26T14:22:42.412543: step 1797, loss 0.0110145, acc 1\n",
      "2017-09-26T14:22:42.688980: step 1798, loss 0.00237685, acc 1\n",
      "2017-09-26T14:22:42.983695: step 1799, loss 0.0089425, acc 1\n",
      "2017-09-26T14:22:43.293976: step 1800, loss 0.00398481, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:22:43.553203: step 1800, loss 0.471898, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1800\n",
      "\n",
      "2017-09-26T14:22:44.216928: step 1801, loss 0.0110596, acc 1\n",
      "2017-09-26T14:22:44.479064: step 1802, loss 0.0161053, acc 1\n",
      "2017-09-26T14:22:44.794135: step 1803, loss 0.0119948, acc 1\n",
      "2017-09-26T14:22:45.075148: step 1804, loss 0.00540625, acc 1\n",
      "2017-09-26T14:22:45.374587: step 1805, loss 0.0191871, acc 1\n",
      "2017-09-26T14:22:45.601909: step 1806, loss 0.0142057, acc 1\n",
      "2017-09-26T14:22:45.925196: step 1807, loss 0.00171828, acc 1\n",
      "2017-09-26T14:22:46.232787: step 1808, loss 0.00930972, acc 1\n",
      "2017-09-26T14:22:46.518663: step 1809, loss 0.00437444, acc 1\n",
      "2017-09-26T14:22:46.816549: step 1810, loss 0.00211286, acc 1\n",
      "2017-09-26T14:22:47.109824: step 1811, loss 0.00649318, acc 1\n",
      "2017-09-26T14:22:47.398550: step 1812, loss 0.000793074, acc 1\n",
      "2017-09-26T14:22:47.701859: step 1813, loss 0.00269294, acc 1\n",
      "2017-09-26T14:22:48.020786: step 1814, loss 0.0363468, acc 0.984375\n",
      "2017-09-26T14:22:48.284991: step 1815, loss 0.0175927, acc 1\n",
      "2017-09-26T14:22:48.587992: step 1816, loss 0.00183252, acc 1\n",
      "2017-09-26T14:22:48.841934: step 1817, loss 0.00315891, acc 1\n",
      "2017-09-26T14:22:49.172063: step 1818, loss 0.00297805, acc 1\n",
      "2017-09-26T14:22:49.427905: step 1819, loss 0.002382, acc 1\n",
      "2017-09-26T14:22:49.727890: step 1820, loss 0.00893145, acc 1\n",
      "2017-09-26T14:22:50.001168: step 1821, loss 0.00437166, acc 1\n",
      "2017-09-26T14:22:50.289513: step 1822, loss 0.00164349, acc 1\n",
      "2017-09-26T14:22:50.608716: step 1823, loss 0.0148538, acc 1\n",
      "2017-09-26T14:22:50.935645: step 1824, loss 0.0059943, acc 1\n",
      "2017-09-26T14:22:51.201495: step 1825, loss 0.00190778, acc 1\n",
      "2017-09-26T14:22:51.492184: step 1826, loss 0.088237, acc 0.984375\n",
      "2017-09-26T14:22:51.776958: step 1827, loss 0.00237741, acc 1\n",
      "2017-09-26T14:22:52.072176: step 1828, loss 0.00166916, acc 1\n",
      "2017-09-26T14:22:52.377641: step 1829, loss 0.00570852, acc 1\n",
      "2017-09-26T14:22:52.641930: step 1830, loss 0.00579091, acc 1\n",
      "2017-09-26T14:22:52.926867: step 1831, loss 0.00714139, acc 1\n",
      "2017-09-26T14:22:53.239700: step 1832, loss 0.00334232, acc 1\n",
      "2017-09-26T14:22:53.536447: step 1833, loss 0.00532456, acc 1\n",
      "2017-09-26T14:22:53.917539: step 1834, loss 0.00879289, acc 1\n",
      "2017-09-26T14:22:54.254449: step 1835, loss 0.00312537, acc 1\n",
      "2017-09-26T14:22:54.581302: step 1836, loss 0.00404244, acc 1\n",
      "2017-09-26T14:22:54.919318: step 1837, loss 0.00829288, acc 1\n",
      "2017-09-26T14:22:55.210219: step 1838, loss 0.0017395, acc 1\n",
      "2017-09-26T14:22:55.470460: step 1839, loss 0.00319232, acc 1\n",
      "2017-09-26T14:22:55.733780: step 1840, loss 0.0455165, acc 0.984375\n",
      "2017-09-26T14:22:55.998269: step 1841, loss 0.00485588, acc 1\n",
      "2017-09-26T14:22:56.277384: step 1842, loss 0.00258358, acc 1\n",
      "2017-09-26T14:22:56.556943: step 1843, loss 0.00123704, acc 1\n",
      "2017-09-26T14:22:56.842213: step 1844, loss 0.00518121, acc 1\n",
      "2017-09-26T14:22:57.204558: step 1845, loss 0.0283549, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:22:57.522572: step 1846, loss 0.00705956, acc 1\n",
      "2017-09-26T14:22:57.875139: step 1847, loss 0.0521611, acc 0.984375\n",
      "2017-09-26T14:22:58.093820: step 1848, loss 0.00242732, acc 1\n",
      "2017-09-26T14:22:58.390924: step 1849, loss 0.00281062, acc 1\n",
      "2017-09-26T14:22:58.668349: step 1850, loss 0.00582579, acc 1\n",
      "2017-09-26T14:22:58.968535: step 1851, loss 0.00378338, acc 1\n",
      "2017-09-26T14:22:59.255865: step 1852, loss 0.00234341, acc 1\n",
      "2017-09-26T14:22:59.554690: step 1853, loss 0.00162759, acc 1\n",
      "2017-09-26T14:22:59.833316: step 1854, loss 0.0130725, acc 1\n",
      "2017-09-26T14:23:00.129926: step 1855, loss 0.00221115, acc 1\n",
      "2017-09-26T14:23:00.433474: step 1856, loss 0.0103467, acc 1\n",
      "2017-09-26T14:23:00.723132: step 1857, loss 0.0185782, acc 0.984375\n",
      "2017-09-26T14:23:01.027801: step 1858, loss 0.0122948, acc 1\n",
      "2017-09-26T14:23:01.338465: step 1859, loss 0.00472479, acc 1\n",
      "2017-09-26T14:23:01.661999: step 1860, loss 0.0342486, acc 0.96875\n",
      "2017-09-26T14:23:01.922854: step 1861, loss 0.00986218, acc 1\n",
      "2017-09-26T14:23:02.193184: step 1862, loss 0.00154868, acc 1\n",
      "2017-09-26T14:23:02.481252: step 1863, loss 0.00649429, acc 1\n",
      "2017-09-26T14:23:02.756523: step 1864, loss 0.00661904, acc 1\n",
      "2017-09-26T14:23:03.049857: step 1865, loss 0.00415181, acc 1\n",
      "2017-09-26T14:23:03.357958: step 1866, loss 0.00515929, acc 1\n",
      "2017-09-26T14:23:03.622419: step 1867, loss 0.00148908, acc 1\n",
      "2017-09-26T14:23:03.934566: step 1868, loss 0.00271175, acc 1\n",
      "2017-09-26T14:23:04.238157: step 1869, loss 0.00421187, acc 1\n",
      "2017-09-26T14:23:04.519897: step 1870, loss 0.0104095, acc 1\n",
      "2017-09-26T14:23:04.794346: step 1871, loss 0.00443084, acc 1\n",
      "2017-09-26T14:23:05.085160: step 1872, loss 0.00562541, acc 1\n",
      "2017-09-26T14:23:05.382027: step 1873, loss 0.0122444, acc 1\n",
      "2017-09-26T14:23:05.677047: step 1874, loss 0.00237205, acc 1\n",
      "2017-09-26T14:23:05.974928: step 1875, loss 0.00376663, acc 1\n",
      "2017-09-26T14:23:06.256011: step 1876, loss 0.0117222, acc 1\n",
      "2017-09-26T14:23:06.571083: step 1877, loss 0.00506284, acc 1\n",
      "2017-09-26T14:23:06.912569: step 1878, loss 0.00404891, acc 1\n",
      "2017-09-26T14:23:07.213981: step 1879, loss 0.002344, acc 1\n",
      "2017-09-26T14:23:07.550334: step 1880, loss 0.00335158, acc 1\n",
      "2017-09-26T14:23:07.849261: step 1881, loss 0.0144028, acc 1\n",
      "2017-09-26T14:23:08.143878: step 1882, loss 0.00464891, acc 1\n",
      "2017-09-26T14:23:08.453864: step 1883, loss 0.0101381, acc 1\n",
      "2017-09-26T14:23:08.735966: step 1884, loss 0.0219302, acc 0.984375\n",
      "2017-09-26T14:23:09.032716: step 1885, loss 0.00892559, acc 1\n",
      "2017-09-26T14:23:09.325220: step 1886, loss 0.00457611, acc 1\n",
      "2017-09-26T14:23:09.614782: step 1887, loss 0.0163306, acc 1\n",
      "2017-09-26T14:23:09.909413: step 1888, loss 0.00481627, acc 1\n",
      "2017-09-26T14:23:10.220101: step 1889, loss 0.0064232, acc 1\n",
      "2017-09-26T14:23:10.469291: step 1890, loss 0.00127299, acc 1\n",
      "2017-09-26T14:23:10.786847: step 1891, loss 0.00315401, acc 1\n",
      "2017-09-26T14:23:11.086157: step 1892, loss 0.012831, acc 0.984375\n",
      "2017-09-26T14:23:11.396679: step 1893, loss 0.017348, acc 0.984375\n",
      "2017-09-26T14:23:11.690410: step 1894, loss 0.00324419, acc 1\n",
      "2017-09-26T14:23:11.970670: step 1895, loss 0.00595435, acc 1\n",
      "2017-09-26T14:23:12.257737: step 1896, loss 0.0155943, acc 1\n",
      "2017-09-26T14:23:12.546913: step 1897, loss 0.00161231, acc 1\n",
      "2017-09-26T14:23:12.815256: step 1898, loss 0.00391751, acc 1\n",
      "2017-09-26T14:23:13.116300: step 1899, loss 0.00751865, acc 1\n",
      "2017-09-26T14:23:13.399325: step 1900, loss 0.00384666, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:23:13.688488: step 1900, loss 0.461425, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-1900\n",
      "\n",
      "2017-09-26T14:23:14.296094: step 1901, loss 0.0182291, acc 1\n",
      "2017-09-26T14:23:14.589855: step 1902, loss 0.00277077, acc 1\n",
      "2017-09-26T14:23:14.906441: step 1903, loss 0.00806919, acc 1\n",
      "2017-09-26T14:23:15.212857: step 1904, loss 0.0203016, acc 0.984375\n",
      "2017-09-26T14:23:15.527341: step 1905, loss 0.018707, acc 0.984375\n",
      "2017-09-26T14:23:15.852105: step 1906, loss 0.0121317, acc 1\n",
      "2017-09-26T14:23:16.146577: step 1907, loss 0.0126887, acc 1\n",
      "2017-09-26T14:23:16.438673: step 1908, loss 0.0049905, acc 1\n",
      "2017-09-26T14:23:16.734652: step 1909, loss 0.00499217, acc 1\n",
      "2017-09-26T14:23:17.023358: step 1910, loss 0.00206417, acc 1\n",
      "2017-09-26T14:23:17.302655: step 1911, loss 0.0162283, acc 1\n",
      "2017-09-26T14:23:17.574417: step 1912, loss 0.00763936, acc 1\n",
      "2017-09-26T14:23:17.867288: step 1913, loss 0.00411695, acc 1\n",
      "2017-09-26T14:23:18.163125: step 1914, loss 0.00318704, acc 1\n",
      "2017-09-26T14:23:18.466480: step 1915, loss 0.0230624, acc 0.984375\n",
      "2017-09-26T14:23:18.768419: step 1916, loss 0.00598917, acc 1\n",
      "2017-09-26T14:23:19.051562: step 1917, loss 0.00520458, acc 1\n",
      "2017-09-26T14:23:19.340239: step 1918, loss 0.00455125, acc 1\n",
      "2017-09-26T14:23:19.608387: step 1919, loss 0.0060911, acc 1\n",
      "2017-09-26T14:23:19.877936: step 1920, loss 0.001364, acc 1\n",
      "2017-09-26T14:23:20.198375: step 1921, loss 0.00489936, acc 1\n",
      "2017-09-26T14:23:20.499931: step 1922, loss 0.00205924, acc 1\n",
      "2017-09-26T14:23:20.800075: step 1923, loss 0.00492411, acc 1\n",
      "2017-09-26T14:23:21.083716: step 1924, loss 0.00402216, acc 1\n",
      "2017-09-26T14:23:21.374825: step 1925, loss 0.0169203, acc 0.984375\n",
      "2017-09-26T14:23:21.663038: step 1926, loss 0.00810652, acc 1\n",
      "2017-09-26T14:23:21.968358: step 1927, loss 0.00585814, acc 1\n",
      "2017-09-26T14:23:22.249326: step 1928, loss 0.00343647, acc 1\n",
      "2017-09-26T14:23:22.556669: step 1929, loss 0.0161404, acc 0.984375\n",
      "2017-09-26T14:23:22.832203: step 1930, loss 0.0082925, acc 1\n",
      "2017-09-26T14:23:23.113717: step 1931, loss 0.0032872, acc 1\n",
      "2017-09-26T14:23:23.362934: step 1932, loss 0.0055355, acc 1\n",
      "2017-09-26T14:23:23.685812: step 1933, loss 0.0212019, acc 0.984375\n",
      "2017-09-26T14:23:23.973910: step 1934, loss 0.00289076, acc 1\n",
      "2017-09-26T14:23:24.238792: step 1935, loss 0.0151389, acc 1\n",
      "2017-09-26T14:23:24.560833: step 1936, loss 0.00620829, acc 1\n",
      "2017-09-26T14:23:24.860355: step 1937, loss 0.0254395, acc 0.984375\n",
      "2017-09-26T14:23:25.158585: step 1938, loss 0.00196483, acc 1\n",
      "2017-09-26T14:23:25.448109: step 1939, loss 0.000831256, acc 1\n",
      "2017-09-26T14:23:25.738188: step 1940, loss 0.00376965, acc 1\n",
      "2017-09-26T14:23:26.019286: step 1941, loss 0.00880821, acc 1\n",
      "2017-09-26T14:23:26.324743: step 1942, loss 0.00646249, acc 1\n",
      "2017-09-26T14:23:26.638971: step 1943, loss 0.00390728, acc 1\n",
      "2017-09-26T14:23:26.942765: step 1944, loss 0.00765692, acc 1\n",
      "2017-09-26T14:23:27.217259: step 1945, loss 0.00331842, acc 1\n",
      "2017-09-26T14:23:27.504114: step 1946, loss 0.0187952, acc 1\n",
      "2017-09-26T14:23:27.798317: step 1947, loss 0.0087158, acc 1\n",
      "2017-09-26T14:23:28.127428: step 1948, loss 0.00334081, acc 1\n",
      "2017-09-26T14:23:28.449220: step 1949, loss 0.00314401, acc 1\n",
      "2017-09-26T14:23:28.751855: step 1950, loss 0.0556917, acc 0.984375\n",
      "2017-09-26T14:23:29.068509: step 1951, loss 0.0740523, acc 0.984375\n",
      "2017-09-26T14:23:29.393163: step 1952, loss 0.00796523, acc 1\n",
      "2017-09-26T14:23:29.706552: step 1953, loss 0.0197585, acc 0.984375\n",
      "2017-09-26T14:23:30.040036: step 1954, loss 0.0299095, acc 0.984375\n",
      "2017-09-26T14:23:30.310370: step 1955, loss 0.0046312, acc 1\n",
      "2017-09-26T14:23:30.617424: step 1956, loss 0.00225626, acc 1\n",
      "2017-09-26T14:23:30.921520: step 1957, loss 0.00333837, acc 1\n",
      "2017-09-26T14:23:31.239446: step 1958, loss 0.00399354, acc 1\n",
      "2017-09-26T14:23:31.560916: step 1959, loss 0.00440052, acc 1\n",
      "2017-09-26T14:23:31.880416: step 1960, loss 0.00449108, acc 1\n",
      "2017-09-26T14:23:32.174219: step 1961, loss 0.000854041, acc 1\n",
      "2017-09-26T14:23:32.491899: step 1962, loss 0.00240845, acc 1\n",
      "2017-09-26T14:23:32.792582: step 1963, loss 0.00669269, acc 1\n",
      "2017-09-26T14:23:33.084324: step 1964, loss 0.0121674, acc 1\n",
      "2017-09-26T14:23:33.415749: step 1965, loss 0.00448968, acc 1\n",
      "2017-09-26T14:23:33.737445: step 1966, loss 0.00347844, acc 1\n",
      "2017-09-26T14:23:34.052820: step 1967, loss 0.0150959, acc 1\n",
      "2017-09-26T14:23:34.363220: step 1968, loss 0.0140337, acc 1\n",
      "2017-09-26T14:23:34.682341: step 1969, loss 0.00393639, acc 1\n",
      "2017-09-26T14:23:34.986551: step 1970, loss 0.0141595, acc 1\n",
      "2017-09-26T14:23:35.283134: step 1971, loss 0.0103724, acc 1\n",
      "2017-09-26T14:23:35.586744: step 1972, loss 0.0123285, acc 1\n",
      "2017-09-26T14:23:35.888023: step 1973, loss 0.0129824, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:23:36.169923: step 1974, loss 0.00550651, acc 1\n",
      "2017-09-26T14:23:36.493839: step 1975, loss 0.0287407, acc 0.984375\n",
      "2017-09-26T14:23:36.781633: step 1976, loss 0.0152239, acc 1\n",
      "2017-09-26T14:23:37.073700: step 1977, loss 0.00990128, acc 1\n",
      "2017-09-26T14:23:37.371277: step 1978, loss 0.00293495, acc 1\n",
      "2017-09-26T14:23:37.666262: step 1979, loss 0.00508468, acc 1\n",
      "2017-09-26T14:23:37.960495: step 1980, loss 0.0070334, acc 1\n",
      "2017-09-26T14:23:38.247698: step 1981, loss 0.0184249, acc 0.984375\n",
      "2017-09-26T14:23:38.528022: step 1982, loss 0.00157267, acc 1\n",
      "2017-09-26T14:23:38.817007: step 1983, loss 0.00480516, acc 1\n",
      "2017-09-26T14:23:39.130106: step 1984, loss 0.0324399, acc 0.984375\n",
      "2017-09-26T14:23:39.408653: step 1985, loss 0.0035063, acc 1\n",
      "2017-09-26T14:23:39.700226: step 1986, loss 0.00475594, acc 1\n",
      "2017-09-26T14:23:40.001065: step 1987, loss 0.00684469, acc 1\n",
      "2017-09-26T14:23:40.343441: step 1988, loss 0.00888315, acc 1\n",
      "2017-09-26T14:23:40.642622: step 1989, loss 0.00273264, acc 1\n",
      "2017-09-26T14:23:40.934266: step 1990, loss 0.0063246, acc 1\n",
      "2017-09-26T14:23:41.326305: step 1991, loss 0.0042458, acc 1\n",
      "2017-09-26T14:23:41.728897: step 1992, loss 0.0051248, acc 1\n",
      "2017-09-26T14:23:42.132002: step 1993, loss 0.00405387, acc 1\n",
      "2017-09-26T14:23:42.519515: step 1994, loss 0.00964179, acc 1\n",
      "2017-09-26T14:23:42.890209: step 1995, loss 0.00332025, acc 1\n",
      "2017-09-26T14:23:43.260673: step 1996, loss 0.00285398, acc 1\n",
      "2017-09-26T14:23:43.572601: step 1997, loss 0.024452, acc 0.984375\n",
      "2017-09-26T14:23:43.873048: step 1998, loss 0.0309499, acc 0.96875\n",
      "2017-09-26T14:23:44.198310: step 1999, loss 0.00751356, acc 1\n",
      "2017-09-26T14:23:44.501302: step 2000, loss 0.0016608, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:23:44.798057: step 2000, loss 0.468298, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2000\n",
      "\n",
      "2017-09-26T14:23:45.501946: step 2001, loss 0.0103426, acc 1\n",
      "2017-09-26T14:23:45.867493: step 2002, loss 0.00249967, acc 1\n",
      "2017-09-26T14:23:46.320681: step 2003, loss 0.00648363, acc 1\n",
      "2017-09-26T14:23:46.864920: step 2004, loss 0.00619485, acc 1\n",
      "2017-09-26T14:23:47.255931: step 2005, loss 0.0151114, acc 0.984375\n",
      "2017-09-26T14:23:47.565318: step 2006, loss 0.00398469, acc 1\n",
      "2017-09-26T14:23:47.882978: step 2007, loss 0.00367997, acc 1\n",
      "2017-09-26T14:23:48.321898: step 2008, loss 0.00604019, acc 1\n",
      "2017-09-26T14:23:48.709087: step 2009, loss 0.0131489, acc 1\n",
      "2017-09-26T14:23:49.039129: step 2010, loss 0.00199687, acc 1\n",
      "2017-09-26T14:23:49.389841: step 2011, loss 0.00442149, acc 1\n",
      "2017-09-26T14:23:49.738848: step 2012, loss 0.00431876, acc 1\n",
      "2017-09-26T14:23:50.116725: step 2013, loss 0.00381633, acc 1\n",
      "2017-09-26T14:23:50.474538: step 2014, loss 0.0122386, acc 1\n",
      "2017-09-26T14:23:50.878552: step 2015, loss 0.00636698, acc 1\n",
      "2017-09-26T14:23:51.213707: step 2016, loss 0.00951772, acc 1\n",
      "2017-09-26T14:23:51.644048: step 2017, loss 0.00234623, acc 1\n",
      "2017-09-26T14:23:52.063018: step 2018, loss 0.00353136, acc 1\n",
      "2017-09-26T14:23:52.473526: step 2019, loss 0.0217141, acc 0.984375\n",
      "2017-09-26T14:23:52.884783: step 2020, loss 0.00479485, acc 1\n",
      "2017-09-26T14:23:53.287062: step 2021, loss 0.00175736, acc 1\n",
      "2017-09-26T14:23:53.666545: step 2022, loss 0.00445445, acc 1\n",
      "2017-09-26T14:23:54.081658: step 2023, loss 0.0091059, acc 1\n",
      "2017-09-26T14:23:54.537057: step 2024, loss 0.0031595, acc 1\n",
      "2017-09-26T14:23:54.947998: step 2025, loss 0.00783763, acc 1\n",
      "2017-09-26T14:23:55.355280: step 2026, loss 0.00184542, acc 1\n",
      "2017-09-26T14:23:55.767889: step 2027, loss 0.0123344, acc 1\n",
      "2017-09-26T14:23:56.116464: step 2028, loss 0.00724701, acc 1\n",
      "2017-09-26T14:23:56.501400: step 2029, loss 0.0228413, acc 0.984375\n",
      "2017-09-26T14:23:56.855243: step 2030, loss 0.0036433, acc 1\n",
      "2017-09-26T14:23:57.233403: step 2031, loss 0.00467804, acc 1\n",
      "2017-09-26T14:23:57.603377: step 2032, loss 0.00383635, acc 1\n",
      "2017-09-26T14:23:57.998021: step 2033, loss 0.00146018, acc 1\n",
      "2017-09-26T14:23:58.398900: step 2034, loss 0.0047615, acc 1\n",
      "2017-09-26T14:23:58.780628: step 2035, loss 0.0019433, acc 1\n",
      "2017-09-26T14:23:59.168710: step 2036, loss 0.00659826, acc 1\n",
      "2017-09-26T14:23:59.505795: step 2037, loss 0.00489235, acc 1\n",
      "2017-09-26T14:23:59.824112: step 2038, loss 0.0134101, acc 0.984375\n",
      "2017-09-26T14:24:00.139748: step 2039, loss 0.0011393, acc 1\n",
      "2017-09-26T14:24:00.450077: step 2040, loss 0.0119526, acc 1\n",
      "2017-09-26T14:24:00.745977: step 2041, loss 0.00446702, acc 1\n",
      "2017-09-26T14:24:01.044495: step 2042, loss 0.00350679, acc 1\n",
      "2017-09-26T14:24:01.370988: step 2043, loss 0.00153184, acc 1\n",
      "2017-09-26T14:24:01.653725: step 2044, loss 0.045889, acc 0.984375\n",
      "2017-09-26T14:24:01.920414: step 2045, loss 0.00384545, acc 1\n",
      "2017-09-26T14:24:02.209861: step 2046, loss 0.00412808, acc 1\n",
      "2017-09-26T14:24:02.502028: step 2047, loss 0.0121802, acc 1\n",
      "2017-09-26T14:24:02.812712: step 2048, loss 0.00251777, acc 1\n",
      "2017-09-26T14:24:03.102787: step 2049, loss 0.00253175, acc 1\n",
      "2017-09-26T14:24:03.376856: step 2050, loss 0.00193994, acc 1\n",
      "2017-09-26T14:24:03.653940: step 2051, loss 0.00352729, acc 1\n",
      "2017-09-26T14:24:03.939846: step 2052, loss 0.00212503, acc 1\n",
      "2017-09-26T14:24:04.206640: step 2053, loss 0.00153435, acc 1\n",
      "2017-09-26T14:24:04.470727: step 2054, loss 0.0122824, acc 1\n",
      "2017-09-26T14:24:04.737846: step 2055, loss 0.00550536, acc 1\n",
      "2017-09-26T14:24:05.011850: step 2056, loss 0.00194949, acc 1\n",
      "2017-09-26T14:24:05.285645: step 2057, loss 0.00657733, acc 1\n",
      "2017-09-26T14:24:05.569945: step 2058, loss 0.00280237, acc 1\n",
      "2017-09-26T14:24:05.870150: step 2059, loss 0.0166936, acc 0.984375\n",
      "2017-09-26T14:24:06.205446: step 2060, loss 0.00524454, acc 1\n",
      "2017-09-26T14:24:06.513012: step 2061, loss 0.00152045, acc 1\n",
      "2017-09-26T14:24:06.810734: step 2062, loss 0.0192777, acc 0.984375\n",
      "2017-09-26T14:24:07.127651: step 2063, loss 0.0059465, acc 1\n",
      "2017-09-26T14:24:07.421653: step 2064, loss 0.00104859, acc 1\n",
      "2017-09-26T14:24:07.686895: step 2065, loss 0.00570629, acc 1\n",
      "2017-09-26T14:24:07.968869: step 2066, loss 0.00515896, acc 1\n",
      "2017-09-26T14:24:08.346215: step 2067, loss 0.00250472, acc 1\n",
      "2017-09-26T14:24:08.680077: step 2068, loss 0.00206117, acc 1\n",
      "2017-09-26T14:24:08.989659: step 2069, loss 0.0409002, acc 0.984375\n",
      "2017-09-26T14:24:09.302373: step 2070, loss 0.00583271, acc 1\n",
      "2017-09-26T14:24:09.577901: step 2071, loss 0.00427191, acc 1\n",
      "2017-09-26T14:24:09.866545: step 2072, loss 0.00136451, acc 1\n",
      "2017-09-26T14:24:10.151333: step 2073, loss 0.00229916, acc 1\n",
      "2017-09-26T14:24:10.507363: step 2074, loss 0.00496699, acc 1\n",
      "2017-09-26T14:24:10.791721: step 2075, loss 0.00505466, acc 1\n",
      "2017-09-26T14:24:11.082479: step 2076, loss 0.00356296, acc 1\n",
      "2017-09-26T14:24:11.366093: step 2077, loss 0.00399486, acc 1\n",
      "2017-09-26T14:24:11.685069: step 2078, loss 0.00326587, acc 1\n",
      "2017-09-26T14:24:12.014680: step 2079, loss 0.00403062, acc 1\n",
      "2017-09-26T14:24:12.366546: step 2080, loss 0.00756131, acc 1\n",
      "2017-09-26T14:24:12.682212: step 2081, loss 0.0117163, acc 1\n",
      "2017-09-26T14:24:12.968360: step 2082, loss 0.00907365, acc 1\n",
      "2017-09-26T14:24:13.276291: step 2083, loss 0.00280082, acc 1\n",
      "2017-09-26T14:24:13.575503: step 2084, loss 0.00208738, acc 1\n",
      "2017-09-26T14:24:13.865073: step 2085, loss 0.00191889, acc 1\n",
      "2017-09-26T14:24:14.187654: step 2086, loss 0.00387277, acc 1\n",
      "2017-09-26T14:24:14.455889: step 2087, loss 0.00139911, acc 1\n",
      "2017-09-26T14:24:14.709759: step 2088, loss 0.0052389, acc 1\n",
      "2017-09-26T14:24:14.971430: step 2089, loss 0.0146508, acc 1\n",
      "2017-09-26T14:24:15.238308: step 2090, loss 0.0597491, acc 0.984375\n",
      "2017-09-26T14:24:15.515733: step 2091, loss 0.00448345, acc 1\n",
      "2017-09-26T14:24:15.786685: step 2092, loss 0.00256863, acc 1\n",
      "2017-09-26T14:24:16.070230: step 2093, loss 0.00345096, acc 1\n",
      "2017-09-26T14:24:16.343583: step 2094, loss 0.00626698, acc 1\n",
      "2017-09-26T14:24:16.601723: step 2095, loss 0.00129002, acc 1\n",
      "2017-09-26T14:24:16.860315: step 2096, loss 0.002652, acc 1\n",
      "2017-09-26T14:24:17.122057: step 2097, loss 0.00501758, acc 1\n",
      "2017-09-26T14:24:17.390505: step 2098, loss 0.000858048, acc 1\n",
      "2017-09-26T14:24:17.645455: step 2099, loss 0.0109687, acc 1\n",
      "2017-09-26T14:24:17.874947: step 2100, loss 0.00102396, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:24:18.178089: step 2100, loss 0.47874, acc 0.885522\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2100\n",
      "\n",
      "2017-09-26T14:24:18.787682: step 2101, loss 0.00429184, acc 1\n",
      "2017-09-26T14:24:19.087484: step 2102, loss 0.0040616, acc 1\n",
      "2017-09-26T14:24:19.388976: step 2103, loss 0.00163056, acc 1\n",
      "2017-09-26T14:24:19.726735: step 2104, loss 0.00292283, acc 1\n",
      "2017-09-26T14:24:20.044670: step 2105, loss 0.00136671, acc 1\n",
      "2017-09-26T14:24:20.340881: step 2106, loss 0.0134152, acc 1\n",
      "2017-09-26T14:24:20.628015: step 2107, loss 0.00560284, acc 1\n",
      "2017-09-26T14:24:20.955461: step 2108, loss 0.00429067, acc 1\n",
      "2017-09-26T14:24:21.264063: step 2109, loss 0.0162886, acc 0.984375\n",
      "2017-09-26T14:24:21.557634: step 2110, loss 0.00206123, acc 1\n",
      "2017-09-26T14:24:21.845161: step 2111, loss 0.00250798, acc 1\n",
      "2017-09-26T14:24:22.120284: step 2112, loss 0.004202, acc 1\n",
      "2017-09-26T14:24:22.445200: step 2113, loss 0.00543397, acc 1\n",
      "2017-09-26T14:24:22.708701: step 2114, loss 0.00746121, acc 1\n",
      "2017-09-26T14:24:22.979494: step 2115, loss 0.00634271, acc 1\n",
      "2017-09-26T14:24:23.281871: step 2116, loss 0.0180794, acc 0.984375\n",
      "2017-09-26T14:24:23.552501: step 2117, loss 0.00463908, acc 1\n",
      "2017-09-26T14:24:23.781308: step 2118, loss 0.00206148, acc 1\n",
      "2017-09-26T14:24:24.015774: step 2119, loss 0.0113692, acc 1\n",
      "2017-09-26T14:24:24.258831: step 2120, loss 0.00451517, acc 1\n",
      "2017-09-26T14:24:24.551066: step 2121, loss 0.00120116, acc 1\n",
      "2017-09-26T14:24:24.836273: step 2122, loss 0.00199346, acc 1\n",
      "2017-09-26T14:24:25.126688: step 2123, loss 0.0041139, acc 1\n",
      "2017-09-26T14:24:25.409876: step 2124, loss 0.00529241, acc 1\n",
      "2017-09-26T14:24:25.682015: step 2125, loss 0.00185865, acc 1\n",
      "2017-09-26T14:24:25.942476: step 2126, loss 0.0285148, acc 0.984375\n",
      "2017-09-26T14:24:26.175580: step 2127, loss 0.00339914, acc 1\n",
      "2017-09-26T14:24:26.407195: step 2128, loss 0.00188462, acc 1\n",
      "2017-09-26T14:24:26.639255: step 2129, loss 0.00518946, acc 1\n",
      "2017-09-26T14:24:26.876277: step 2130, loss 0.00221725, acc 1\n",
      "2017-09-26T14:24:27.107903: step 2131, loss 0.0929558, acc 0.984375\n",
      "2017-09-26T14:24:27.348184: step 2132, loss 0.00265443, acc 1\n",
      "2017-09-26T14:24:27.627611: step 2133, loss 0.00463359, acc 1\n",
      "2017-09-26T14:24:27.928920: step 2134, loss 0.0342669, acc 0.984375\n",
      "2017-09-26T14:24:28.243325: step 2135, loss 0.0047309, acc 1\n",
      "2017-09-26T14:24:28.492733: step 2136, loss 0.0144958, acc 1\n",
      "2017-09-26T14:24:28.726051: step 2137, loss 0.00112904, acc 1\n",
      "2017-09-26T14:24:28.988808: step 2138, loss 0.00527255, acc 1\n",
      "2017-09-26T14:24:29.220880: step 2139, loss 0.0121341, acc 1\n",
      "2017-09-26T14:24:29.460650: step 2140, loss 0.00697197, acc 1\n",
      "2017-09-26T14:24:29.721450: step 2141, loss 0.00413889, acc 1\n",
      "2017-09-26T14:24:29.936855: step 2142, loss 0.00729358, acc 1\n",
      "2017-09-26T14:24:30.216543: step 2143, loss 0.0044748, acc 1\n",
      "2017-09-26T14:24:30.499172: step 2144, loss 0.00173031, acc 1\n",
      "2017-09-26T14:24:30.743501: step 2145, loss 0.00461663, acc 1\n",
      "2017-09-26T14:24:31.000255: step 2146, loss 0.0176035, acc 1\n",
      "2017-09-26T14:24:31.243128: step 2147, loss 0.0184361, acc 0.984375\n",
      "2017-09-26T14:24:31.487405: step 2148, loss 0.0119903, acc 1\n",
      "2017-09-26T14:24:31.779064: step 2149, loss 0.00178597, acc 1\n",
      "2017-09-26T14:24:32.039707: step 2150, loss 0.00277041, acc 1\n",
      "2017-09-26T14:24:32.288003: step 2151, loss 0.0108897, acc 1\n",
      "2017-09-26T14:24:32.574620: step 2152, loss 0.000998145, acc 1\n",
      "2017-09-26T14:24:32.854281: step 2153, loss 0.0019594, acc 1\n",
      "2017-09-26T14:24:33.158869: step 2154, loss 0.00630957, acc 1\n",
      "2017-09-26T14:24:33.452952: step 2155, loss 0.00197791, acc 1\n",
      "2017-09-26T14:24:33.710855: step 2156, loss 0.00145297, acc 1\n",
      "2017-09-26T14:24:34.003120: step 2157, loss 0.00296979, acc 1\n",
      "2017-09-26T14:24:34.293076: step 2158, loss 0.0115076, acc 1\n",
      "2017-09-26T14:24:34.592532: step 2159, loss 0.0016287, acc 1\n",
      "2017-09-26T14:24:34.889216: step 2160, loss 0.00459253, acc 1\n",
      "2017-09-26T14:24:35.185321: step 2161, loss 0.00116175, acc 1\n",
      "2017-09-26T14:24:35.440922: step 2162, loss 0.00515108, acc 1\n",
      "2017-09-26T14:24:35.830436: step 2163, loss 0.00362882, acc 1\n",
      "2017-09-26T14:24:36.200114: step 2164, loss 0.00714537, acc 1\n",
      "2017-09-26T14:24:36.591481: step 2165, loss 0.00606059, acc 1\n",
      "2017-09-26T14:24:36.905221: step 2166, loss 0.00147368, acc 1\n",
      "2017-09-26T14:24:37.190992: step 2167, loss 0.00155315, acc 1\n",
      "2017-09-26T14:24:37.466112: step 2168, loss 0.00137345, acc 1\n",
      "2017-09-26T14:24:37.777370: step 2169, loss 0.00560732, acc 1\n",
      "2017-09-26T14:24:38.052540: step 2170, loss 0.00195256, acc 1\n",
      "2017-09-26T14:24:38.356141: step 2171, loss 0.00222255, acc 1\n",
      "2017-09-26T14:24:38.690170: step 2172, loss 0.00291074, acc 1\n",
      "2017-09-26T14:24:39.022277: step 2173, loss 0.00540293, acc 1\n",
      "2017-09-26T14:24:39.328674: step 2174, loss 0.000587189, acc 1\n",
      "2017-09-26T14:24:39.606803: step 2175, loss 0.00146912, acc 1\n",
      "2017-09-26T14:24:39.897951: step 2176, loss 0.0585347, acc 0.984375\n",
      "2017-09-26T14:24:40.281037: step 2177, loss 0.00556253, acc 1\n",
      "2017-09-26T14:24:40.649535: step 2178, loss 0.00142617, acc 1\n",
      "2017-09-26T14:24:41.035589: step 2179, loss 0.000885002, acc 1\n",
      "2017-09-26T14:24:41.376750: step 2180, loss 0.00311809, acc 1\n",
      "2017-09-26T14:24:41.703131: step 2181, loss 0.00160666, acc 1\n",
      "2017-09-26T14:24:41.951372: step 2182, loss 0.00710666, acc 1\n",
      "2017-09-26T14:24:42.201729: step 2183, loss 0.003575, acc 1\n",
      "2017-09-26T14:24:42.434926: step 2184, loss 0.00345397, acc 1\n",
      "2017-09-26T14:24:42.696963: step 2185, loss 0.00912296, acc 1\n",
      "2017-09-26T14:24:43.020356: step 2186, loss 0.00220269, acc 1\n",
      "2017-09-26T14:24:43.276761: step 2187, loss 0.00591171, acc 1\n",
      "2017-09-26T14:24:43.552531: step 2188, loss 0.00364807, acc 1\n",
      "2017-09-26T14:24:43.829365: step 2189, loss 0.00137805, acc 1\n",
      "2017-09-26T14:24:44.123667: step 2190, loss 0.00348413, acc 1\n",
      "2017-09-26T14:24:44.417820: step 2191, loss 0.0234226, acc 0.984375\n",
      "2017-09-26T14:24:44.701953: step 2192, loss 0.00348492, acc 1\n",
      "2017-09-26T14:24:45.036005: step 2193, loss 0.00109796, acc 1\n",
      "2017-09-26T14:24:45.407119: step 2194, loss 0.00835677, acc 1\n",
      "2017-09-26T14:24:45.755464: step 2195, loss 0.00133276, acc 1\n",
      "2017-09-26T14:24:46.150468: step 2196, loss 0.00118006, acc 1\n",
      "2017-09-26T14:24:46.554408: step 2197, loss 0.00888547, acc 1\n",
      "2017-09-26T14:24:46.933391: step 2198, loss 0.00274927, acc 1\n",
      "2017-09-26T14:24:47.281344: step 2199, loss 0.00200982, acc 1\n",
      "2017-09-26T14:24:47.655300: step 2200, loss 0.00320241, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:24:48.017777: step 2200, loss 0.480597, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2200\n",
      "\n",
      "2017-09-26T14:24:48.594727: step 2201, loss 0.00177075, acc 1\n",
      "2017-09-26T14:24:48.872694: step 2202, loss 0.00117483, acc 1\n",
      "2017-09-26T14:24:49.142024: step 2203, loss 0.00515529, acc 1\n",
      "2017-09-26T14:24:49.411392: step 2204, loss 0.00134143, acc 1\n",
      "2017-09-26T14:24:49.685856: step 2205, loss 0.0175016, acc 0.984375\n",
      "2017-09-26T14:24:49.940525: step 2206, loss 0.0008684, acc 1\n",
      "2017-09-26T14:24:50.215070: step 2207, loss 0.0044872, acc 1\n",
      "2017-09-26T14:24:50.473775: step 2208, loss 0.00893368, acc 1\n",
      "2017-09-26T14:24:50.746456: step 2209, loss 0.00151578, acc 1\n",
      "2017-09-26T14:24:51.006491: step 2210, loss 0.00663039, acc 1\n",
      "2017-09-26T14:24:51.320327: step 2211, loss 0.00259496, acc 1\n",
      "2017-09-26T14:24:51.678186: step 2212, loss 0.00470769, acc 1\n",
      "2017-09-26T14:24:52.008718: step 2213, loss 0.0173444, acc 1\n",
      "2017-09-26T14:24:52.325173: step 2214, loss 0.00454615, acc 1\n",
      "2017-09-26T14:24:52.647937: step 2215, loss 0.00580455, acc 1\n",
      "2017-09-26T14:24:53.011986: step 2216, loss 0.00507011, acc 1\n",
      "2017-09-26T14:24:53.331627: step 2217, loss 0.0139365, acc 1\n",
      "2017-09-26T14:24:53.641438: step 2218, loss 0.00138814, acc 1\n",
      "2017-09-26T14:24:53.929682: step 2219, loss 0.00627038, acc 1\n",
      "2017-09-26T14:24:54.185174: step 2220, loss 0.00365971, acc 1\n",
      "2017-09-26T14:24:54.478677: step 2221, loss 0.00225577, acc 1\n",
      "2017-09-26T14:24:54.753982: step 2222, loss 0.0368692, acc 0.984375\n",
      "2017-09-26T14:24:55.008819: step 2223, loss 0.00116756, acc 1\n",
      "2017-09-26T14:24:55.296395: step 2224, loss 0.00608775, acc 1\n",
      "2017-09-26T14:24:55.564930: step 2225, loss 0.00248624, acc 1\n",
      "2017-09-26T14:24:55.808521: step 2226, loss 0.00304646, acc 1\n",
      "2017-09-26T14:24:56.094081: step 2227, loss 0.00359632, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:24:56.359763: step 2228, loss 0.00159005, acc 1\n",
      "2017-09-26T14:24:56.641984: step 2229, loss 0.0155404, acc 0.984375\n",
      "2017-09-26T14:24:56.944253: step 2230, loss 0.00170355, acc 1\n",
      "2017-09-26T14:24:57.199489: step 2231, loss 0.000898691, acc 1\n",
      "2017-09-26T14:24:57.452014: step 2232, loss 0.000467113, acc 1\n",
      "2017-09-26T14:24:57.734046: step 2233, loss 0.02235, acc 0.984375\n",
      "2017-09-26T14:24:58.028352: step 2234, loss 0.00948752, acc 1\n",
      "2017-09-26T14:24:58.359268: step 2235, loss 0.00647702, acc 1\n",
      "2017-09-26T14:24:58.607592: step 2236, loss 0.00929583, acc 1\n",
      "2017-09-26T14:24:58.846788: step 2237, loss 0.0171167, acc 0.984375\n",
      "2017-09-26T14:24:59.076289: step 2238, loss 0.00284384, acc 1\n",
      "2017-09-26T14:24:59.310338: step 2239, loss 0.00335788, acc 1\n",
      "2017-09-26T14:24:59.545353: step 2240, loss 0.00147843, acc 1\n",
      "2017-09-26T14:24:59.780187: step 2241, loss 0.00585392, acc 1\n",
      "2017-09-26T14:25:00.012610: step 2242, loss 0.00213546, acc 1\n",
      "2017-09-26T14:25:00.252531: step 2243, loss 0.00299535, acc 1\n",
      "2017-09-26T14:25:00.495127: step 2244, loss 0.0019149, acc 1\n",
      "2017-09-26T14:25:00.727450: step 2245, loss 0.0015498, acc 1\n",
      "2017-09-26T14:25:00.983255: step 2246, loss 0.0195909, acc 0.984375\n",
      "2017-09-26T14:25:01.214045: step 2247, loss 0.0032094, acc 1\n",
      "2017-09-26T14:25:01.451885: step 2248, loss 0.0392547, acc 0.984375\n",
      "2017-09-26T14:25:01.689849: step 2249, loss 0.0606719, acc 0.984375\n",
      "2017-09-26T14:25:01.921679: step 2250, loss 0.00295086, acc 1\n",
      "2017-09-26T14:25:02.152715: step 2251, loss 0.00565553, acc 1\n",
      "2017-09-26T14:25:02.401779: step 2252, loss 0.0017849, acc 1\n",
      "2017-09-26T14:25:02.644156: step 2253, loss 0.00414201, acc 1\n",
      "2017-09-26T14:25:02.875017: step 2254, loss 0.00495019, acc 1\n",
      "2017-09-26T14:25:03.118583: step 2255, loss 0.00890276, acc 1\n",
      "2017-09-26T14:25:03.356261: step 2256, loss 0.00217878, acc 1\n",
      "2017-09-26T14:25:03.587726: step 2257, loss 0.00190263, acc 1\n",
      "2017-09-26T14:25:03.824025: step 2258, loss 0.0228729, acc 0.984375\n",
      "2017-09-26T14:25:04.097460: step 2259, loss 0.00402921, acc 1\n",
      "2017-09-26T14:25:04.332870: step 2260, loss 0.00264276, acc 1\n",
      "2017-09-26T14:25:04.566397: step 2261, loss 0.00143003, acc 1\n",
      "2017-09-26T14:25:04.797766: step 2262, loss 0.00313667, acc 1\n",
      "2017-09-26T14:25:05.038814: step 2263, loss 0.00510784, acc 1\n",
      "2017-09-26T14:25:05.302699: step 2264, loss 0.0349534, acc 0.984375\n",
      "2017-09-26T14:25:05.529507: step 2265, loss 0.0086859, acc 1\n",
      "2017-09-26T14:25:05.768741: step 2266, loss 0.00388143, acc 1\n",
      "2017-09-26T14:25:06.020273: step 2267, loss 0.00343295, acc 1\n",
      "2017-09-26T14:25:06.222998: step 2268, loss 0.00339849, acc 1\n",
      "2017-09-26T14:25:06.483020: step 2269, loss 0.00304274, acc 1\n",
      "2017-09-26T14:25:06.714936: step 2270, loss 0.0044607, acc 1\n",
      "2017-09-26T14:25:06.953388: step 2271, loss 0.0119202, acc 1\n",
      "2017-09-26T14:25:07.204614: step 2272, loss 0.0200348, acc 0.984375\n",
      "2017-09-26T14:25:07.438979: step 2273, loss 0.0177549, acc 0.984375\n",
      "2017-09-26T14:25:07.673064: step 2274, loss 0.00347913, acc 1\n",
      "2017-09-26T14:25:07.907729: step 2275, loss 0.00444441, acc 1\n",
      "2017-09-26T14:25:08.146078: step 2276, loss 0.00205274, acc 1\n",
      "2017-09-26T14:25:08.379175: step 2277, loss 0.000542946, acc 1\n",
      "2017-09-26T14:25:08.615885: step 2278, loss 0.0030149, acc 1\n",
      "2017-09-26T14:25:08.850697: step 2279, loss 0.00143906, acc 1\n",
      "2017-09-26T14:25:09.081758: step 2280, loss 0.00409404, acc 1\n",
      "2017-09-26T14:25:09.325363: step 2281, loss 0.0159153, acc 1\n",
      "2017-09-26T14:25:09.560787: step 2282, loss 0.0208015, acc 0.984375\n",
      "2017-09-26T14:25:09.805169: step 2283, loss 0.0394069, acc 0.984375\n",
      "2017-09-26T14:25:10.038094: step 2284, loss 0.0102352, acc 1\n",
      "2017-09-26T14:25:10.273975: step 2285, loss 0.00815577, acc 1\n",
      "2017-09-26T14:25:10.512781: step 2286, loss 0.0543344, acc 0.984375\n",
      "2017-09-26T14:25:10.745139: step 2287, loss 0.00230363, acc 1\n",
      "2017-09-26T14:25:10.978282: step 2288, loss 0.00903999, acc 1\n",
      "2017-09-26T14:25:11.223741: step 2289, loss 0.00339167, acc 1\n",
      "2017-09-26T14:25:11.473839: step 2290, loss 0.0248032, acc 0.984375\n",
      "2017-09-26T14:25:11.707067: step 2291, loss 0.0029178, acc 1\n",
      "2017-09-26T14:25:11.952162: step 2292, loss 0.00321598, acc 1\n",
      "2017-09-26T14:25:12.189602: step 2293, loss 0.00260421, acc 1\n",
      "2017-09-26T14:25:12.424614: step 2294, loss 0.00225376, acc 1\n",
      "2017-09-26T14:25:12.660160: step 2295, loss 0.00405729, acc 1\n",
      "2017-09-26T14:25:12.893307: step 2296, loss 0.00233257, acc 1\n",
      "2017-09-26T14:25:13.126773: step 2297, loss 0.00435113, acc 1\n",
      "2017-09-26T14:25:13.361274: step 2298, loss 0.000944106, acc 1\n",
      "2017-09-26T14:25:13.596333: step 2299, loss 0.00200782, acc 1\n",
      "2017-09-26T14:25:13.836422: step 2300, loss 0.00839798, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:25:14.067399: step 2300, loss 0.467365, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2300\n",
      "\n",
      "2017-09-26T14:25:14.564093: step 2301, loss 0.00094936, acc 1\n",
      "2017-09-26T14:25:14.796659: step 2302, loss 0.0119928, acc 1\n",
      "2017-09-26T14:25:15.028833: step 2303, loss 0.00231849, acc 1\n",
      "2017-09-26T14:25:15.268230: step 2304, loss 0.00558398, acc 1\n",
      "2017-09-26T14:25:15.505831: step 2305, loss 0.00180761, acc 1\n",
      "2017-09-26T14:25:15.746721: step 2306, loss 0.00307131, acc 1\n",
      "2017-09-26T14:25:15.985407: step 2307, loss 0.00101111, acc 1\n",
      "2017-09-26T14:25:16.227643: step 2308, loss 0.00198271, acc 1\n",
      "2017-09-26T14:25:16.483679: step 2309, loss 0.00570463, acc 1\n",
      "2017-09-26T14:25:16.690101: step 2310, loss 0.00598879, acc 1\n",
      "2017-09-26T14:25:16.931241: step 2311, loss 0.00381521, acc 1\n",
      "2017-09-26T14:25:17.169006: step 2312, loss 0.000542927, acc 1\n",
      "2017-09-26T14:25:17.406312: step 2313, loss 0.017833, acc 0.984375\n",
      "2017-09-26T14:25:17.644521: step 2314, loss 0.00360085, acc 1\n",
      "2017-09-26T14:25:17.904686: step 2315, loss 0.000902127, acc 1\n",
      "2017-09-26T14:25:18.156313: step 2316, loss 0.00407985, acc 1\n",
      "2017-09-26T14:25:18.391255: step 2317, loss 0.00200772, acc 1\n",
      "2017-09-26T14:25:18.625999: step 2318, loss 0.00578851, acc 1\n",
      "2017-09-26T14:25:18.866360: step 2319, loss 0.00541514, acc 1\n",
      "2017-09-26T14:25:19.104499: step 2320, loss 0.00779559, acc 1\n",
      "2017-09-26T14:25:19.343196: step 2321, loss 0.00152359, acc 1\n",
      "2017-09-26T14:25:19.580331: step 2322, loss 0.0303762, acc 0.984375\n",
      "2017-09-26T14:25:19.814882: step 2323, loss 0.00276864, acc 1\n",
      "2017-09-26T14:25:20.071941: step 2324, loss 0.00072185, acc 1\n",
      "2017-09-26T14:25:20.311873: step 2325, loss 0.00187052, acc 1\n",
      "2017-09-26T14:25:20.545184: step 2326, loss 0.00110727, acc 1\n",
      "2017-09-26T14:25:20.786470: step 2327, loss 0.00632086, acc 1\n",
      "2017-09-26T14:25:21.022185: step 2328, loss 0.0601571, acc 0.984375\n",
      "2017-09-26T14:25:21.271371: step 2329, loss 0.00479196, acc 1\n",
      "2017-09-26T14:25:21.506525: step 2330, loss 0.00374292, acc 1\n",
      "2017-09-26T14:25:21.745220: step 2331, loss 0.00197138, acc 1\n",
      "2017-09-26T14:25:21.981383: step 2332, loss 0.00660503, acc 1\n",
      "2017-09-26T14:25:22.217809: step 2333, loss 0.00111637, acc 1\n",
      "2017-09-26T14:25:22.455632: step 2334, loss 0.00105893, acc 1\n",
      "2017-09-26T14:25:22.695056: step 2335, loss 0.0391739, acc 0.984375\n",
      "2017-09-26T14:25:22.937636: step 2336, loss 0.0112407, acc 1\n",
      "2017-09-26T14:25:23.173507: step 2337, loss 0.00546003, acc 1\n",
      "2017-09-26T14:25:23.425233: step 2338, loss 0.00115608, acc 1\n",
      "2017-09-26T14:25:23.690355: step 2339, loss 0.00196246, acc 1\n",
      "2017-09-26T14:25:24.012802: step 2340, loss 0.0345835, acc 0.984375\n",
      "2017-09-26T14:25:24.275741: step 2341, loss 0.00133163, acc 1\n",
      "2017-09-26T14:25:24.588163: step 2342, loss 0.00740044, acc 1\n",
      "2017-09-26T14:25:24.886011: step 2343, loss 0.00161823, acc 1\n",
      "2017-09-26T14:25:25.178839: step 2344, loss 0.00281183, acc 1\n",
      "2017-09-26T14:25:25.502174: step 2345, loss 0.00182398, acc 1\n",
      "2017-09-26T14:25:25.745010: step 2346, loss 0.00264978, acc 1\n",
      "2017-09-26T14:25:25.995783: step 2347, loss 0.00137209, acc 1\n",
      "2017-09-26T14:25:26.240618: step 2348, loss 0.00415322, acc 1\n",
      "2017-09-26T14:25:26.488070: step 2349, loss 0.00873488, acc 1\n",
      "2017-09-26T14:25:26.755479: step 2350, loss 0.00842555, acc 1\n",
      "2017-09-26T14:25:26.995693: step 2351, loss 0.0156726, acc 0.984375\n",
      "2017-09-26T14:25:27.234006: step 2352, loss 0.00415992, acc 1\n",
      "2017-09-26T14:25:27.536613: step 2353, loss 0.00428867, acc 1\n",
      "2017-09-26T14:25:27.842820: step 2354, loss 0.00231691, acc 1\n",
      "2017-09-26T14:25:28.155864: step 2355, loss 0.00186626, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:25:28.482960: step 2356, loss 0.00265046, acc 1\n",
      "2017-09-26T14:25:28.807152: step 2357, loss 0.00135288, acc 1\n",
      "2017-09-26T14:25:29.072318: step 2358, loss 0.00449475, acc 1\n",
      "2017-09-26T14:25:29.455493: step 2359, loss 0.00224684, acc 1\n",
      "2017-09-26T14:25:29.789224: step 2360, loss 0.00397075, acc 1\n",
      "2017-09-26T14:25:30.121308: step 2361, loss 0.00869252, acc 1\n",
      "2017-09-26T14:25:30.370944: step 2362, loss 0.00129149, acc 1\n",
      "2017-09-26T14:25:30.600608: step 2363, loss 0.00834816, acc 1\n",
      "2017-09-26T14:25:30.836567: step 2364, loss 0.0010444, acc 1\n",
      "2017-09-26T14:25:31.094030: step 2365, loss 0.0012165, acc 1\n",
      "2017-09-26T14:25:31.336440: step 2366, loss 0.00172225, acc 1\n",
      "2017-09-26T14:25:31.666092: step 2367, loss 0.0101548, acc 1\n",
      "2017-09-26T14:25:32.023885: step 2368, loss 0.00388692, acc 1\n",
      "2017-09-26T14:25:32.303720: step 2369, loss 0.0017889, acc 1\n",
      "2017-09-26T14:25:32.604499: step 2370, loss 0.00449039, acc 1\n",
      "2017-09-26T14:25:32.869303: step 2371, loss 0.00199004, acc 1\n",
      "2017-09-26T14:25:33.100015: step 2372, loss 0.00171245, acc 1\n",
      "2017-09-26T14:25:33.372642: step 2373, loss 0.00490784, acc 1\n",
      "2017-09-26T14:25:33.709108: step 2374, loss 0.00102801, acc 1\n",
      "2017-09-26T14:25:34.030319: step 2375, loss 0.0117799, acc 1\n",
      "2017-09-26T14:25:34.305071: step 2376, loss 0.00112874, acc 1\n",
      "2017-09-26T14:25:34.571645: step 2377, loss 0.00076483, acc 1\n",
      "2017-09-26T14:25:34.810998: step 2378, loss 0.00506038, acc 1\n",
      "2017-09-26T14:25:35.058126: step 2379, loss 0.00330536, acc 1\n",
      "2017-09-26T14:25:35.312041: step 2380, loss 0.00272405, acc 1\n",
      "2017-09-26T14:25:35.558796: step 2381, loss 0.0431675, acc 0.984375\n",
      "2017-09-26T14:25:35.795514: step 2382, loss 0.00225404, acc 1\n",
      "2017-09-26T14:25:36.048352: step 2383, loss 0.0120585, acc 1\n",
      "2017-09-26T14:25:36.277643: step 2384, loss 0.00250969, acc 1\n",
      "2017-09-26T14:25:36.528456: step 2385, loss 0.00368977, acc 1\n",
      "2017-09-26T14:25:36.777136: step 2386, loss 0.00175678, acc 1\n",
      "2017-09-26T14:25:37.017806: step 2387, loss 0.000376822, acc 1\n",
      "2017-09-26T14:25:37.268400: step 2388, loss 0.00653816, acc 1\n",
      "2017-09-26T14:25:37.516997: step 2389, loss 0.00546448, acc 1\n",
      "2017-09-26T14:25:37.754611: step 2390, loss 0.00450341, acc 1\n",
      "2017-09-26T14:25:38.003543: step 2391, loss 0.0064226, acc 1\n",
      "2017-09-26T14:25:38.295616: step 2392, loss 0.00722651, acc 1\n",
      "2017-09-26T14:25:38.606057: step 2393, loss 0.00128052, acc 1\n",
      "2017-09-26T14:25:38.811264: step 2394, loss 0.00251212, acc 1\n",
      "2017-09-26T14:25:39.091327: step 2395, loss 0.00313385, acc 1\n",
      "2017-09-26T14:25:39.380107: step 2396, loss 0.0109779, acc 1\n",
      "2017-09-26T14:25:39.726445: step 2397, loss 0.000965499, acc 1\n",
      "2017-09-26T14:25:40.038669: step 2398, loss 0.00198974, acc 1\n",
      "2017-09-26T14:25:40.340166: step 2399, loss 0.00327351, acc 1\n",
      "2017-09-26T14:25:40.597374: step 2400, loss 0.00614036, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:25:40.898116: step 2400, loss 0.508155, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2400\n",
      "\n",
      "2017-09-26T14:25:41.447225: step 2401, loss 0.00976224, acc 1\n",
      "2017-09-26T14:25:41.734101: step 2402, loss 0.00332629, acc 1\n",
      "2017-09-26T14:25:42.020738: step 2403, loss 0.000809778, acc 1\n",
      "2017-09-26T14:25:42.360661: step 2404, loss 0.00266885, acc 1\n",
      "2017-09-26T14:25:42.686886: step 2405, loss 0.00238433, acc 1\n",
      "2017-09-26T14:25:42.958088: step 2406, loss 0.00242629, acc 1\n",
      "2017-09-26T14:25:43.238211: step 2407, loss 0.00767318, acc 1\n",
      "2017-09-26T14:25:43.531273: step 2408, loss 0.00417223, acc 1\n",
      "2017-09-26T14:25:43.823440: step 2409, loss 0.00153344, acc 1\n",
      "2017-09-26T14:25:44.167424: step 2410, loss 0.00112168, acc 1\n",
      "2017-09-26T14:25:44.550692: step 2411, loss 0.00471917, acc 1\n",
      "2017-09-26T14:25:44.875220: step 2412, loss 0.00185522, acc 1\n",
      "2017-09-26T14:25:45.198132: step 2413, loss 0.017133, acc 1\n",
      "2017-09-26T14:25:45.593451: step 2414, loss 0.00170683, acc 1\n",
      "2017-09-26T14:25:46.023781: step 2415, loss 0.00135094, acc 1\n",
      "2017-09-26T14:25:46.458930: step 2416, loss 0.000712872, acc 1\n",
      "2017-09-26T14:25:46.832091: step 2417, loss 0.0010489, acc 1\n",
      "2017-09-26T14:25:47.215033: step 2418, loss 0.0129196, acc 1\n",
      "2017-09-26T14:25:47.644472: step 2419, loss 0.000700002, acc 1\n",
      "2017-09-26T14:25:47.994052: step 2420, loss 0.00401459, acc 1\n",
      "2017-09-26T14:25:48.362377: step 2421, loss 0.0104354, acc 1\n",
      "2017-09-26T14:25:48.706746: step 2422, loss 0.0632336, acc 0.984375\n",
      "2017-09-26T14:25:49.079382: step 2423, loss 0.0112312, acc 1\n",
      "2017-09-26T14:25:49.464893: step 2424, loss 0.00174837, acc 1\n",
      "2017-09-26T14:25:49.860498: step 2425, loss 0.00531812, acc 1\n",
      "2017-09-26T14:25:50.177324: step 2426, loss 0.00112385, acc 1\n",
      "2017-09-26T14:25:50.500799: step 2427, loss 0.00301357, acc 1\n",
      "2017-09-26T14:25:50.790125: step 2428, loss 0.00134212, acc 1\n",
      "2017-09-26T14:25:51.079050: step 2429, loss 0.0143547, acc 0.984375\n",
      "2017-09-26T14:25:51.359365: step 2430, loss 0.0623254, acc 0.984375\n",
      "2017-09-26T14:25:51.650351: step 2431, loss 0.00217431, acc 1\n",
      "2017-09-26T14:25:51.949925: step 2432, loss 0.00309578, acc 1\n",
      "2017-09-26T14:25:52.239811: step 2433, loss 0.0395699, acc 0.984375\n",
      "2017-09-26T14:25:52.507479: step 2434, loss 0.00467043, acc 1\n",
      "2017-09-26T14:25:52.807368: step 2435, loss 0.0135469, acc 1\n",
      "2017-09-26T14:25:53.082206: step 2436, loss 0.0011434, acc 1\n",
      "2017-09-26T14:25:53.388363: step 2437, loss 0.00359194, acc 1\n",
      "2017-09-26T14:25:53.698287: step 2438, loss 0.0014501, acc 1\n",
      "2017-09-26T14:25:53.997028: step 2439, loss 0.0018643, acc 1\n",
      "2017-09-26T14:25:54.324965: step 2440, loss 0.00160781, acc 1\n",
      "2017-09-26T14:25:54.635171: step 2441, loss 0.013869, acc 1\n",
      "2017-09-26T14:25:54.934641: step 2442, loss 0.00374558, acc 1\n",
      "2017-09-26T14:25:55.215305: step 2443, loss 0.00224188, acc 1\n",
      "2017-09-26T14:25:55.514771: step 2444, loss 0.00481174, acc 1\n",
      "2017-09-26T14:25:55.790937: step 2445, loss 0.0228945, acc 0.984375\n",
      "2017-09-26T14:25:56.080403: step 2446, loss 0.00231769, acc 1\n",
      "2017-09-26T14:25:56.365684: step 2447, loss 0.0103162, acc 1\n",
      "2017-09-26T14:25:56.693194: step 2448, loss 0.00560417, acc 1\n",
      "2017-09-26T14:25:57.014601: step 2449, loss 0.000637456, acc 1\n",
      "2017-09-26T14:25:57.332930: step 2450, loss 0.00261956, acc 1\n",
      "2017-09-26T14:25:57.600233: step 2451, loss 0.00273014, acc 1\n",
      "2017-09-26T14:25:57.974119: step 2452, loss 0.00531886, acc 1\n",
      "2017-09-26T14:25:58.353437: step 2453, loss 0.0054581, acc 1\n",
      "2017-09-26T14:25:58.699591: step 2454, loss 0.0843265, acc 0.96875\n",
      "2017-09-26T14:25:59.086314: step 2455, loss 0.00640166, acc 1\n",
      "2017-09-26T14:25:59.382185: step 2456, loss 0.0127452, acc 1\n",
      "2017-09-26T14:25:59.661135: step 2457, loss 0.00484575, acc 1\n",
      "2017-09-26T14:25:59.962007: step 2458, loss 0.00384814, acc 1\n",
      "2017-09-26T14:26:00.350297: step 2459, loss 0.00110345, acc 1\n",
      "2017-09-26T14:26:00.734942: step 2460, loss 0.000549342, acc 1\n",
      "2017-09-26T14:26:01.124106: step 2461, loss 0.00150611, acc 1\n",
      "2017-09-26T14:26:01.506919: step 2462, loss 0.00376699, acc 1\n",
      "2017-09-26T14:26:01.868079: step 2463, loss 0.00213661, acc 1\n",
      "2017-09-26T14:26:02.222605: step 2464, loss 0.00386835, acc 1\n",
      "2017-09-26T14:26:02.531068: step 2465, loss 0.00457996, acc 1\n",
      "2017-09-26T14:26:02.780954: step 2466, loss 0.00236532, acc 1\n",
      "2017-09-26T14:26:03.048754: step 2467, loss 0.00777149, acc 1\n",
      "2017-09-26T14:26:03.325989: step 2468, loss 0.00187091, acc 1\n",
      "2017-09-26T14:26:03.584373: step 2469, loss 0.00422343, acc 1\n",
      "2017-09-26T14:26:03.836728: step 2470, loss 0.00210743, acc 1\n",
      "2017-09-26T14:26:04.093867: step 2471, loss 0.00241949, acc 1\n",
      "2017-09-26T14:26:04.353595: step 2472, loss 0.00378052, acc 1\n",
      "2017-09-26T14:26:04.613763: step 2473, loss 0.000996797, acc 1\n",
      "2017-09-26T14:26:04.872964: step 2474, loss 0.00107587, acc 1\n",
      "2017-09-26T14:26:05.131317: step 2475, loss 0.00726752, acc 1\n",
      "2017-09-26T14:26:05.394955: step 2476, loss 0.00483869, acc 1\n",
      "2017-09-26T14:26:05.647932: step 2477, loss 0.00857242, acc 1\n",
      "2017-09-26T14:26:05.868954: step 2478, loss 0.00634806, acc 1\n",
      "2017-09-26T14:26:06.132288: step 2479, loss 0.00909295, acc 1\n",
      "2017-09-26T14:26:06.385347: step 2480, loss 0.00205198, acc 1\n",
      "2017-09-26T14:26:06.638001: step 2481, loss 0.0030351, acc 1\n",
      "2017-09-26T14:26:06.915233: step 2482, loss 0.0020665, acc 1\n",
      "2017-09-26T14:26:07.173670: step 2483, loss 0.000708767, acc 1\n",
      "2017-09-26T14:26:07.430590: step 2484, loss 0.0162677, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:26:07.700956: step 2485, loss 0.00118249, acc 1\n",
      "2017-09-26T14:26:07.957787: step 2486, loss 0.000919443, acc 1\n",
      "2017-09-26T14:26:08.282131: step 2487, loss 0.00520503, acc 1\n",
      "2017-09-26T14:26:08.615275: step 2488, loss 0.00437382, acc 1\n",
      "2017-09-26T14:26:08.859121: step 2489, loss 0.0139044, acc 1\n",
      "2017-09-26T14:26:09.124907: step 2490, loss 0.0187541, acc 0.984375\n",
      "2017-09-26T14:26:09.419025: step 2491, loss 0.00133534, acc 1\n",
      "2017-09-26T14:26:09.666223: step 2492, loss 0.00108163, acc 1\n",
      "2017-09-26T14:26:09.924312: step 2493, loss 0.00593605, acc 1\n",
      "2017-09-26T14:26:10.173795: step 2494, loss 0.00152128, acc 1\n",
      "2017-09-26T14:26:10.462954: step 2495, loss 0.00159417, acc 1\n",
      "2017-09-26T14:26:10.713148: step 2496, loss 0.00266348, acc 1\n",
      "2017-09-26T14:26:10.967332: step 2497, loss 0.00182736, acc 1\n",
      "2017-09-26T14:26:11.218982: step 2498, loss 0.00579349, acc 1\n",
      "2017-09-26T14:26:11.481018: step 2499, loss 0.00317446, acc 1\n",
      "2017-09-26T14:26:11.723773: step 2500, loss 0.00137687, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:26:11.989460: step 2500, loss 0.484388, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2500\n",
      "\n",
      "2017-09-26T14:26:12.525856: step 2501, loss 0.00280161, acc 1\n",
      "2017-09-26T14:26:12.789748: step 2502, loss 0.00215607, acc 1\n",
      "2017-09-26T14:26:13.051386: step 2503, loss 0.00110566, acc 1\n",
      "2017-09-26T14:26:13.310240: step 2504, loss 0.0728345, acc 0.984375\n",
      "2017-09-26T14:26:13.563615: step 2505, loss 0.00202811, acc 1\n",
      "2017-09-26T14:26:13.849720: step 2506, loss 0.000290376, acc 1\n",
      "2017-09-26T14:26:14.110018: step 2507, loss 0.00366606, acc 1\n",
      "2017-09-26T14:26:14.370748: step 2508, loss 0.0158636, acc 1\n",
      "2017-09-26T14:26:14.660454: step 2509, loss 0.00171528, acc 1\n",
      "2017-09-26T14:26:15.005214: step 2510, loss 0.00678276, acc 1\n",
      "2017-09-26T14:26:15.334370: step 2511, loss 0.00329927, acc 1\n",
      "2017-09-26T14:26:15.594853: step 2512, loss 0.00114402, acc 1\n",
      "2017-09-26T14:26:15.893476: step 2513, loss 0.00136164, acc 1\n",
      "2017-09-26T14:26:16.168740: step 2514, loss 0.00301188, acc 1\n",
      "2017-09-26T14:26:16.421066: step 2515, loss 0.00444851, acc 1\n",
      "2017-09-26T14:26:16.693643: step 2516, loss 0.00526771, acc 1\n",
      "2017-09-26T14:26:16.942801: step 2517, loss 0.00546274, acc 1\n",
      "2017-09-26T14:26:17.224201: step 2518, loss 0.00101025, acc 1\n",
      "2017-09-26T14:26:17.590538: step 2519, loss 0.00146749, acc 1\n",
      "2017-09-26T14:26:17.845922: step 2520, loss 0.00118417, acc 1\n",
      "2017-09-26T14:26:18.185649: step 2521, loss 0.00461291, acc 1\n",
      "2017-09-26T14:26:18.448564: step 2522, loss 0.000864094, acc 1\n",
      "2017-09-26T14:26:18.710591: step 2523, loss 0.000833077, acc 1\n",
      "2017-09-26T14:26:18.983682: step 2524, loss 0.00302826, acc 1\n",
      "2017-09-26T14:26:19.278174: step 2525, loss 0.00146581, acc 1\n",
      "2017-09-26T14:26:19.566703: step 2526, loss 0.00213311, acc 1\n",
      "2017-09-26T14:26:19.842688: step 2527, loss 0.0026344, acc 1\n",
      "2017-09-26T14:26:20.107373: step 2528, loss 0.0102573, acc 1\n",
      "2017-09-26T14:26:20.375967: step 2529, loss 0.00938746, acc 1\n",
      "2017-09-26T14:26:20.654131: step 2530, loss 0.00600729, acc 1\n",
      "2017-09-26T14:26:20.937964: step 2531, loss 0.00482811, acc 1\n",
      "2017-09-26T14:26:21.221925: step 2532, loss 0.00460128, acc 1\n",
      "2017-09-26T14:26:21.508156: step 2533, loss 0.00517885, acc 1\n",
      "2017-09-26T14:26:21.796650: step 2534, loss 0.00242187, acc 1\n",
      "2017-09-26T14:26:22.076588: step 2535, loss 0.000917159, acc 1\n",
      "2017-09-26T14:26:22.354407: step 2536, loss 0.00247103, acc 1\n",
      "2017-09-26T14:26:22.704649: step 2537, loss 0.00112458, acc 1\n",
      "2017-09-26T14:26:22.998192: step 2538, loss 0.00127338, acc 1\n",
      "2017-09-26T14:26:23.370433: step 2539, loss 0.00166754, acc 1\n",
      "2017-09-26T14:26:23.646173: step 2540, loss 0.00217922, acc 1\n",
      "2017-09-26T14:26:23.925813: step 2541, loss 0.00895412, acc 1\n",
      "2017-09-26T14:26:24.227850: step 2542, loss 0.00168811, acc 1\n",
      "2017-09-26T14:26:24.522302: step 2543, loss 0.00299257, acc 1\n",
      "2017-09-26T14:26:24.806571: step 2544, loss 0.00142482, acc 1\n",
      "2017-09-26T14:26:25.075345: step 2545, loss 0.00406201, acc 1\n",
      "2017-09-26T14:26:25.337424: step 2546, loss 0.00167264, acc 1\n",
      "2017-09-26T14:26:25.614305: step 2547, loss 0.00133001, acc 1\n",
      "2017-09-26T14:26:25.875889: step 2548, loss 0.00349468, acc 1\n",
      "2017-09-26T14:26:26.145214: step 2549, loss 0.00083974, acc 1\n",
      "2017-09-26T14:26:26.388973: step 2550, loss 0.00436509, acc 1\n",
      "2017-09-26T14:26:26.665990: step 2551, loss 0.000928203, acc 1\n",
      "2017-09-26T14:26:26.918667: step 2552, loss 0.00158748, acc 1\n",
      "2017-09-26T14:26:27.185428: step 2553, loss 0.00136824, acc 1\n",
      "2017-09-26T14:26:27.445343: step 2554, loss 0.000417726, acc 1\n",
      "2017-09-26T14:26:27.720434: step 2555, loss 0.016855, acc 0.984375\n",
      "2017-09-26T14:26:27.979876: step 2556, loss 0.00376197, acc 1\n",
      "2017-09-26T14:26:28.258895: step 2557, loss 0.00296751, acc 1\n",
      "2017-09-26T14:26:28.562666: step 2558, loss 0.00225227, acc 1\n",
      "2017-09-26T14:26:28.903202: step 2559, loss 0.00255061, acc 1\n",
      "2017-09-26T14:26:29.212940: step 2560, loss 0.00215126, acc 1\n",
      "2017-09-26T14:26:29.497889: step 2561, loss 0.000316703, acc 1\n",
      "2017-09-26T14:26:29.721684: step 2562, loss 0.00114992, acc 1\n",
      "2017-09-26T14:26:30.043746: step 2563, loss 0.0032527, acc 1\n",
      "2017-09-26T14:26:30.386587: step 2564, loss 0.00161137, acc 1\n",
      "2017-09-26T14:26:30.681334: step 2565, loss 0.000621224, acc 1\n",
      "2017-09-26T14:26:31.006430: step 2566, loss 0.00294006, acc 1\n",
      "2017-09-26T14:26:31.291200: step 2567, loss 0.00169407, acc 1\n",
      "2017-09-26T14:26:31.630693: step 2568, loss 0.00363805, acc 1\n",
      "2017-09-26T14:26:31.923318: step 2569, loss 0.00104442, acc 1\n",
      "2017-09-26T14:26:32.244340: step 2570, loss 0.00188432, acc 1\n",
      "2017-09-26T14:26:32.587510: step 2571, loss 0.00130531, acc 1\n",
      "2017-09-26T14:26:32.857027: step 2572, loss 0.00144455, acc 1\n",
      "2017-09-26T14:26:33.135710: step 2573, loss 0.000543981, acc 1\n",
      "2017-09-26T14:26:33.498240: step 2574, loss 0.00316398, acc 1\n",
      "2017-09-26T14:26:33.840000: step 2575, loss 0.00193971, acc 1\n",
      "2017-09-26T14:26:34.203618: step 2576, loss 0.00197864, acc 1\n",
      "2017-09-26T14:26:34.485916: step 2577, loss 0.00127485, acc 1\n",
      "2017-09-26T14:26:34.746048: step 2578, loss 0.0107598, acc 1\n",
      "2017-09-26T14:26:35.000030: step 2579, loss 0.000612761, acc 1\n",
      "2017-09-26T14:26:35.251942: step 2580, loss 0.000950746, acc 1\n",
      "2017-09-26T14:26:35.505979: step 2581, loss 0.00351428, acc 1\n",
      "2017-09-26T14:26:35.774753: step 2582, loss 0.0128752, acc 1\n",
      "2017-09-26T14:26:36.045455: step 2583, loss 0.00799138, acc 1\n",
      "2017-09-26T14:26:36.330166: step 2584, loss 0.000526187, acc 1\n",
      "2017-09-26T14:26:36.587021: step 2585, loss 0.0700011, acc 0.984375\n",
      "2017-09-26T14:26:36.879965: step 2586, loss 0.00296005, acc 1\n",
      "2017-09-26T14:26:37.135685: step 2587, loss 0.0011548, acc 1\n",
      "2017-09-26T14:26:37.398275: step 2588, loss 0.000319317, acc 1\n",
      "2017-09-26T14:26:37.690211: step 2589, loss 0.00112874, acc 1\n",
      "2017-09-26T14:26:38.013861: step 2590, loss 0.00145891, acc 1\n",
      "2017-09-26T14:26:38.368267: step 2591, loss 0.000851525, acc 1\n",
      "2017-09-26T14:26:38.633390: step 2592, loss 0.00346483, acc 1\n",
      "2017-09-26T14:26:38.926772: step 2593, loss 0.00182481, acc 1\n",
      "2017-09-26T14:26:39.250217: step 2594, loss 0.00145855, acc 1\n",
      "2017-09-26T14:26:39.594751: step 2595, loss 0.00102644, acc 1\n",
      "2017-09-26T14:26:39.904221: step 2596, loss 0.0042593, acc 1\n",
      "2017-09-26T14:26:40.165496: step 2597, loss 0.00123056, acc 1\n",
      "2017-09-26T14:26:40.516218: step 2598, loss 0.00190187, acc 1\n",
      "2017-09-26T14:26:40.859758: step 2599, loss 0.00137297, acc 1\n",
      "2017-09-26T14:26:41.203924: step 2600, loss 0.00298183, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:26:41.449196: step 2600, loss 0.49886, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2600\n",
      "\n",
      "2017-09-26T14:26:41.973848: step 2601, loss 0.00283895, acc 1\n",
      "2017-09-26T14:26:42.318988: step 2602, loss 0.0076543, acc 1\n",
      "2017-09-26T14:26:42.665974: step 2603, loss 0.00852705, acc 1\n",
      "2017-09-26T14:26:42.951896: step 2604, loss 0.000670817, acc 1\n",
      "2017-09-26T14:26:43.297084: step 2605, loss 0.00113864, acc 1\n",
      "2017-09-26T14:26:43.599381: step 2606, loss 0.00128994, acc 1\n",
      "2017-09-26T14:26:43.849146: step 2607, loss 0.00382267, acc 1\n",
      "2017-09-26T14:26:44.106173: step 2608, loss 0.00222932, acc 1\n",
      "2017-09-26T14:26:44.369192: step 2609, loss 0.00184221, acc 1\n",
      "2017-09-26T14:26:44.622757: step 2610, loss 0.000490554, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:26:44.869364: step 2611, loss 0.000265609, acc 1\n",
      "2017-09-26T14:26:45.123199: step 2612, loss 0.0130219, acc 1\n",
      "2017-09-26T14:26:45.390548: step 2613, loss 0.00363611, acc 1\n",
      "2017-09-26T14:26:45.724946: step 2614, loss 0.00332345, acc 1\n",
      "2017-09-26T14:26:46.052065: step 2615, loss 0.000469613, acc 1\n",
      "2017-09-26T14:26:46.386404: step 2616, loss 0.00164713, acc 1\n",
      "2017-09-26T14:26:46.641408: step 2617, loss 0.00126013, acc 1\n",
      "2017-09-26T14:26:46.881049: step 2618, loss 0.000834292, acc 1\n",
      "2017-09-26T14:26:47.124345: step 2619, loss 0.00163429, acc 1\n",
      "2017-09-26T14:26:47.363167: step 2620, loss 0.0181545, acc 0.984375\n",
      "2017-09-26T14:26:47.616903: step 2621, loss 0.00249775, acc 1\n",
      "2017-09-26T14:26:47.861800: step 2622, loss 0.0582887, acc 0.984375\n",
      "2017-09-26T14:26:48.113939: step 2623, loss 0.00186661, acc 1\n",
      "2017-09-26T14:26:48.363190: step 2624, loss 0.00118089, acc 1\n",
      "2017-09-26T14:26:48.603144: step 2625, loss 0.00183227, acc 1\n",
      "2017-09-26T14:26:48.842888: step 2626, loss 0.0115326, acc 1\n",
      "2017-09-26T14:26:49.085215: step 2627, loss 0.0199637, acc 0.984375\n",
      "2017-09-26T14:26:49.331410: step 2628, loss 0.000484075, acc 1\n",
      "2017-09-26T14:26:49.569903: step 2629, loss 0.00639164, acc 1\n",
      "2017-09-26T14:26:49.815476: step 2630, loss 0.00457935, acc 1\n",
      "2017-09-26T14:26:50.067834: step 2631, loss 0.00277142, acc 1\n",
      "2017-09-26T14:26:50.313265: step 2632, loss 0.00272893, acc 1\n",
      "2017-09-26T14:26:50.558161: step 2633, loss 0.00222901, acc 1\n",
      "2017-09-26T14:26:50.805151: step 2634, loss 0.00108601, acc 1\n",
      "2017-09-26T14:26:51.046040: step 2635, loss 0.058405, acc 0.984375\n",
      "2017-09-26T14:26:51.288399: step 2636, loss 0.00356644, acc 1\n",
      "2017-09-26T14:26:51.531139: step 2637, loss 0.00217198, acc 1\n",
      "2017-09-26T14:26:51.776273: step 2638, loss 0.0018042, acc 1\n",
      "2017-09-26T14:26:52.024182: step 2639, loss 0.00233334, acc 1\n",
      "2017-09-26T14:26:52.266304: step 2640, loss 0.00339204, acc 1\n",
      "2017-09-26T14:26:52.513031: step 2641, loss 0.00143022, acc 1\n",
      "2017-09-26T14:26:52.752741: step 2642, loss 0.00156318, acc 1\n",
      "2017-09-26T14:26:52.993277: step 2643, loss 0.00103903, acc 1\n",
      "2017-09-26T14:26:53.242740: step 2644, loss 0.00604889, acc 1\n",
      "2017-09-26T14:26:53.482050: step 2645, loss 0.00105414, acc 1\n",
      "2017-09-26T14:26:53.689955: step 2646, loss 0.00241011, acc 1\n",
      "2017-09-26T14:26:53.943959: step 2647, loss 0.00315873, acc 1\n",
      "2017-09-26T14:26:54.187977: step 2648, loss 0.00152247, acc 1\n",
      "2017-09-26T14:26:54.437594: step 2649, loss 0.000821985, acc 1\n",
      "2017-09-26T14:26:54.683049: step 2650, loss 0.00192278, acc 1\n",
      "2017-09-26T14:26:54.926547: step 2651, loss 0.00047784, acc 1\n",
      "2017-09-26T14:26:55.168740: step 2652, loss 0.000801446, acc 1\n",
      "2017-09-26T14:26:55.412129: step 2653, loss 0.0233334, acc 0.984375\n",
      "2017-09-26T14:26:55.652686: step 2654, loss 0.000645577, acc 1\n",
      "2017-09-26T14:26:55.897798: step 2655, loss 0.0403019, acc 0.984375\n",
      "2017-09-26T14:26:56.159237: step 2656, loss 0.00185993, acc 1\n",
      "2017-09-26T14:26:56.399112: step 2657, loss 0.00240079, acc 1\n",
      "2017-09-26T14:26:56.650368: step 2658, loss 0.00149301, acc 1\n",
      "2017-09-26T14:26:56.890861: step 2659, loss 0.000801989, acc 1\n",
      "2017-09-26T14:26:57.135104: step 2660, loss 0.00570293, acc 1\n",
      "2017-09-26T14:26:57.382740: step 2661, loss 0.00541662, acc 1\n",
      "2017-09-26T14:26:57.628476: step 2662, loss 0.0133847, acc 0.984375\n",
      "2017-09-26T14:26:57.878559: step 2663, loss 0.00385445, acc 1\n",
      "2017-09-26T14:26:58.148920: step 2664, loss 0.0058344, acc 1\n",
      "2017-09-26T14:26:58.406778: step 2665, loss 0.00415949, acc 1\n",
      "2017-09-26T14:26:58.670010: step 2666, loss 0.00769628, acc 1\n",
      "2017-09-26T14:26:58.929734: step 2667, loss 0.000962294, acc 1\n",
      "2017-09-26T14:26:59.197441: step 2668, loss 0.00101512, acc 1\n",
      "2017-09-26T14:26:59.562461: step 2669, loss 0.00218284, acc 1\n",
      "2017-09-26T14:26:59.919449: step 2670, loss 0.0027384, acc 1\n",
      "2017-09-26T14:27:00.252835: step 2671, loss 0.00307736, acc 1\n",
      "2017-09-26T14:27:00.593856: step 2672, loss 0.000561456, acc 1\n",
      "2017-09-26T14:27:00.944920: step 2673, loss 0.0017719, acc 1\n",
      "2017-09-26T14:27:01.276179: step 2674, loss 0.00405031, acc 1\n",
      "2017-09-26T14:27:01.653312: step 2675, loss 0.0126927, acc 1\n",
      "2017-09-26T14:27:01.999137: step 2676, loss 0.00125846, acc 1\n",
      "2017-09-26T14:27:02.342622: step 2677, loss 0.00564965, acc 1\n",
      "2017-09-26T14:27:02.605324: step 2678, loss 0.00210867, acc 1\n",
      "2017-09-26T14:27:02.850934: step 2679, loss 0.00230875, acc 1\n",
      "2017-09-26T14:27:03.102558: step 2680, loss 0.0108194, acc 1\n",
      "2017-09-26T14:27:03.348582: step 2681, loss 0.00107836, acc 1\n",
      "2017-09-26T14:27:03.597506: step 2682, loss 0.00154246, acc 1\n",
      "2017-09-26T14:27:03.848298: step 2683, loss 0.000361469, acc 1\n",
      "2017-09-26T14:27:04.138132: step 2684, loss 0.00144206, acc 1\n",
      "2017-09-26T14:27:04.500391: step 2685, loss 0.00138157, acc 1\n",
      "2017-09-26T14:27:04.776213: step 2686, loss 0.00194694, acc 1\n",
      "2017-09-26T14:27:05.022800: step 2687, loss 0.050688, acc 0.96875\n",
      "2017-09-26T14:27:05.245823: step 2688, loss 0.0058264, acc 1\n",
      "2017-09-26T14:27:05.515307: step 2689, loss 0.0125272, acc 0.984375\n",
      "2017-09-26T14:27:05.765250: step 2690, loss 0.000728049, acc 1\n",
      "2017-09-26T14:27:06.018844: step 2691, loss 0.000459249, acc 1\n",
      "2017-09-26T14:27:06.305778: step 2692, loss 0.000989327, acc 1\n",
      "2017-09-26T14:27:06.565223: step 2693, loss 0.00110802, acc 1\n",
      "2017-09-26T14:27:06.827681: step 2694, loss 0.0314678, acc 0.984375\n",
      "2017-09-26T14:27:07.098416: step 2695, loss 0.031825, acc 0.984375\n",
      "2017-09-26T14:27:07.360715: step 2696, loss 0.000483518, acc 1\n",
      "2017-09-26T14:27:07.622459: step 2697, loss 0.00152614, acc 1\n",
      "2017-09-26T14:27:07.890018: step 2698, loss 0.00352918, acc 1\n",
      "2017-09-26T14:27:08.158136: step 2699, loss 0.00111752, acc 1\n",
      "2017-09-26T14:27:08.412756: step 2700, loss 0.000409571, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:27:08.672542: step 2700, loss 0.504049, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2700\n",
      "\n",
      "2017-09-26T14:27:09.222645: step 2701, loss 0.00116644, acc 1\n",
      "2017-09-26T14:27:09.486487: step 2702, loss 0.00187068, acc 1\n",
      "2017-09-26T14:27:09.735282: step 2703, loss 0.00267174, acc 1\n",
      "2017-09-26T14:27:10.007902: step 2704, loss 0.00306448, acc 1\n",
      "2017-09-26T14:27:10.265963: step 2705, loss 0.00915659, acc 1\n",
      "2017-09-26T14:27:10.518921: step 2706, loss 0.000570422, acc 1\n",
      "2017-09-26T14:27:10.798313: step 2707, loss 0.003064, acc 1\n",
      "2017-09-26T14:27:11.063320: step 2708, loss 0.0015661, acc 1\n",
      "2017-09-26T14:27:11.324473: step 2709, loss 0.00211583, acc 1\n",
      "2017-09-26T14:27:11.581947: step 2710, loss 0.00248634, acc 1\n",
      "2017-09-26T14:27:11.834846: step 2711, loss 0.0596357, acc 0.984375\n",
      "2017-09-26T14:27:12.097433: step 2712, loss 0.000409596, acc 1\n",
      "2017-09-26T14:27:12.353919: step 2713, loss 0.0026647, acc 1\n",
      "2017-09-26T14:27:12.656703: step 2714, loss 0.000377534, acc 1\n",
      "2017-09-26T14:27:12.944115: step 2715, loss 0.00123935, acc 1\n",
      "2017-09-26T14:27:13.205503: step 2716, loss 0.0129408, acc 1\n",
      "2017-09-26T14:27:13.525210: step 2717, loss 0.00129148, acc 1\n",
      "2017-09-26T14:27:13.792743: step 2718, loss 0.00154569, acc 1\n",
      "2017-09-26T14:27:14.092820: step 2719, loss 0.00280533, acc 1\n",
      "2017-09-26T14:27:14.362359: step 2720, loss 0.00418426, acc 1\n",
      "2017-09-26T14:27:14.620998: step 2721, loss 0.00432689, acc 1\n",
      "2017-09-26T14:27:14.879162: step 2722, loss 0.00827658, acc 1\n",
      "2017-09-26T14:27:15.144039: step 2723, loss 0.00118839, acc 1\n",
      "2017-09-26T14:27:15.400022: step 2724, loss 0.00175954, acc 1\n",
      "2017-09-26T14:27:15.655247: step 2725, loss 0.00618795, acc 1\n",
      "2017-09-26T14:27:15.914083: step 2726, loss 0.0160968, acc 1\n",
      "2017-09-26T14:27:16.190529: step 2727, loss 0.00857231, acc 1\n",
      "2017-09-26T14:27:16.447030: step 2728, loss 0.00258651, acc 1\n",
      "2017-09-26T14:27:16.700610: step 2729, loss 0.00115565, acc 1\n",
      "2017-09-26T14:27:16.944157: step 2730, loss 0.00665798, acc 1\n",
      "2017-09-26T14:27:17.286161: step 2731, loss 0.00137925, acc 1\n",
      "2017-09-26T14:27:17.614157: step 2732, loss 0.000985639, acc 1\n",
      "2017-09-26T14:27:17.947244: step 2733, loss 0.00106948, acc 1\n",
      "2017-09-26T14:27:18.293934: step 2734, loss 0.00250761, acc 1\n",
      "2017-09-26T14:27:18.601004: step 2735, loss 0.00277423, acc 1\n",
      "2017-09-26T14:27:18.891806: step 2736, loss 0.00091396, acc 1\n",
      "2017-09-26T14:27:19.198150: step 2737, loss 0.00236568, acc 1\n",
      "2017-09-26T14:27:19.458129: step 2738, loss 0.00864743, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:27:19.716069: step 2739, loss 0.000419669, acc 1\n",
      "2017-09-26T14:27:20.000273: step 2740, loss 0.00271217, acc 1\n",
      "2017-09-26T14:27:20.333752: step 2741, loss 0.00110956, acc 1\n",
      "2017-09-26T14:27:20.647736: step 2742, loss 0.00280627, acc 1\n",
      "2017-09-26T14:27:20.948617: step 2743, loss 0.00105567, acc 1\n",
      "2017-09-26T14:27:21.277421: step 2744, loss 0.00183378, acc 1\n",
      "2017-09-26T14:27:21.521886: step 2745, loss 0.000769317, acc 1\n",
      "2017-09-26T14:27:21.768219: step 2746, loss 0.000891605, acc 1\n",
      "2017-09-26T14:27:22.106155: step 2747, loss 0.000596227, acc 1\n",
      "2017-09-26T14:27:22.435548: step 2748, loss 0.00103421, acc 1\n",
      "2017-09-26T14:27:22.758692: step 2749, loss 0.0049265, acc 1\n",
      "2017-09-26T14:27:23.082199: step 2750, loss 0.00380098, acc 1\n",
      "2017-09-26T14:27:23.407508: step 2751, loss 0.00208409, acc 1\n",
      "2017-09-26T14:27:23.739051: step 2752, loss 0.00184015, acc 1\n",
      "2017-09-26T14:27:24.056595: step 2753, loss 0.0101713, acc 1\n",
      "2017-09-26T14:27:24.386796: step 2754, loss 0.00233574, acc 1\n",
      "2017-09-26T14:27:24.712113: step 2755, loss 0.00257901, acc 1\n",
      "2017-09-26T14:27:25.031581: step 2756, loss 0.00128969, acc 1\n",
      "2017-09-26T14:27:25.357882: step 2757, loss 0.0018503, acc 1\n",
      "2017-09-26T14:27:25.680644: step 2758, loss 0.013856, acc 1\n",
      "2017-09-26T14:27:25.947610: step 2759, loss 0.000987017, acc 1\n",
      "2017-09-26T14:27:26.269185: step 2760, loss 0.0012821, acc 1\n",
      "2017-09-26T14:27:26.613822: step 2761, loss 0.000414929, acc 1\n",
      "2017-09-26T14:27:26.937471: step 2762, loss 0.00210942, acc 1\n",
      "2017-09-26T14:27:27.254002: step 2763, loss 0.00102684, acc 1\n",
      "2017-09-26T14:27:27.520212: step 2764, loss 0.00208784, acc 1\n",
      "2017-09-26T14:27:27.844546: step 2765, loss 0.0215244, acc 0.984375\n",
      "2017-09-26T14:27:28.175944: step 2766, loss 0.00363093, acc 1\n",
      "2017-09-26T14:27:28.528184: step 2767, loss 0.00129828, acc 1\n",
      "2017-09-26T14:27:28.862350: step 2768, loss 0.000763043, acc 1\n",
      "2017-09-26T14:27:29.183554: step 2769, loss 0.000439419, acc 1\n",
      "2017-09-26T14:27:29.504776: step 2770, loss 0.000675238, acc 1\n",
      "2017-09-26T14:27:29.824922: step 2771, loss 0.00700716, acc 1\n",
      "2017-09-26T14:27:30.034926: step 2772, loss 0.00445321, acc 1\n",
      "2017-09-26T14:27:30.306766: step 2773, loss 0.00501959, acc 1\n",
      "2017-09-26T14:27:30.612314: step 2774, loss 0.0341087, acc 0.984375\n",
      "2017-09-26T14:27:30.928006: step 2775, loss 0.00217959, acc 1\n",
      "2017-09-26T14:27:31.255059: step 2776, loss 0.00291456, acc 1\n",
      "2017-09-26T14:27:31.591286: step 2777, loss 0.00118891, acc 1\n",
      "2017-09-26T14:27:31.916134: step 2778, loss 0.0819346, acc 0.984375\n",
      "2017-09-26T14:27:32.192057: step 2779, loss 0.0122226, acc 1\n",
      "2017-09-26T14:27:32.440037: step 2780, loss 0.00367805, acc 1\n",
      "2017-09-26T14:27:32.683083: step 2781, loss 0.00255544, acc 1\n",
      "2017-09-26T14:27:32.938005: step 2782, loss 0.015939, acc 0.984375\n",
      "2017-09-26T14:27:33.201637: step 2783, loss 0.00278613, acc 1\n",
      "2017-09-26T14:27:33.530423: step 2784, loss 0.00201711, acc 1\n",
      "2017-09-26T14:27:33.871314: step 2785, loss 0.00377049, acc 1\n",
      "2017-09-26T14:27:34.192709: step 2786, loss 0.00100244, acc 1\n",
      "2017-09-26T14:27:34.507144: step 2787, loss 0.00276953, acc 1\n",
      "2017-09-26T14:27:34.830331: step 2788, loss 0.000618623, acc 1\n",
      "2017-09-26T14:27:35.073237: step 2789, loss 0.000340066, acc 1\n",
      "2017-09-26T14:27:35.325750: step 2790, loss 0.000581433, acc 1\n",
      "2017-09-26T14:27:35.573541: step 2791, loss 0.00221507, acc 1\n",
      "2017-09-26T14:27:35.912976: step 2792, loss 0.00459542, acc 1\n",
      "2017-09-26T14:27:36.238355: step 2793, loss 0.00953028, acc 1\n",
      "2017-09-26T14:27:36.511706: step 2794, loss 0.00593299, acc 1\n",
      "2017-09-26T14:27:36.770622: step 2795, loss 0.00437368, acc 1\n",
      "2017-09-26T14:27:37.024903: step 2796, loss 0.00335502, acc 1\n",
      "2017-09-26T14:27:37.287287: step 2797, loss 0.00543151, acc 1\n",
      "2017-09-26T14:27:37.538172: step 2798, loss 0.000399392, acc 1\n",
      "2017-09-26T14:27:37.792273: step 2799, loss 0.000849526, acc 1\n",
      "2017-09-26T14:27:38.045975: step 2800, loss 0.0010025, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:27:38.312204: step 2800, loss 0.515192, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2800\n",
      "\n",
      "2017-09-26T14:27:38.861500: step 2801, loss 0.0566291, acc 0.984375\n",
      "2017-09-26T14:27:39.112027: step 2802, loss 0.0113918, acc 1\n",
      "2017-09-26T14:27:39.407535: step 2803, loss 0.00234895, acc 1\n",
      "2017-09-26T14:27:39.661138: step 2804, loss 0.000949923, acc 1\n",
      "2017-09-26T14:27:39.917741: step 2805, loss 0.0011186, acc 1\n",
      "2017-09-26T14:27:40.211505: step 2806, loss 0.000912285, acc 1\n",
      "2017-09-26T14:27:40.453867: step 2807, loss 0.00691947, acc 1\n",
      "2017-09-26T14:27:40.720930: step 2808, loss 0.00186501, acc 1\n",
      "2017-09-26T14:27:41.000372: step 2809, loss 0.00156609, acc 1\n",
      "2017-09-26T14:27:41.346752: step 2810, loss 0.0026596, acc 1\n",
      "2017-09-26T14:27:41.677919: step 2811, loss 0.0035849, acc 1\n",
      "2017-09-26T14:27:41.973927: step 2812, loss 0.0016248, acc 1\n",
      "2017-09-26T14:27:42.319316: step 2813, loss 0.00267982, acc 1\n",
      "2017-09-26T14:27:42.581882: step 2814, loss 0.00491161, acc 1\n",
      "2017-09-26T14:27:42.887352: step 2815, loss 0.000748515, acc 1\n",
      "2017-09-26T14:27:43.214241: step 2816, loss 0.000760022, acc 1\n",
      "2017-09-26T14:27:43.552143: step 2817, loss 0.00201422, acc 1\n",
      "2017-09-26T14:27:43.868148: step 2818, loss 0.00138368, acc 1\n",
      "2017-09-26T14:27:44.115958: step 2819, loss 0.00198166, acc 1\n",
      "2017-09-26T14:27:44.365960: step 2820, loss 0.00169437, acc 1\n",
      "2017-09-26T14:27:44.624306: step 2821, loss 0.00215132, acc 1\n",
      "2017-09-26T14:27:44.890441: step 2822, loss 0.00124399, acc 1\n",
      "2017-09-26T14:27:45.141506: step 2823, loss 0.00663695, acc 1\n",
      "2017-09-26T14:27:45.398631: step 2824, loss 0.00374074, acc 1\n",
      "2017-09-26T14:27:45.647507: step 2825, loss 0.0262331, acc 0.984375\n",
      "2017-09-26T14:27:45.904462: step 2826, loss 0.0100963, acc 1\n",
      "2017-09-26T14:27:46.253411: step 2827, loss 0.00707436, acc 1\n",
      "2017-09-26T14:27:46.498957: step 2828, loss 0.00071472, acc 1\n",
      "2017-09-26T14:27:46.762350: step 2829, loss 0.00104221, acc 1\n",
      "2017-09-26T14:27:47.007701: step 2830, loss 0.000838987, acc 1\n",
      "2017-09-26T14:27:47.259383: step 2831, loss 0.00265829, acc 1\n",
      "2017-09-26T14:27:47.504777: step 2832, loss 0.00219119, acc 1\n",
      "2017-09-26T14:27:47.771274: step 2833, loss 0.00499125, acc 1\n",
      "2017-09-26T14:27:48.016893: step 2834, loss 0.0107132, acc 1\n",
      "2017-09-26T14:27:48.269372: step 2835, loss 0.0029388, acc 1\n",
      "2017-09-26T14:27:48.526030: step 2836, loss 0.00141076, acc 1\n",
      "2017-09-26T14:27:48.774193: step 2837, loss 0.00245882, acc 1\n",
      "2017-09-26T14:27:49.033924: step 2838, loss 0.00475681, acc 1\n",
      "2017-09-26T14:27:49.289740: step 2839, loss 0.00101379, acc 1\n",
      "2017-09-26T14:27:49.536725: step 2840, loss 0.000618199, acc 1\n",
      "2017-09-26T14:27:49.789930: step 2841, loss 0.000697266, acc 1\n",
      "2017-09-26T14:27:50.059844: step 2842, loss 0.00112351, acc 1\n",
      "2017-09-26T14:27:50.317874: step 2843, loss 0.00317488, acc 1\n",
      "2017-09-26T14:27:50.566666: step 2844, loss 0.00789624, acc 1\n",
      "2017-09-26T14:27:50.818732: step 2845, loss 0.0090633, acc 1\n",
      "2017-09-26T14:27:51.083497: step 2846, loss 0.00443685, acc 1\n",
      "2017-09-26T14:27:51.342176: step 2847, loss 0.0026367, acc 1\n",
      "2017-09-26T14:27:51.587936: step 2848, loss 0.00122364, acc 1\n",
      "2017-09-26T14:27:51.843061: step 2849, loss 0.000995509, acc 1\n",
      "2017-09-26T14:27:52.091933: step 2850, loss 0.000970302, acc 1\n",
      "2017-09-26T14:27:52.347876: step 2851, loss 0.000727545, acc 1\n",
      "2017-09-26T14:27:52.597648: step 2852, loss 0.00650302, acc 1\n",
      "2017-09-26T14:27:52.846867: step 2853, loss 0.00151472, acc 1\n",
      "2017-09-26T14:27:53.097804: step 2854, loss 0.00536891, acc 1\n",
      "2017-09-26T14:27:53.351739: step 2855, loss 0.00151686, acc 1\n",
      "2017-09-26T14:27:53.573411: step 2856, loss 0.00363893, acc 1\n",
      "2017-09-26T14:27:53.858823: step 2857, loss 0.00379446, acc 1\n",
      "2017-09-26T14:27:54.109288: step 2858, loss 0.00658389, acc 1\n",
      "2017-09-26T14:27:54.398381: step 2859, loss 0.00322144, acc 1\n",
      "2017-09-26T14:27:54.732849: step 2860, loss 0.0190383, acc 0.984375\n",
      "2017-09-26T14:27:54.980940: step 2861, loss 0.00174364, acc 1\n",
      "2017-09-26T14:27:55.269525: step 2862, loss 0.00312108, acc 1\n",
      "2017-09-26T14:27:55.518147: step 2863, loss 0.000784527, acc 1\n",
      "2017-09-26T14:27:55.765824: step 2864, loss 0.00116417, acc 1\n",
      "2017-09-26T14:27:56.016240: step 2865, loss 0.000795662, acc 1\n",
      "2017-09-26T14:27:56.273239: step 2866, loss 0.00359871, acc 1\n",
      "2017-09-26T14:27:56.525281: step 2867, loss 0.00432495, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:27:56.799990: step 2868, loss 0.00204391, acc 1\n",
      "2017-09-26T14:27:57.051512: step 2869, loss 0.000887078, acc 1\n",
      "2017-09-26T14:27:57.296203: step 2870, loss 0.00784731, acc 1\n",
      "2017-09-26T14:27:57.539569: step 2871, loss 0.00104169, acc 1\n",
      "2017-09-26T14:27:57.784733: step 2872, loss 0.00269476, acc 1\n",
      "2017-09-26T14:27:58.026357: step 2873, loss 0.000259627, acc 1\n",
      "2017-09-26T14:27:58.266161: step 2874, loss 0.000664799, acc 1\n",
      "2017-09-26T14:27:58.506941: step 2875, loss 0.0918016, acc 0.984375\n",
      "2017-09-26T14:27:58.754538: step 2876, loss 0.00200118, acc 1\n",
      "2017-09-26T14:27:58.993891: step 2877, loss 0.0032934, acc 1\n",
      "2017-09-26T14:27:59.241930: step 2878, loss 0.0116587, acc 1\n",
      "2017-09-26T14:27:59.487328: step 2879, loss 0.00403487, acc 1\n",
      "2017-09-26T14:27:59.732180: step 2880, loss 0.00275615, acc 1\n",
      "2017-09-26T14:27:59.987235: step 2881, loss 0.00125982, acc 1\n",
      "2017-09-26T14:28:00.235606: step 2882, loss 0.00182926, acc 1\n",
      "2017-09-26T14:28:00.475523: step 2883, loss 0.0021428, acc 1\n",
      "2017-09-26T14:28:00.724660: step 2884, loss 0.000465968, acc 1\n",
      "2017-09-26T14:28:00.964656: step 2885, loss 0.00236733, acc 1\n",
      "2017-09-26T14:28:01.206537: step 2886, loss 0.0013314, acc 1\n",
      "2017-09-26T14:28:01.448046: step 2887, loss 0.000932973, acc 1\n",
      "2017-09-26T14:28:01.687103: step 2888, loss 0.00247849, acc 1\n",
      "2017-09-26T14:28:01.944681: step 2889, loss 0.00092357, acc 1\n",
      "2017-09-26T14:28:02.182893: step 2890, loss 0.001662, acc 1\n",
      "2017-09-26T14:28:02.431122: step 2891, loss 0.00210866, acc 1\n",
      "2017-09-26T14:28:02.674176: step 2892, loss 0.0102943, acc 1\n",
      "2017-09-26T14:28:02.936202: step 2893, loss 0.00162445, acc 1\n",
      "2017-09-26T14:28:03.178510: step 2894, loss 0.000644311, acc 1\n",
      "2017-09-26T14:28:03.422261: step 2895, loss 0.00106387, acc 1\n",
      "2017-09-26T14:28:03.670758: step 2896, loss 0.00127607, acc 1\n",
      "2017-09-26T14:28:03.933729: step 2897, loss 0.00125028, acc 1\n",
      "2017-09-26T14:28:04.143062: step 2898, loss 0.00350853, acc 1\n",
      "2017-09-26T14:28:04.419330: step 2899, loss 0.00073386, acc 1\n",
      "2017-09-26T14:28:04.654822: step 2900, loss 0.00184319, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:28:04.904677: step 2900, loss 0.513449, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-2900\n",
      "\n",
      "2017-09-26T14:28:05.468706: step 2901, loss 0.00151588, acc 1\n",
      "2017-09-26T14:28:05.711600: step 2902, loss 0.00307267, acc 1\n",
      "2017-09-26T14:28:05.963667: step 2903, loss 0.00392835, acc 1\n",
      "2017-09-26T14:28:06.204343: step 2904, loss 0.00352027, acc 1\n",
      "2017-09-26T14:28:06.450282: step 2905, loss 0.000299179, acc 1\n",
      "2017-09-26T14:28:06.689319: step 2906, loss 0.00386684, acc 1\n",
      "2017-09-26T14:28:06.946916: step 2907, loss 0.00204174, acc 1\n",
      "2017-09-26T14:28:07.181137: step 2908, loss 0.0054189, acc 1\n",
      "2017-09-26T14:28:07.428874: step 2909, loss 0.00226914, acc 1\n",
      "2017-09-26T14:28:07.745372: step 2910, loss 0.00317712, acc 1\n",
      "2017-09-26T14:28:08.122275: step 2911, loss 0.00526992, acc 1\n",
      "2017-09-26T14:28:08.366022: step 2912, loss 0.00176461, acc 1\n",
      "2017-09-26T14:28:08.609967: step 2913, loss 0.000320974, acc 1\n",
      "2017-09-26T14:28:08.853910: step 2914, loss 0.000428453, acc 1\n",
      "2017-09-26T14:28:09.129719: step 2915, loss 0.000661987, acc 1\n",
      "2017-09-26T14:28:09.397688: step 2916, loss 0.0040559, acc 1\n",
      "2017-09-26T14:28:09.635023: step 2917, loss 0.000190534, acc 1\n",
      "2017-09-26T14:28:09.884049: step 2918, loss 0.00032917, acc 1\n",
      "2017-09-26T14:28:10.152064: step 2919, loss 0.00153912, acc 1\n",
      "2017-09-26T14:28:10.431090: step 2920, loss 0.000800577, acc 1\n",
      "2017-09-26T14:28:10.692392: step 2921, loss 0.00256595, acc 1\n",
      "2017-09-26T14:28:10.981234: step 2922, loss 0.0763612, acc 0.984375\n",
      "2017-09-26T14:28:11.229733: step 2923, loss 0.00117305, acc 1\n",
      "2017-09-26T14:28:11.504425: step 2924, loss 0.00404001, acc 1\n",
      "2017-09-26T14:28:11.755854: step 2925, loss 0.00156847, acc 1\n",
      "2017-09-26T14:28:11.999874: step 2926, loss 0.0125284, acc 1\n",
      "2017-09-26T14:28:12.256721: step 2927, loss 0.00119602, acc 1\n",
      "2017-09-26T14:28:12.517071: step 2928, loss 0.00096865, acc 1\n",
      "2017-09-26T14:28:12.785998: step 2929, loss 0.00430441, acc 1\n",
      "2017-09-26T14:28:13.043544: step 2930, loss 0.00064689, acc 1\n",
      "2017-09-26T14:28:13.296777: step 2931, loss 0.000559321, acc 1\n",
      "2017-09-26T14:28:13.564209: step 2932, loss 0.00171566, acc 1\n",
      "2017-09-26T14:28:13.827120: step 2933, loss 0.0188308, acc 0.984375\n",
      "2017-09-26T14:28:14.072683: step 2934, loss 0.0228491, acc 0.984375\n",
      "2017-09-26T14:28:14.321403: step 2935, loss 0.0787872, acc 0.984375\n",
      "2017-09-26T14:28:14.569772: step 2936, loss 0.000924659, acc 1\n",
      "2017-09-26T14:28:14.853332: step 2937, loss 0.00161203, acc 1\n",
      "2017-09-26T14:28:15.123781: step 2938, loss 0.00110255, acc 1\n",
      "2017-09-26T14:28:15.427710: step 2939, loss 0.0350616, acc 0.984375\n",
      "2017-09-26T14:28:15.667353: step 2940, loss 0.00272741, acc 1\n",
      "2017-09-26T14:28:15.946303: step 2941, loss 0.00103772, acc 1\n",
      "2017-09-26T14:28:16.230969: step 2942, loss 0.000926418, acc 1\n",
      "2017-09-26T14:28:16.498372: step 2943, loss 0.00122067, acc 1\n",
      "2017-09-26T14:28:16.769105: step 2944, loss 0.0202402, acc 0.984375\n",
      "2017-09-26T14:28:17.053506: step 2945, loss 0.000521063, acc 1\n",
      "2017-09-26T14:28:17.336115: step 2946, loss 0.0057522, acc 1\n",
      "2017-09-26T14:28:17.596338: step 2947, loss 0.00547525, acc 1\n",
      "2017-09-26T14:28:17.866833: step 2948, loss 0.00275053, acc 1\n",
      "2017-09-26T14:28:18.124047: step 2949, loss 0.000366362, acc 1\n",
      "2017-09-26T14:28:18.399804: step 2950, loss 0.00204495, acc 1\n",
      "2017-09-26T14:28:18.660766: step 2951, loss 0.00207108, acc 1\n",
      "2017-09-26T14:28:18.905856: step 2952, loss 0.00958255, acc 1\n",
      "2017-09-26T14:28:19.167223: step 2953, loss 0.00142132, acc 1\n",
      "2017-09-26T14:28:19.409157: step 2954, loss 0.00332588, acc 1\n",
      "2017-09-26T14:28:19.650695: step 2955, loss 0.0547082, acc 0.953125\n",
      "2017-09-26T14:28:19.914525: step 2956, loss 0.0180974, acc 0.984375\n",
      "2017-09-26T14:28:20.156545: step 2957, loss 0.00321162, acc 1\n",
      "2017-09-26T14:28:20.404319: step 2958, loss 0.00115901, acc 1\n",
      "2017-09-26T14:28:20.643727: step 2959, loss 0.00125057, acc 1\n",
      "2017-09-26T14:28:20.894960: step 2960, loss 0.00173891, acc 1\n",
      "2017-09-26T14:28:21.134938: step 2961, loss 0.00240361, acc 1\n",
      "2017-09-26T14:28:21.392319: step 2962, loss 0.000349757, acc 1\n",
      "2017-09-26T14:28:21.640659: step 2963, loss 0.000806014, acc 1\n",
      "2017-09-26T14:28:21.890244: step 2964, loss 0.00042169, acc 1\n",
      "2017-09-26T14:28:22.136594: step 2965, loss 0.000825402, acc 1\n",
      "2017-09-26T14:28:22.384718: step 2966, loss 0.000922203, acc 1\n",
      "2017-09-26T14:28:22.634871: step 2967, loss 0.00115281, acc 1\n",
      "2017-09-26T14:28:22.882306: step 2968, loss 0.00453951, acc 1\n",
      "2017-09-26T14:28:23.127341: step 2969, loss 0.00194668, acc 1\n",
      "2017-09-26T14:28:23.418930: step 2970, loss 0.00181744, acc 1\n",
      "2017-09-26T14:28:23.674860: step 2971, loss 0.000482637, acc 1\n",
      "2017-09-26T14:28:23.939994: step 2972, loss 0.00263378, acc 1\n",
      "2017-09-26T14:28:24.186893: step 2973, loss 0.00435449, acc 1\n",
      "2017-09-26T14:28:24.456431: step 2974, loss 0.000844911, acc 1\n",
      "2017-09-26T14:28:24.705103: step 2975, loss 0.00248065, acc 1\n",
      "2017-09-26T14:28:24.945313: step 2976, loss 0.00152929, acc 1\n",
      "2017-09-26T14:28:25.196106: step 2977, loss 0.0645341, acc 0.984375\n",
      "2017-09-26T14:28:25.449648: step 2978, loss 0.00846605, acc 1\n",
      "2017-09-26T14:28:25.708619: step 2979, loss 0.000668186, acc 1\n",
      "2017-09-26T14:28:25.975839: step 2980, loss 0.000977628, acc 1\n",
      "2017-09-26T14:28:26.239570: step 2981, loss 0.000860435, acc 1\n",
      "2017-09-26T14:28:26.456136: step 2982, loss 0.00179496, acc 1\n",
      "2017-09-26T14:28:26.710569: step 2983, loss 0.00196926, acc 1\n",
      "2017-09-26T14:28:26.955969: step 2984, loss 0.000832692, acc 1\n",
      "2017-09-26T14:28:27.208267: step 2985, loss 0.0051155, acc 1\n",
      "2017-09-26T14:28:27.458971: step 2986, loss 0.00304282, acc 1\n",
      "2017-09-26T14:28:27.722590: step 2987, loss 0.00154538, acc 1\n",
      "2017-09-26T14:28:27.967093: step 2988, loss 0.00176656, acc 1\n",
      "2017-09-26T14:28:28.214413: step 2989, loss 0.00148679, acc 1\n",
      "2017-09-26T14:28:28.466753: step 2990, loss 0.00147481, acc 1\n",
      "2017-09-26T14:28:28.716085: step 2991, loss 0.000996607, acc 1\n",
      "2017-09-26T14:28:28.963656: step 2992, loss 0.00745956, acc 1\n",
      "2017-09-26T14:28:29.212742: step 2993, loss 0.00183465, acc 1\n",
      "2017-09-26T14:28:29.471710: step 2994, loss 0.00102261, acc 1\n",
      "2017-09-26T14:28:29.718700: step 2995, loss 0.00629279, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:28:29.965778: step 2996, loss 0.0348685, acc 0.96875\n",
      "2017-09-26T14:28:30.226428: step 2997, loss 0.00356977, acc 1\n",
      "2017-09-26T14:28:30.469707: step 2998, loss 0.000806344, acc 1\n",
      "2017-09-26T14:28:30.705200: step 2999, loss 0.00640936, acc 1\n",
      "2017-09-26T14:28:30.944644: step 3000, loss 0.000957921, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:28:31.183149: step 3000, loss 0.519538, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3000\n",
      "\n",
      "2017-09-26T14:28:31.701966: step 3001, loss 0.00103899, acc 1\n",
      "2017-09-26T14:28:31.953847: step 3002, loss 0.00209823, acc 1\n",
      "2017-09-26T14:28:32.195271: step 3003, loss 0.00108271, acc 1\n",
      "2017-09-26T14:28:32.454250: step 3004, loss 0.000885358, acc 1\n",
      "2017-09-26T14:28:32.696315: step 3005, loss 0.00125606, acc 1\n",
      "2017-09-26T14:28:32.938298: step 3006, loss 0.00149023, acc 1\n",
      "2017-09-26T14:28:33.175914: step 3007, loss 0.00735956, acc 1\n",
      "2017-09-26T14:28:33.428761: step 3008, loss 0.00565909, acc 1\n",
      "2017-09-26T14:28:33.672403: step 3009, loss 0.00193277, acc 1\n",
      "2017-09-26T14:28:33.910772: step 3010, loss 0.0287177, acc 0.984375\n",
      "2017-09-26T14:28:34.151806: step 3011, loss 0.020574, acc 0.984375\n",
      "2017-09-26T14:28:34.405328: step 3012, loss 0.000278593, acc 1\n",
      "2017-09-26T14:28:34.648951: step 3013, loss 0.00129015, acc 1\n",
      "2017-09-26T14:28:34.888118: step 3014, loss 0.00111316, acc 1\n",
      "2017-09-26T14:28:35.130762: step 3015, loss 0.0175383, acc 1\n",
      "2017-09-26T14:28:35.381297: step 3016, loss 0.000540543, acc 1\n",
      "2017-09-26T14:28:35.633532: step 3017, loss 0.00102979, acc 1\n",
      "2017-09-26T14:28:35.878171: step 3018, loss 0.0848416, acc 0.984375\n",
      "2017-09-26T14:28:36.129297: step 3019, loss 0.00161802, acc 1\n",
      "2017-09-26T14:28:36.371569: step 3020, loss 0.0156394, acc 0.984375\n",
      "2017-09-26T14:28:36.631690: step 3021, loss 0.00299064, acc 1\n",
      "2017-09-26T14:28:36.876862: step 3022, loss 0.000577036, acc 1\n",
      "2017-09-26T14:28:37.116503: step 3023, loss 0.000798621, acc 1\n",
      "2017-09-26T14:28:37.323698: step 3024, loss 0.0388705, acc 0.980769\n",
      "2017-09-26T14:28:37.577273: step 3025, loss 0.0295454, acc 0.984375\n",
      "2017-09-26T14:28:37.821442: step 3026, loss 0.00109374, acc 1\n",
      "2017-09-26T14:28:38.066192: step 3027, loss 0.00449442, acc 1\n",
      "2017-09-26T14:28:38.308131: step 3028, loss 0.00377758, acc 1\n",
      "2017-09-26T14:28:38.545923: step 3029, loss 0.000526975, acc 1\n",
      "2017-09-26T14:28:38.787281: step 3030, loss 0.00398141, acc 1\n",
      "2017-09-26T14:28:39.032205: step 3031, loss 0.000604506, acc 1\n",
      "2017-09-26T14:28:39.278646: step 3032, loss 0.000436365, acc 1\n",
      "2017-09-26T14:28:39.521242: step 3033, loss 0.000937273, acc 1\n",
      "2017-09-26T14:28:39.762461: step 3034, loss 0.00677966, acc 1\n",
      "2017-09-26T14:28:40.017658: step 3035, loss 0.020815, acc 0.984375\n",
      "2017-09-26T14:28:40.260961: step 3036, loss 0.00332022, acc 1\n",
      "2017-09-26T14:28:40.505088: step 3037, loss 0.0870412, acc 0.984375\n",
      "2017-09-26T14:28:40.749693: step 3038, loss 0.000973901, acc 1\n",
      "2017-09-26T14:28:40.994647: step 3039, loss 0.00407953, acc 1\n",
      "2017-09-26T14:28:41.237269: step 3040, loss 0.00380126, acc 1\n",
      "2017-09-26T14:28:41.480328: step 3041, loss 0.00838012, acc 1\n",
      "2017-09-26T14:28:41.721911: step 3042, loss 0.0045584, acc 1\n",
      "2017-09-26T14:28:41.966775: step 3043, loss 0.00107845, acc 1\n",
      "2017-09-26T14:28:42.207134: step 3044, loss 0.00228009, acc 1\n",
      "2017-09-26T14:28:42.448373: step 3045, loss 0.0034172, acc 1\n",
      "2017-09-26T14:28:42.692313: step 3046, loss 0.0031041, acc 1\n",
      "2017-09-26T14:28:42.938966: step 3047, loss 0.00188384, acc 1\n",
      "2017-09-26T14:28:43.177874: step 3048, loss 0.00184475, acc 1\n",
      "2017-09-26T14:28:43.425163: step 3049, loss 0.0388145, acc 0.984375\n",
      "2017-09-26T14:28:43.661860: step 3050, loss 0.000689175, acc 1\n",
      "2017-09-26T14:28:43.906527: step 3051, loss 0.00182276, acc 1\n",
      "2017-09-26T14:28:44.150334: step 3052, loss 0.000701834, acc 1\n",
      "2017-09-26T14:28:44.390640: step 3053, loss 0.000759221, acc 1\n",
      "2017-09-26T14:28:44.651436: step 3054, loss 0.00148335, acc 1\n",
      "2017-09-26T14:28:44.894393: step 3055, loss 0.00183282, acc 1\n",
      "2017-09-26T14:28:45.170893: step 3056, loss 0.000378142, acc 1\n",
      "2017-09-26T14:28:45.425003: step 3057, loss 0.00237701, acc 1\n",
      "2017-09-26T14:28:45.683437: step 3058, loss 0.00135822, acc 1\n",
      "2017-09-26T14:28:45.935335: step 3059, loss 0.00205533, acc 1\n",
      "2017-09-26T14:28:46.250942: step 3060, loss 0.000578527, acc 1\n",
      "2017-09-26T14:28:46.516358: step 3061, loss 0.000947821, acc 1\n",
      "2017-09-26T14:28:46.782215: step 3062, loss 0.000937119, acc 1\n",
      "2017-09-26T14:28:47.041914: step 3063, loss 0.0144599, acc 0.984375\n",
      "2017-09-26T14:28:47.329294: step 3064, loss 0.000708749, acc 1\n",
      "2017-09-26T14:28:47.588066: step 3065, loss 0.000642302, acc 1\n",
      "2017-09-26T14:28:47.823738: step 3066, loss 0.00221183, acc 1\n",
      "2017-09-26T14:28:48.098179: step 3067, loss 0.00460895, acc 1\n",
      "2017-09-26T14:28:48.368973: step 3068, loss 0.00151336, acc 1\n",
      "2017-09-26T14:28:48.625373: step 3069, loss 0.0017158, acc 1\n",
      "2017-09-26T14:28:48.959641: step 3070, loss 0.00046212, acc 1\n",
      "2017-09-26T14:28:49.310114: step 3071, loss 0.00149268, acc 1\n",
      "2017-09-26T14:28:49.645185: step 3072, loss 0.00105441, acc 1\n",
      "2017-09-26T14:28:49.968393: step 3073, loss 0.00197944, acc 1\n",
      "2017-09-26T14:28:50.264643: step 3074, loss 0.000730052, acc 1\n",
      "2017-09-26T14:28:50.594735: step 3075, loss 0.00173962, acc 1\n",
      "2017-09-26T14:28:50.837256: step 3076, loss 0.0011244, acc 1\n",
      "2017-09-26T14:28:51.093074: step 3077, loss 0.00677942, acc 1\n",
      "2017-09-26T14:28:51.343578: step 3078, loss 0.00103276, acc 1\n",
      "2017-09-26T14:28:51.644485: step 3079, loss 0.000751983, acc 1\n",
      "2017-09-26T14:28:51.980610: step 3080, loss 0.0193047, acc 1\n",
      "2017-09-26T14:28:52.294986: step 3081, loss 0.000434846, acc 1\n",
      "2017-09-26T14:28:52.620143: step 3082, loss 0.00575928, acc 1\n",
      "2017-09-26T14:28:52.935534: step 3083, loss 0.000453218, acc 1\n",
      "2017-09-26T14:28:53.228167: step 3084, loss 0.00238934, acc 1\n",
      "2017-09-26T14:28:53.519536: step 3085, loss 0.00427276, acc 1\n",
      "2017-09-26T14:28:53.859494: step 3086, loss 0.000354672, acc 1\n",
      "2017-09-26T14:28:54.189003: step 3087, loss 0.00172041, acc 1\n",
      "2017-09-26T14:28:54.513653: step 3088, loss 0.000455278, acc 1\n",
      "2017-09-26T14:28:54.822269: step 3089, loss 0.00272047, acc 1\n",
      "2017-09-26T14:28:55.134155: step 3090, loss 0.0008652, acc 1\n",
      "2017-09-26T14:28:55.450698: step 3091, loss 0.00416063, acc 1\n",
      "2017-09-26T14:28:55.701783: step 3092, loss 0.00133376, acc 1\n",
      "2017-09-26T14:28:56.001221: step 3093, loss 0.00297751, acc 1\n",
      "2017-09-26T14:28:56.307890: step 3094, loss 0.000406389, acc 1\n",
      "2017-09-26T14:28:56.584520: step 3095, loss 0.00339836, acc 1\n",
      "2017-09-26T14:28:56.846029: step 3096, loss 0.00297957, acc 1\n",
      "2017-09-26T14:28:57.116728: step 3097, loss 0.00219309, acc 1\n",
      "2017-09-26T14:28:57.386962: step 3098, loss 0.00344759, acc 1\n",
      "2017-09-26T14:28:57.659294: step 3099, loss 0.00506017, acc 1\n",
      "2017-09-26T14:28:57.941408: step 3100, loss 0.00144138, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:28:58.187284: step 3100, loss 0.515762, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3100\n",
      "\n",
      "2017-09-26T14:28:58.765604: step 3101, loss 0.000734106, acc 1\n",
      "2017-09-26T14:28:59.029663: step 3102, loss 0.000932533, acc 1\n",
      "2017-09-26T14:28:59.299120: step 3103, loss 0.0427368, acc 0.984375\n",
      "2017-09-26T14:28:59.553610: step 3104, loss 0.0085669, acc 1\n",
      "2017-09-26T14:28:59.864295: step 3105, loss 0.000631427, acc 1\n",
      "2017-09-26T14:29:00.109961: step 3106, loss 0.000900742, acc 1\n",
      "2017-09-26T14:29:00.402612: step 3107, loss 0.0255526, acc 0.984375\n",
      "2017-09-26T14:29:00.626863: step 3108, loss 0.0177779, acc 1\n",
      "2017-09-26T14:29:00.887786: step 3109, loss 0.0334742, acc 0.984375\n",
      "2017-09-26T14:29:01.159475: step 3110, loss 0.000943419, acc 1\n",
      "2017-09-26T14:29:01.414403: step 3111, loss 0.00107634, acc 1\n",
      "2017-09-26T14:29:01.666665: step 3112, loss 0.000739578, acc 1\n",
      "2017-09-26T14:29:01.925138: step 3113, loss 0.00154804, acc 1\n",
      "2017-09-26T14:29:02.211064: step 3114, loss 0.0067581, acc 1\n",
      "2017-09-26T14:29:02.551323: step 3115, loss 0.000785975, acc 1\n",
      "2017-09-26T14:29:02.890328: step 3116, loss 0.0011688, acc 1\n",
      "2017-09-26T14:29:03.205262: step 3117, loss 0.00251048, acc 1\n",
      "2017-09-26T14:29:03.495973: step 3118, loss 0.00371855, acc 1\n",
      "2017-09-26T14:29:03.744491: step 3119, loss 0.000617043, acc 1\n",
      "2017-09-26T14:29:03.999772: step 3120, loss 0.00066219, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:29:04.255979: step 3121, loss 0.000580273, acc 1\n",
      "2017-09-26T14:29:04.510849: step 3122, loss 0.000390711, acc 1\n",
      "2017-09-26T14:29:04.763990: step 3123, loss 0.000591406, acc 1\n",
      "2017-09-26T14:29:05.007077: step 3124, loss 0.000415341, acc 1\n",
      "2017-09-26T14:29:05.252401: step 3125, loss 0.00581989, acc 1\n",
      "2017-09-26T14:29:05.499591: step 3126, loss 0.00164644, acc 1\n",
      "2017-09-26T14:29:05.739921: step 3127, loss 0.00351032, acc 1\n",
      "2017-09-26T14:29:05.984603: step 3128, loss 0.00284252, acc 1\n",
      "2017-09-26T14:29:06.226415: step 3129, loss 0.000559353, acc 1\n",
      "2017-09-26T14:29:06.472285: step 3130, loss 0.0185928, acc 0.984375\n",
      "2017-09-26T14:29:06.717804: step 3131, loss 0.000852516, acc 1\n",
      "2017-09-26T14:29:06.968758: step 3132, loss 0.00121847, acc 1\n",
      "2017-09-26T14:29:07.204660: step 3133, loss 0.00119096, acc 1\n",
      "2017-09-26T14:29:07.452990: step 3134, loss 0.000618979, acc 1\n",
      "2017-09-26T14:29:07.689096: step 3135, loss 0.012391, acc 1\n",
      "2017-09-26T14:29:07.929218: step 3136, loss 0.011396, acc 1\n",
      "2017-09-26T14:29:08.171020: step 3137, loss 0.000294556, acc 1\n",
      "2017-09-26T14:29:08.462286: step 3138, loss 0.0071836, acc 1\n",
      "2017-09-26T14:29:08.730759: step 3139, loss 0.00118209, acc 1\n",
      "2017-09-26T14:29:08.997829: step 3140, loss 0.00229136, acc 1\n",
      "2017-09-26T14:29:09.262113: step 3141, loss 0.00772273, acc 1\n",
      "2017-09-26T14:29:09.560946: step 3142, loss 0.000141534, acc 1\n",
      "2017-09-26T14:29:09.905842: step 3143, loss 0.0139536, acc 1\n",
      "2017-09-26T14:29:10.231525: step 3144, loss 0.00219543, acc 1\n",
      "2017-09-26T14:29:10.574291: step 3145, loss 0.00593774, acc 1\n",
      "2017-09-26T14:29:10.839581: step 3146, loss 0.00229772, acc 1\n",
      "2017-09-26T14:29:11.081242: step 3147, loss 0.000760073, acc 1\n",
      "2017-09-26T14:29:11.362783: step 3148, loss 0.0021264, acc 1\n",
      "2017-09-26T14:29:11.603507: step 3149, loss 0.00101956, acc 1\n",
      "2017-09-26T14:29:11.816510: step 3150, loss 0.00044363, acc 1\n",
      "2017-09-26T14:29:12.067623: step 3151, loss 0.00100157, acc 1\n",
      "2017-09-26T14:29:12.407964: step 3152, loss 0.00146095, acc 1\n",
      "2017-09-26T14:29:12.751384: step 3153, loss 0.000782759, acc 1\n",
      "2017-09-26T14:29:13.095653: step 3154, loss 0.00139049, acc 1\n",
      "2017-09-26T14:29:13.427692: step 3155, loss 0.000661515, acc 1\n",
      "2017-09-26T14:29:13.773623: step 3156, loss 0.0279066, acc 0.984375\n",
      "2017-09-26T14:29:14.120505: step 3157, loss 0.000822972, acc 1\n",
      "2017-09-26T14:29:14.480958: step 3158, loss 0.00176174, acc 1\n",
      "2017-09-26T14:29:14.785875: step 3159, loss 0.000446558, acc 1\n",
      "2017-09-26T14:29:15.062042: step 3160, loss 0.00129219, acc 1\n",
      "2017-09-26T14:29:15.415179: step 3161, loss 0.00163355, acc 1\n",
      "2017-09-26T14:29:15.725282: step 3162, loss 0.000432373, acc 1\n",
      "2017-09-26T14:29:16.035116: step 3163, loss 0.0130546, acc 0.984375\n",
      "2017-09-26T14:29:16.371510: step 3164, loss 0.0020578, acc 1\n",
      "2017-09-26T14:29:16.674010: step 3165, loss 0.00134795, acc 1\n",
      "2017-09-26T14:29:17.016436: step 3166, loss 0.00098004, acc 1\n",
      "2017-09-26T14:29:17.261095: step 3167, loss 0.0057374, acc 1\n",
      "2017-09-26T14:29:17.585352: step 3168, loss 0.00763462, acc 1\n",
      "2017-09-26T14:29:17.913124: step 3169, loss 0.0079138, acc 1\n",
      "2017-09-26T14:29:18.245779: step 3170, loss 0.0435641, acc 0.984375\n",
      "2017-09-26T14:29:18.578151: step 3171, loss 0.000467823, acc 1\n",
      "2017-09-26T14:29:18.852352: step 3172, loss 0.000666908, acc 1\n",
      "2017-09-26T14:29:19.176927: step 3173, loss 0.000679773, acc 1\n",
      "2017-09-26T14:29:19.494218: step 3174, loss 0.00243744, acc 1\n",
      "2017-09-26T14:29:19.826057: step 3175, loss 0.000501148, acc 1\n",
      "2017-09-26T14:29:20.159691: step 3176, loss 0.000525865, acc 1\n",
      "2017-09-26T14:29:20.490492: step 3177, loss 0.000516572, acc 1\n",
      "2017-09-26T14:29:20.794048: step 3178, loss 0.000218169, acc 1\n",
      "2017-09-26T14:29:21.125436: step 3179, loss 0.00184571, acc 1\n",
      "2017-09-26T14:29:21.453002: step 3180, loss 0.00262754, acc 1\n",
      "2017-09-26T14:29:21.778743: step 3181, loss 0.00319329, acc 1\n",
      "2017-09-26T14:29:22.117633: step 3182, loss 0.00147057, acc 1\n",
      "2017-09-26T14:29:22.402107: step 3183, loss 0.000609826, acc 1\n",
      "2017-09-26T14:29:22.654969: step 3184, loss 0.00286522, acc 1\n",
      "2017-09-26T14:29:22.899389: step 3185, loss 0.0104176, acc 1\n",
      "2017-09-26T14:29:23.145400: step 3186, loss 0.000620817, acc 1\n",
      "2017-09-26T14:29:23.393382: step 3187, loss 0.00193493, acc 1\n",
      "2017-09-26T14:29:23.650136: step 3188, loss 0.00206607, acc 1\n",
      "2017-09-26T14:29:23.895131: step 3189, loss 0.000558537, acc 1\n",
      "2017-09-26T14:29:24.136271: step 3190, loss 0.000445445, acc 1\n",
      "2017-09-26T14:29:24.381456: step 3191, loss 0.00246875, acc 1\n",
      "2017-09-26T14:29:24.595364: step 3192, loss 0.000625848, acc 1\n",
      "2017-09-26T14:29:24.850120: step 3193, loss 0.000853452, acc 1\n",
      "2017-09-26T14:29:25.093870: step 3194, loss 0.0494375, acc 0.984375\n",
      "2017-09-26T14:29:25.349505: step 3195, loss 0.000879974, acc 1\n",
      "2017-09-26T14:29:25.603602: step 3196, loss 0.0028485, acc 1\n",
      "2017-09-26T14:29:25.851209: step 3197, loss 0.00283467, acc 1\n",
      "2017-09-26T14:29:26.098799: step 3198, loss 0.00424269, acc 1\n",
      "2017-09-26T14:29:26.349519: step 3199, loss 0.000618553, acc 1\n",
      "2017-09-26T14:29:26.602818: step 3200, loss 0.00264205, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:29:26.847918: step 3200, loss 0.545925, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3200\n",
      "\n",
      "2017-09-26T14:29:27.583839: step 3201, loss 0.00333646, acc 1\n",
      "2017-09-26T14:29:27.835311: step 3202, loss 0.000443724, acc 1\n",
      "2017-09-26T14:29:28.089625: step 3203, loss 0.000652384, acc 1\n",
      "2017-09-26T14:29:28.335537: step 3204, loss 0.00246917, acc 1\n",
      "2017-09-26T14:29:28.580069: step 3205, loss 0.000649613, acc 1\n",
      "2017-09-26T14:29:28.839445: step 3206, loss 0.00115368, acc 1\n",
      "2017-09-26T14:29:29.095596: step 3207, loss 0.000716638, acc 1\n",
      "2017-09-26T14:29:29.342395: step 3208, loss 0.00645753, acc 1\n",
      "2017-09-26T14:29:29.594022: step 3209, loss 0.00306081, acc 1\n",
      "2017-09-26T14:29:29.837286: step 3210, loss 0.00140384, acc 1\n",
      "2017-09-26T14:29:30.106002: step 3211, loss 0.00258376, acc 1\n",
      "2017-09-26T14:29:30.418040: step 3212, loss 0.0107337, acc 1\n",
      "2017-09-26T14:29:30.753059: step 3213, loss 0.000928167, acc 1\n",
      "2017-09-26T14:29:31.093812: step 3214, loss 0.00508689, acc 1\n",
      "2017-09-26T14:29:31.430651: step 3215, loss 0.00110072, acc 1\n",
      "2017-09-26T14:29:31.781016: step 3216, loss 0.000483177, acc 1\n",
      "2017-09-26T14:29:32.092936: step 3217, loss 0.00176542, acc 1\n",
      "2017-09-26T14:29:32.393399: step 3218, loss 0.00262355, acc 1\n",
      "2017-09-26T14:29:32.682020: step 3219, loss 0.00489788, acc 1\n",
      "2017-09-26T14:29:32.925652: step 3220, loss 0.00162727, acc 1\n",
      "2017-09-26T14:29:33.182916: step 3221, loss 0.00785814, acc 1\n",
      "2017-09-26T14:29:33.500124: step 3222, loss 0.0894499, acc 0.984375\n",
      "2017-09-26T14:29:33.844768: step 3223, loss 0.00122159, acc 1\n",
      "2017-09-26T14:29:34.116793: step 3224, loss 0.000803093, acc 1\n",
      "2017-09-26T14:29:34.392042: step 3225, loss 0.00136704, acc 1\n",
      "2017-09-26T14:29:34.649770: step 3226, loss 0.00486671, acc 1\n",
      "2017-09-26T14:29:34.903931: step 3227, loss 0.0011849, acc 1\n",
      "2017-09-26T14:29:35.205465: step 3228, loss 0.00314572, acc 1\n",
      "2017-09-26T14:29:35.538541: step 3229, loss 0.0324414, acc 0.984375\n",
      "2017-09-26T14:29:35.860454: step 3230, loss 0.00215411, acc 1\n",
      "2017-09-26T14:29:36.188373: step 3231, loss 0.000362915, acc 1\n",
      "2017-09-26T14:29:36.552427: step 3232, loss 0.00116626, acc 1\n",
      "2017-09-26T14:29:36.881388: step 3233, loss 0.000405009, acc 1\n",
      "2017-09-26T14:29:37.115971: step 3234, loss 0.00139047, acc 1\n",
      "2017-09-26T14:29:37.441967: step 3235, loss 0.00103584, acc 1\n",
      "2017-09-26T14:29:37.777686: step 3236, loss 0.000515026, acc 1\n",
      "2017-09-26T14:29:38.077480: step 3237, loss 0.000302293, acc 1\n",
      "2017-09-26T14:29:38.330436: step 3238, loss 0.000632727, acc 1\n",
      "2017-09-26T14:29:38.643004: step 3239, loss 0.00372206, acc 1\n",
      "2017-09-26T14:29:38.947034: step 3240, loss 0.00035574, acc 1\n",
      "2017-09-26T14:29:39.208064: step 3241, loss 0.00122883, acc 1\n",
      "2017-09-26T14:29:39.460338: step 3242, loss 0.000583903, acc 1\n",
      "2017-09-26T14:29:39.717377: step 3243, loss 0.00453215, acc 1\n",
      "2017-09-26T14:29:40.046009: step 3244, loss 0.0144028, acc 0.984375\n",
      "2017-09-26T14:29:40.383357: step 3245, loss 0.000856451, acc 1\n",
      "2017-09-26T14:29:40.672939: step 3246, loss 0.0017891, acc 1\n",
      "2017-09-26T14:29:41.026189: step 3247, loss 0.000728296, acc 1\n",
      "2017-09-26T14:29:41.358392: step 3248, loss 0.000464692, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:29:41.609944: step 3249, loss 0.000277124, acc 1\n",
      "2017-09-26T14:29:41.897239: step 3250, loss 0.000953221, acc 1\n",
      "2017-09-26T14:29:42.257205: step 3251, loss 0.00200248, acc 1\n",
      "2017-09-26T14:29:42.612195: step 3252, loss 0.000724692, acc 1\n",
      "2017-09-26T14:29:42.918851: step 3253, loss 0.00216891, acc 1\n",
      "2017-09-26T14:29:43.266908: step 3254, loss 0.00262654, acc 1\n",
      "2017-09-26T14:29:43.598210: step 3255, loss 0.00228859, acc 1\n",
      "2017-09-26T14:29:43.958642: step 3256, loss 0.000616134, acc 1\n",
      "2017-09-26T14:29:44.260039: step 3257, loss 0.00149541, acc 1\n",
      "2017-09-26T14:29:44.565996: step 3258, loss 0.0308016, acc 0.984375\n",
      "2017-09-26T14:29:44.896987: step 3259, loss 0.000746012, acc 1\n",
      "2017-09-26T14:29:45.223838: step 3260, loss 0.00242182, acc 1\n",
      "2017-09-26T14:29:45.573970: step 3261, loss 0.00121373, acc 1\n",
      "2017-09-26T14:29:45.900167: step 3262, loss 0.00327335, acc 1\n",
      "2017-09-26T14:29:46.147917: step 3263, loss 0.00155411, acc 1\n",
      "2017-09-26T14:29:46.490174: step 3264, loss 0.0127665, acc 1\n",
      "2017-09-26T14:29:46.837019: step 3265, loss 0.000918673, acc 1\n",
      "2017-09-26T14:29:47.178399: step 3266, loss 0.000814041, acc 1\n",
      "2017-09-26T14:29:47.524963: step 3267, loss 0.00171872, acc 1\n",
      "2017-09-26T14:29:47.857212: step 3268, loss 0.00204777, acc 1\n",
      "2017-09-26T14:29:48.158862: step 3269, loss 0.00063167, acc 1\n",
      "2017-09-26T14:29:48.513983: step 3270, loss 0.0010556, acc 1\n",
      "2017-09-26T14:29:48.867064: step 3271, loss 0.00068935, acc 1\n",
      "2017-09-26T14:29:49.164373: step 3272, loss 0.000796888, acc 1\n",
      "2017-09-26T14:29:49.481191: step 3273, loss 0.00273319, acc 1\n",
      "2017-09-26T14:29:49.846276: step 3274, loss 0.000797119, acc 1\n",
      "2017-09-26T14:29:50.151089: step 3275, loss 0.00321201, acc 1\n",
      "2017-09-26T14:29:50.447307: step 3276, loss 0.00527503, acc 1\n",
      "2017-09-26T14:29:50.746877: step 3277, loss 0.000993865, acc 1\n",
      "2017-09-26T14:29:51.104525: step 3278, loss 0.000846363, acc 1\n",
      "2017-09-26T14:29:51.439664: step 3279, loss 0.00263148, acc 1\n",
      "2017-09-26T14:29:51.680988: step 3280, loss 0.00102757, acc 1\n",
      "2017-09-26T14:29:51.932785: step 3281, loss 0.00144506, acc 1\n",
      "2017-09-26T14:29:52.179716: step 3282, loss 0.00213161, acc 1\n",
      "2017-09-26T14:29:52.442104: step 3283, loss 0.000173299, acc 1\n",
      "2017-09-26T14:29:52.704501: step 3284, loss 0.00444859, acc 1\n",
      "2017-09-26T14:29:52.952939: step 3285, loss 0.000837403, acc 1\n",
      "2017-09-26T14:29:53.212261: step 3286, loss 0.00377309, acc 1\n",
      "2017-09-26T14:29:53.485629: step 3287, loss 0.00246897, acc 1\n",
      "2017-09-26T14:29:53.753340: step 3288, loss 0.00943904, acc 1\n",
      "2017-09-26T14:29:54.023754: step 3289, loss 0.00424493, acc 1\n",
      "2017-09-26T14:29:54.299604: step 3290, loss 0.00100909, acc 1\n",
      "2017-09-26T14:29:54.578898: step 3291, loss 0.000530009, acc 1\n",
      "2017-09-26T14:29:54.897134: step 3292, loss 0.00050407, acc 1\n",
      "2017-09-26T14:29:55.165097: step 3293, loss 0.000828755, acc 1\n",
      "2017-09-26T14:29:55.448362: step 3294, loss 0.0395847, acc 0.984375\n",
      "2017-09-26T14:29:55.743321: step 3295, loss 0.00119272, acc 1\n",
      "2017-09-26T14:29:56.016926: step 3296, loss 0.000422421, acc 1\n",
      "2017-09-26T14:29:56.306757: step 3297, loss 0.00286235, acc 1\n",
      "2017-09-26T14:29:56.605602: step 3298, loss 0.000603332, acc 1\n",
      "2017-09-26T14:29:56.906742: step 3299, loss 0.00284436, acc 1\n",
      "2017-09-26T14:29:57.154976: step 3300, loss 0.00102061, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:29:57.414398: step 3300, loss 0.551321, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3300\n",
      "\n",
      "2017-09-26T14:29:57.936995: step 3301, loss 0.00750362, acc 1\n",
      "2017-09-26T14:29:58.194314: step 3302, loss 0.00084902, acc 1\n",
      "2017-09-26T14:29:58.476790: step 3303, loss 0.000943406, acc 1\n",
      "2017-09-26T14:29:58.762991: step 3304, loss 0.00293854, acc 1\n",
      "2017-09-26T14:29:59.034855: step 3305, loss 0.00106529, acc 1\n",
      "2017-09-26T14:29:59.312691: step 3306, loss 0.000673153, acc 1\n",
      "2017-09-26T14:29:59.616081: step 3307, loss 0.0052586, acc 1\n",
      "2017-09-26T14:29:59.905370: step 3308, loss 0.00477687, acc 1\n",
      "2017-09-26T14:30:00.233282: step 3309, loss 0.000609519, acc 1\n",
      "2017-09-26T14:30:00.512754: step 3310, loss 0.000625934, acc 1\n",
      "2017-09-26T14:30:00.812036: step 3311, loss 0.00119351, acc 1\n",
      "2017-09-26T14:30:01.108266: step 3312, loss 0.000692605, acc 1\n",
      "2017-09-26T14:30:01.406679: step 3313, loss 0.0741381, acc 0.984375\n",
      "2017-09-26T14:30:01.705249: step 3314, loss 0.000576395, acc 1\n",
      "2017-09-26T14:30:01.997743: step 3315, loss 0.000703056, acc 1\n",
      "2017-09-26T14:30:02.297780: step 3316, loss 0.00200517, acc 1\n",
      "2017-09-26T14:30:02.599106: step 3317, loss 0.00202935, acc 1\n",
      "2017-09-26T14:30:02.845626: step 3318, loss 0.00476191, acc 1\n",
      "2017-09-26T14:30:03.140565: step 3319, loss 0.00118801, acc 1\n",
      "2017-09-26T14:30:03.408621: step 3320, loss 0.00257459, acc 1\n",
      "2017-09-26T14:30:03.734028: step 3321, loss 0.000519199, acc 1\n",
      "2017-09-26T14:30:04.020459: step 3322, loss 0.00156148, acc 1\n",
      "2017-09-26T14:30:04.313349: step 3323, loss 0.0106124, acc 1\n",
      "2017-09-26T14:30:04.602350: step 3324, loss 0.000327413, acc 1\n",
      "2017-09-26T14:30:04.916734: step 3325, loss 0.00101608, acc 1\n",
      "2017-09-26T14:30:05.210114: step 3326, loss 0.00423929, acc 1\n",
      "2017-09-26T14:30:05.483003: step 3327, loss 0.0031721, acc 1\n",
      "2017-09-26T14:30:05.798621: step 3328, loss 0.00262211, acc 1\n",
      "2017-09-26T14:30:06.091287: step 3329, loss 0.0353044, acc 0.984375\n",
      "2017-09-26T14:30:06.375534: step 3330, loss 0.00356089, acc 1\n",
      "2017-09-26T14:30:06.654248: step 3331, loss 0.00518635, acc 1\n",
      "2017-09-26T14:30:06.932012: step 3332, loss 0.00164622, acc 1\n",
      "2017-09-26T14:30:07.201091: step 3333, loss 0.00152104, acc 1\n",
      "2017-09-26T14:30:07.467390: step 3334, loss 0.00157028, acc 1\n",
      "2017-09-26T14:30:07.752098: step 3335, loss 0.0295535, acc 0.984375\n",
      "2017-09-26T14:30:08.029207: step 3336, loss 0.00119304, acc 1\n",
      "2017-09-26T14:30:08.288353: step 3337, loss 0.00120875, acc 1\n",
      "2017-09-26T14:30:08.571113: step 3338, loss 0.000345068, acc 1\n",
      "2017-09-26T14:30:08.838966: step 3339, loss 0.000640194, acc 1\n",
      "2017-09-26T14:30:09.122327: step 3340, loss 0.00134524, acc 1\n",
      "2017-09-26T14:30:09.363109: step 3341, loss 0.00159558, acc 1\n",
      "2017-09-26T14:30:09.610363: step 3342, loss 0.000567733, acc 1\n",
      "2017-09-26T14:30:09.853293: step 3343, loss 0.00241076, acc 1\n",
      "2017-09-26T14:30:10.095508: step 3344, loss 0.085816, acc 0.984375\n",
      "2017-09-26T14:30:10.344846: step 3345, loss 0.00294955, acc 1\n",
      "2017-09-26T14:30:10.586610: step 3346, loss 0.00101961, acc 1\n",
      "2017-09-26T14:30:10.830129: step 3347, loss 0.00123736, acc 1\n",
      "2017-09-26T14:30:11.096370: step 3348, loss 0.00358279, acc 1\n",
      "2017-09-26T14:30:11.374425: step 3349, loss 0.00276791, acc 1\n",
      "2017-09-26T14:30:11.656065: step 3350, loss 0.00217998, acc 1\n",
      "2017-09-26T14:30:11.935861: step 3351, loss 0.000379147, acc 1\n",
      "2017-09-26T14:30:12.201062: step 3352, loss 0.00138831, acc 1\n",
      "2017-09-26T14:30:12.542225: step 3353, loss 0.000940792, acc 1\n",
      "2017-09-26T14:30:12.819495: step 3354, loss 0.00378528, acc 1\n",
      "2017-09-26T14:30:13.084460: step 3355, loss 0.000701207, acc 1\n",
      "2017-09-26T14:30:13.357399: step 3356, loss 0.00253483, acc 1\n",
      "2017-09-26T14:30:13.662689: step 3357, loss 0.000292034, acc 1\n",
      "2017-09-26T14:30:13.994595: step 3358, loss 0.00203111, acc 1\n",
      "2017-09-26T14:30:14.282086: step 3359, loss 0.000938087, acc 1\n",
      "2017-09-26T14:30:14.513798: step 3360, loss 0.116443, acc 0.980769\n",
      "2017-09-26T14:30:14.799842: step 3361, loss 0.000278651, acc 1\n",
      "2017-09-26T14:30:15.115776: step 3362, loss 0.00152781, acc 1\n",
      "2017-09-26T14:30:15.409416: step 3363, loss 0.000395064, acc 1\n",
      "2017-09-26T14:30:15.722563: step 3364, loss 0.0022586, acc 1\n",
      "2017-09-26T14:30:15.996327: step 3365, loss 0.00268881, acc 1\n",
      "2017-09-26T14:30:16.306163: step 3366, loss 0.000620601, acc 1\n",
      "2017-09-26T14:30:16.617229: step 3367, loss 0.00483002, acc 1\n",
      "2017-09-26T14:30:16.919835: step 3368, loss 0.0163699, acc 0.984375\n",
      "2017-09-26T14:30:17.200687: step 3369, loss 0.00393811, acc 1\n",
      "2017-09-26T14:30:17.474858: step 3370, loss 0.016569, acc 0.984375\n",
      "2017-09-26T14:30:17.793124: step 3371, loss 0.00119305, acc 1\n",
      "2017-09-26T14:30:18.104686: step 3372, loss 0.00288338, acc 1\n",
      "2017-09-26T14:30:18.387148: step 3373, loss 0.000435284, acc 1\n",
      "2017-09-26T14:30:18.705204: step 3374, loss 0.000728209, acc 1\n",
      "2017-09-26T14:30:19.003879: step 3375, loss 0.00567523, acc 1\n",
      "2017-09-26T14:30:19.291378: step 3376, loss 0.00250831, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:30:19.597245: step 3377, loss 0.0140776, acc 1\n",
      "2017-09-26T14:30:19.900527: step 3378, loss 0.000342254, acc 1\n",
      "2017-09-26T14:30:20.172176: step 3379, loss 0.000808605, acc 1\n",
      "2017-09-26T14:30:20.447310: step 3380, loss 0.000318818, acc 1\n",
      "2017-09-26T14:30:20.739427: step 3381, loss 0.00272838, acc 1\n",
      "2017-09-26T14:30:21.018694: step 3382, loss 0.000354417, acc 1\n",
      "2017-09-26T14:30:21.294031: step 3383, loss 0.00456458, acc 1\n",
      "2017-09-26T14:30:21.567344: step 3384, loss 0.00138106, acc 1\n",
      "2017-09-26T14:30:21.845297: step 3385, loss 0.00148137, acc 1\n",
      "2017-09-26T14:30:22.082467: step 3386, loss 0.000764096, acc 1\n",
      "2017-09-26T14:30:22.341458: step 3387, loss 0.00231749, acc 1\n",
      "2017-09-26T14:30:22.588244: step 3388, loss 0.00140458, acc 1\n",
      "2017-09-26T14:30:22.863063: step 3389, loss 0.000852656, acc 1\n",
      "2017-09-26T14:30:23.144126: step 3390, loss 0.00156182, acc 1\n",
      "2017-09-26T14:30:23.406008: step 3391, loss 0.000414078, acc 1\n",
      "2017-09-26T14:30:23.665802: step 3392, loss 0.000603074, acc 1\n",
      "2017-09-26T14:30:23.924682: step 3393, loss 0.00337554, acc 1\n",
      "2017-09-26T14:30:24.184123: step 3394, loss 0.0029593, acc 1\n",
      "2017-09-26T14:30:24.476187: step 3395, loss 0.000648782, acc 1\n",
      "2017-09-26T14:30:24.745926: step 3396, loss 0.00262985, acc 1\n",
      "2017-09-26T14:30:25.006486: step 3397, loss 0.0019482, acc 1\n",
      "2017-09-26T14:30:25.303584: step 3398, loss 0.00807205, acc 1\n",
      "2017-09-26T14:30:25.592081: step 3399, loss 0.00181662, acc 1\n",
      "2017-09-26T14:30:25.871251: step 3400, loss 0.00561946, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:30:26.159775: step 3400, loss 0.554261, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3400\n",
      "\n",
      "2017-09-26T14:30:26.711813: step 3401, loss 0.00188208, acc 1\n",
      "2017-09-26T14:30:26.963600: step 3402, loss 0.000865345, acc 1\n",
      "2017-09-26T14:30:27.229092: step 3403, loss 0.0115442, acc 1\n",
      "2017-09-26T14:30:27.516301: step 3404, loss 0.00232263, acc 1\n",
      "2017-09-26T14:30:27.822902: step 3405, loss 0.00094902, acc 1\n",
      "2017-09-26T14:30:28.103919: step 3406, loss 0.000768956, acc 1\n",
      "2017-09-26T14:30:28.416266: step 3407, loss 0.0235758, acc 0.984375\n",
      "2017-09-26T14:30:28.696878: step 3408, loss 0.00156161, acc 1\n",
      "2017-09-26T14:30:28.967406: step 3409, loss 0.00190145, acc 1\n",
      "2017-09-26T14:30:29.262367: step 3410, loss 0.000875968, acc 1\n",
      "2017-09-26T14:30:29.542725: step 3411, loss 0.00875145, acc 1\n",
      "2017-09-26T14:30:29.818174: step 3412, loss 0.00652554, acc 1\n",
      "2017-09-26T14:30:30.092963: step 3413, loss 0.000831111, acc 1\n",
      "2017-09-26T14:30:30.395068: step 3414, loss 0.00157996, acc 1\n",
      "2017-09-26T14:30:30.664713: step 3415, loss 0.000456124, acc 1\n",
      "2017-09-26T14:30:30.937307: step 3416, loss 0.00119938, acc 1\n",
      "2017-09-26T14:30:31.210037: step 3417, loss 0.00349031, acc 1\n",
      "2017-09-26T14:30:31.487079: step 3418, loss 0.000781642, acc 1\n",
      "2017-09-26T14:30:31.757239: step 3419, loss 0.000838498, acc 1\n",
      "2017-09-26T14:30:32.050673: step 3420, loss 0.00160204, acc 1\n",
      "2017-09-26T14:30:32.319930: step 3421, loss 0.00203802, acc 1\n",
      "2017-09-26T14:30:32.599896: step 3422, loss 0.000449083, acc 1\n",
      "2017-09-26T14:30:32.907923: step 3423, loss 0.00126079, acc 1\n",
      "2017-09-26T14:30:33.190054: step 3424, loss 0.000704437, acc 1\n",
      "2017-09-26T14:30:33.483729: step 3425, loss 0.000440544, acc 1\n",
      "2017-09-26T14:30:33.786472: step 3426, loss 0.003634, acc 1\n",
      "2017-09-26T14:30:34.045448: step 3427, loss 0.000548421, acc 1\n",
      "2017-09-26T14:30:34.327852: step 3428, loss 0.000363745, acc 1\n",
      "2017-09-26T14:30:34.595101: step 3429, loss 0.000702243, acc 1\n",
      "2017-09-26T14:30:34.873373: step 3430, loss 0.000767827, acc 1\n",
      "2017-09-26T14:30:35.182255: step 3431, loss 0.000462552, acc 1\n",
      "2017-09-26T14:30:35.450610: step 3432, loss 0.000699477, acc 1\n",
      "2017-09-26T14:30:35.736600: step 3433, loss 0.00029373, acc 1\n",
      "2017-09-26T14:30:36.039944: step 3434, loss 0.00161084, acc 1\n",
      "2017-09-26T14:30:36.331017: step 3435, loss 0.000317426, acc 1\n",
      "2017-09-26T14:30:36.600372: step 3436, loss 0.00320214, acc 1\n",
      "2017-09-26T14:30:36.889299: step 3437, loss 0.000855936, acc 1\n",
      "2017-09-26T14:30:37.147223: step 3438, loss 0.00197357, acc 1\n",
      "2017-09-26T14:30:37.405670: step 3439, loss 0.024712, acc 0.984375\n",
      "2017-09-26T14:30:37.651974: step 3440, loss 0.0010245, acc 1\n",
      "2017-09-26T14:30:37.895300: step 3441, loss 0.00112862, acc 1\n",
      "2017-09-26T14:30:38.137605: step 3442, loss 0.0224667, acc 0.984375\n",
      "2017-09-26T14:30:38.380647: step 3443, loss 0.000300098, acc 1\n",
      "2017-09-26T14:30:38.599532: step 3444, loss 0.00213754, acc 1\n",
      "2017-09-26T14:30:38.849094: step 3445, loss 0.000444663, acc 1\n",
      "2017-09-26T14:30:39.100143: step 3446, loss 0.000235566, acc 1\n",
      "2017-09-26T14:30:39.418329: step 3447, loss 0.000894785, acc 1\n",
      "2017-09-26T14:30:39.745181: step 3448, loss 0.000563671, acc 1\n",
      "2017-09-26T14:30:40.073995: step 3449, loss 0.00056966, acc 1\n",
      "2017-09-26T14:30:40.391575: step 3450, loss 0.00120844, acc 1\n",
      "2017-09-26T14:30:40.679913: step 3451, loss 0.0231971, acc 0.984375\n",
      "2017-09-26T14:30:40.936210: step 3452, loss 0.000585568, acc 1\n",
      "2017-09-26T14:30:41.182592: step 3453, loss 0.000410536, acc 1\n",
      "2017-09-26T14:30:41.438786: step 3454, loss 0.00190672, acc 1\n",
      "2017-09-26T14:30:41.702889: step 3455, loss 0.00133432, acc 1\n",
      "2017-09-26T14:30:42.050488: step 3456, loss 0.036144, acc 0.984375\n",
      "2017-09-26T14:30:42.418074: step 3457, loss 0.000654599, acc 1\n",
      "2017-09-26T14:30:42.738342: step 3458, loss 0.000719993, acc 1\n",
      "2017-09-26T14:30:43.079688: step 3459, loss 0.000269923, acc 1\n",
      "2017-09-26T14:30:43.418300: step 3460, loss 0.000723102, acc 1\n",
      "2017-09-26T14:30:43.746988: step 3461, loss 0.0040042, acc 1\n",
      "2017-09-26T14:30:44.024556: step 3462, loss 0.000400853, acc 1\n",
      "2017-09-26T14:30:44.278920: step 3463, loss 0.000770265, acc 1\n",
      "2017-09-26T14:30:44.536621: step 3464, loss 0.0692551, acc 0.984375\n",
      "2017-09-26T14:30:44.779397: step 3465, loss 0.00324776, acc 1\n",
      "2017-09-26T14:30:45.028517: step 3466, loss 0.00172371, acc 1\n",
      "2017-09-26T14:30:45.282085: step 3467, loss 0.000769052, acc 1\n",
      "2017-09-26T14:30:45.544517: step 3468, loss 0.00777004, acc 1\n",
      "2017-09-26T14:30:45.883758: step 3469, loss 0.00386489, acc 1\n",
      "2017-09-26T14:30:46.221385: step 3470, loss 0.00493157, acc 1\n",
      "2017-09-26T14:30:46.558380: step 3471, loss 0.00108154, acc 1\n",
      "2017-09-26T14:30:46.899142: step 3472, loss 0.000376539, acc 1\n",
      "2017-09-26T14:30:47.245170: step 3473, loss 0.0140484, acc 0.984375\n",
      "2017-09-26T14:30:47.574907: step 3474, loss 0.0010342, acc 1\n",
      "2017-09-26T14:30:47.912830: step 3475, loss 0.00177498, acc 1\n",
      "2017-09-26T14:30:48.265484: step 3476, loss 0.00153265, acc 1\n",
      "2017-09-26T14:30:48.578773: step 3477, loss 0.000857998, acc 1\n",
      "2017-09-26T14:30:48.888472: step 3478, loss 0.00291363, acc 1\n",
      "2017-09-26T14:30:49.171527: step 3479, loss 0.00464523, acc 1\n",
      "2017-09-26T14:30:49.450359: step 3480, loss 0.000772415, acc 1\n",
      "2017-09-26T14:30:49.711143: step 3481, loss 0.0470243, acc 0.984375\n",
      "2017-09-26T14:30:49.970255: step 3482, loss 0.000398066, acc 1\n",
      "2017-09-26T14:30:50.251474: step 3483, loss 0.00049282, acc 1\n",
      "2017-09-26T14:30:50.514200: step 3484, loss 0.00125333, acc 1\n",
      "2017-09-26T14:30:50.768101: step 3485, loss 0.00114806, acc 1\n",
      "2017-09-26T14:30:50.997276: step 3486, loss 0.000157261, acc 1\n",
      "2017-09-26T14:30:51.254390: step 3487, loss 0.00800787, acc 1\n",
      "2017-09-26T14:30:51.510762: step 3488, loss 0.000364225, acc 1\n",
      "2017-09-26T14:30:51.782587: step 3489, loss 0.00387881, acc 1\n",
      "2017-09-26T14:30:52.038661: step 3490, loss 0.000593987, acc 1\n",
      "2017-09-26T14:30:52.289902: step 3491, loss 0.00199758, acc 1\n",
      "2017-09-26T14:30:52.554711: step 3492, loss 0.000668097, acc 1\n",
      "2017-09-26T14:30:52.818327: step 3493, loss 0.0209283, acc 0.984375\n",
      "2017-09-26T14:30:53.070342: step 3494, loss 0.000902125, acc 1\n",
      "2017-09-26T14:30:53.324751: step 3495, loss 0.00291719, acc 1\n",
      "2017-09-26T14:30:53.603852: step 3496, loss 0.00380137, acc 1\n",
      "2017-09-26T14:30:53.875796: step 3497, loss 0.00132641, acc 1\n",
      "2017-09-26T14:30:54.144954: step 3498, loss 0.00217675, acc 1\n",
      "2017-09-26T14:30:54.413841: step 3499, loss 0.000589983, acc 1\n",
      "2017-09-26T14:30:54.688469: step 3500, loss 0.000657877, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:30:54.966787: step 3500, loss 0.555098, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3500\n",
      "\n",
      "2017-09-26T14:30:55.599275: step 3501, loss 0.000355143, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:30:55.909692: step 3502, loss 0.000948689, acc 1\n",
      "2017-09-26T14:30:56.208023: step 3503, loss 0.000348908, acc 1\n",
      "2017-09-26T14:30:56.519318: step 3504, loss 0.0699214, acc 0.96875\n",
      "2017-09-26T14:30:56.795532: step 3505, loss 0.00170538, acc 1\n",
      "2017-09-26T14:30:57.080457: step 3506, loss 0.00334749, acc 1\n",
      "2017-09-26T14:30:57.355685: step 3507, loss 0.000516202, acc 1\n",
      "2017-09-26T14:30:57.646088: step 3508, loss 0.000400031, acc 1\n",
      "2017-09-26T14:30:57.938504: step 3509, loss 0.0174401, acc 0.984375\n",
      "2017-09-26T14:30:58.222963: step 3510, loss 0.00197455, acc 1\n",
      "2017-09-26T14:30:58.509413: step 3511, loss 0.000629524, acc 1\n",
      "2017-09-26T14:30:58.813972: step 3512, loss 0.00160274, acc 1\n",
      "2017-09-26T14:30:59.082085: step 3513, loss 0.000666735, acc 1\n",
      "2017-09-26T14:30:59.370611: step 3514, loss 0.00730851, acc 1\n",
      "2017-09-26T14:30:59.672241: step 3515, loss 0.00434234, acc 1\n",
      "2017-09-26T14:30:59.975191: step 3516, loss 0.000194692, acc 1\n",
      "2017-09-26T14:31:00.247622: step 3517, loss 0.000775915, acc 1\n",
      "2017-09-26T14:31:00.546019: step 3518, loss 0.00429508, acc 1\n",
      "2017-09-26T14:31:00.839777: step 3519, loss 0.0212937, acc 0.984375\n",
      "2017-09-26T14:31:01.138280: step 3520, loss 0.00534126, acc 1\n",
      "2017-09-26T14:31:01.426755: step 3521, loss 0.00859075, acc 1\n",
      "2017-09-26T14:31:01.722911: step 3522, loss 0.00568981, acc 1\n",
      "2017-09-26T14:31:01.990047: step 3523, loss 0.00112916, acc 1\n",
      "2017-09-26T14:31:02.267301: step 3524, loss 0.00273282, acc 1\n",
      "2017-09-26T14:31:02.550665: step 3525, loss 0.000283961, acc 1\n",
      "2017-09-26T14:31:02.825160: step 3526, loss 0.000359988, acc 1\n",
      "2017-09-26T14:31:03.098690: step 3527, loss 0.00341127, acc 1\n",
      "2017-09-26T14:31:03.328250: step 3528, loss 0.000690805, acc 1\n",
      "2017-09-26T14:31:03.614994: step 3529, loss 0.00107734, acc 1\n",
      "2017-09-26T14:31:03.876865: step 3530, loss 0.00366952, acc 1\n",
      "2017-09-26T14:31:04.138416: step 3531, loss 0.0005054, acc 1\n",
      "2017-09-26T14:31:04.431956: step 3532, loss 0.00211212, acc 1\n",
      "2017-09-26T14:31:04.692439: step 3533, loss 0.000797702, acc 1\n",
      "2017-09-26T14:31:04.980503: step 3534, loss 0.00110665, acc 1\n",
      "2017-09-26T14:31:05.246285: step 3535, loss 0.000594375, acc 1\n",
      "2017-09-26T14:31:05.510092: step 3536, loss 0.000423602, acc 1\n",
      "2017-09-26T14:31:05.790017: step 3537, loss 0.00338783, acc 1\n",
      "2017-09-26T14:31:06.073717: step 3538, loss 0.000546233, acc 1\n",
      "2017-09-26T14:31:06.346904: step 3539, loss 0.000393114, acc 1\n",
      "2017-09-26T14:31:06.602650: step 3540, loss 0.00116035, acc 1\n",
      "2017-09-26T14:31:06.885864: step 3541, loss 0.000667331, acc 1\n",
      "2017-09-26T14:31:07.168653: step 3542, loss 0.00352146, acc 1\n",
      "2017-09-26T14:31:07.424104: step 3543, loss 0.000505386, acc 1\n",
      "2017-09-26T14:31:07.684567: step 3544, loss 0.00115685, acc 1\n",
      "2017-09-26T14:31:07.969973: step 3545, loss 0.00106409, acc 1\n",
      "2017-09-26T14:31:08.241411: step 3546, loss 0.001219, acc 1\n",
      "2017-09-26T14:31:08.507818: step 3547, loss 0.00154123, acc 1\n",
      "2017-09-26T14:31:08.775732: step 3548, loss 0.000361816, acc 1\n",
      "2017-09-26T14:31:09.055277: step 3549, loss 0.00259873, acc 1\n",
      "2017-09-26T14:31:09.324311: step 3550, loss 0.000859816, acc 1\n",
      "2017-09-26T14:31:09.590406: step 3551, loss 0.000734901, acc 1\n",
      "2017-09-26T14:31:09.854428: step 3552, loss 0.000255986, acc 1\n",
      "2017-09-26T14:31:10.109989: step 3553, loss 0.00251061, acc 1\n",
      "2017-09-26T14:31:10.442488: step 3554, loss 0.00866638, acc 1\n",
      "2017-09-26T14:31:10.826801: step 3555, loss 0.000577416, acc 1\n",
      "2017-09-26T14:31:11.132349: step 3556, loss 0.0163864, acc 0.984375\n",
      "2017-09-26T14:31:11.482937: step 3557, loss 0.000682185, acc 1\n",
      "2017-09-26T14:31:11.865057: step 3558, loss 0.00295879, acc 1\n",
      "2017-09-26T14:31:12.217193: step 3559, loss 0.00111664, acc 1\n",
      "2017-09-26T14:31:12.554635: step 3560, loss 0.00060384, acc 1\n",
      "2017-09-26T14:31:12.889018: step 3561, loss 0.000690669, acc 1\n",
      "2017-09-26T14:31:13.205061: step 3562, loss 0.00100337, acc 1\n",
      "2017-09-26T14:31:13.535803: step 3563, loss 0.000155818, acc 1\n",
      "2017-09-26T14:31:13.823294: step 3564, loss 0.00147193, acc 1\n",
      "2017-09-26T14:31:14.098983: step 3565, loss 0.00139347, acc 1\n",
      "2017-09-26T14:31:14.430901: step 3566, loss 0.0112962, acc 1\n",
      "2017-09-26T14:31:14.812215: step 3567, loss 0.0471114, acc 0.984375\n",
      "2017-09-26T14:31:15.169711: step 3568, loss 0.000786993, acc 1\n",
      "2017-09-26T14:31:15.509454: step 3569, loss 0.000462052, acc 1\n",
      "2017-09-26T14:31:15.817275: step 3570, loss 0.00318833, acc 1\n",
      "2017-09-26T14:31:16.125628: step 3571, loss 0.000107566, acc 1\n",
      "2017-09-26T14:31:16.445081: step 3572, loss 0.000512062, acc 1\n",
      "2017-09-26T14:31:16.819470: step 3573, loss 0.000302581, acc 1\n",
      "2017-09-26T14:31:17.215509: step 3574, loss 0.000849318, acc 1\n",
      "2017-09-26T14:31:17.571876: step 3575, loss 0.000930872, acc 1\n",
      "2017-09-26T14:31:17.924098: step 3576, loss 0.000433558, acc 1\n",
      "2017-09-26T14:31:18.214359: step 3577, loss 0.00135192, acc 1\n",
      "2017-09-26T14:31:18.593947: step 3578, loss 0.00032237, acc 1\n",
      "2017-09-26T14:31:18.986893: step 3579, loss 0.000509844, acc 1\n",
      "2017-09-26T14:31:19.364995: step 3580, loss 0.0019706, acc 1\n",
      "2017-09-26T14:31:19.708070: step 3581, loss 0.00948209, acc 1\n",
      "2017-09-26T14:31:19.996153: step 3582, loss 0.0331024, acc 0.984375\n",
      "2017-09-26T14:31:20.260001: step 3583, loss 0.000774519, acc 1\n",
      "2017-09-26T14:31:20.534993: step 3584, loss 0.00111845, acc 1\n",
      "2017-09-26T14:31:20.828591: step 3585, loss 0.00501748, acc 1\n",
      "2017-09-26T14:31:21.121655: step 3586, loss 0.000299854, acc 1\n",
      "2017-09-26T14:31:21.412841: step 3587, loss 0.00109585, acc 1\n",
      "2017-09-26T14:31:21.693103: step 3588, loss 0.00476323, acc 1\n",
      "2017-09-26T14:31:22.004349: step 3589, loss 0.00923824, acc 1\n",
      "2017-09-26T14:31:22.282518: step 3590, loss 0.00264475, acc 1\n",
      "2017-09-26T14:31:22.569223: step 3591, loss 0.00324722, acc 1\n",
      "2017-09-26T14:31:22.847999: step 3592, loss 0.000284317, acc 1\n",
      "2017-09-26T14:31:23.126214: step 3593, loss 0.0292509, acc 0.984375\n",
      "2017-09-26T14:31:23.474751: step 3594, loss 0.000991761, acc 1\n",
      "2017-09-26T14:31:23.847627: step 3595, loss 0.000413755, acc 1\n",
      "2017-09-26T14:31:24.230829: step 3596, loss 0.00491766, acc 1\n",
      "2017-09-26T14:31:24.591483: step 3597, loss 0.00469522, acc 1\n",
      "2017-09-26T14:31:24.889436: step 3598, loss 0.030906, acc 0.984375\n",
      "2017-09-26T14:31:25.268534: step 3599, loss 0.000943013, acc 1\n",
      "2017-09-26T14:31:25.672075: step 3600, loss 0.00347175, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:31:26.035757: step 3600, loss 0.535343, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3600\n",
      "\n",
      "2017-09-26T14:31:26.729716: step 3601, loss 0.00134541, acc 1\n",
      "2017-09-26T14:31:27.102632: step 3602, loss 0.00102015, acc 1\n",
      "2017-09-26T14:31:27.477656: step 3603, loss 0.000731967, acc 1\n",
      "2017-09-26T14:31:27.758255: step 3604, loss 0.00080958, acc 1\n",
      "2017-09-26T14:31:28.027335: step 3605, loss 0.000569373, acc 1\n",
      "2017-09-26T14:31:28.377446: step 3606, loss 0.000386112, acc 1\n",
      "2017-09-26T14:31:28.691759: step 3607, loss 0.00205124, acc 1\n",
      "2017-09-26T14:31:28.988773: step 3608, loss 0.000784426, acc 1\n",
      "2017-09-26T14:31:29.281008: step 3609, loss 0.00284002, acc 1\n",
      "2017-09-26T14:31:29.579037: step 3610, loss 0.000387322, acc 1\n",
      "2017-09-26T14:31:29.833302: step 3611, loss 0.02282, acc 0.984375\n",
      "2017-09-26T14:31:30.058818: step 3612, loss 0.00148615, acc 1\n",
      "2017-09-26T14:31:30.324677: step 3613, loss 0.000535951, acc 1\n",
      "2017-09-26T14:31:30.584462: step 3614, loss 0.00271553, acc 1\n",
      "2017-09-26T14:31:30.875625: step 3615, loss 0.000842689, acc 1\n",
      "2017-09-26T14:31:31.155479: step 3616, loss 0.00097736, acc 1\n",
      "2017-09-26T14:31:31.418976: step 3617, loss 0.00199817, acc 1\n",
      "2017-09-26T14:31:31.674150: step 3618, loss 0.0028085, acc 1\n",
      "2017-09-26T14:31:31.933201: step 3619, loss 0.00156793, acc 1\n",
      "2017-09-26T14:31:32.196512: step 3620, loss 0.00103544, acc 1\n",
      "2017-09-26T14:31:32.499984: step 3621, loss 0.00440414, acc 1\n",
      "2017-09-26T14:31:32.822825: step 3622, loss 0.000841815, acc 1\n",
      "2017-09-26T14:31:33.166762: step 3623, loss 0.00172923, acc 1\n",
      "2017-09-26T14:31:33.446044: step 3624, loss 0.000486471, acc 1\n",
      "2017-09-26T14:31:33.720912: step 3625, loss 0.00849408, acc 1\n",
      "2017-09-26T14:31:34.017661: step 3626, loss 0.00154167, acc 1\n",
      "2017-09-26T14:31:34.336171: step 3627, loss 0.001115, acc 1\n",
      "2017-09-26T14:31:34.648535: step 3628, loss 0.0012267, acc 1\n",
      "2017-09-26T14:31:34.943036: step 3629, loss 0.00101696, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:31:35.287298: step 3630, loss 0.0281866, acc 0.984375\n",
      "2017-09-26T14:31:35.600598: step 3631, loss 0.000698113, acc 1\n",
      "2017-09-26T14:31:35.857818: step 3632, loss 0.00105053, acc 1\n",
      "2017-09-26T14:31:36.136644: step 3633, loss 0.0010017, acc 1\n",
      "2017-09-26T14:31:36.410910: step 3634, loss 0.0013829, acc 1\n",
      "2017-09-26T14:31:36.718383: step 3635, loss 0.00173495, acc 1\n",
      "2017-09-26T14:31:36.998537: step 3636, loss 0.00195861, acc 1\n",
      "2017-09-26T14:31:37.245292: step 3637, loss 0.00123668, acc 1\n",
      "2017-09-26T14:31:37.496071: step 3638, loss 0.000368228, acc 1\n",
      "2017-09-26T14:31:37.755295: step 3639, loss 0.0105468, acc 1\n",
      "2017-09-26T14:31:38.100006: step 3640, loss 0.00171231, acc 1\n",
      "2017-09-26T14:31:38.438318: step 3641, loss 0.000457501, acc 1\n",
      "2017-09-26T14:31:38.750330: step 3642, loss 0.00636133, acc 1\n",
      "2017-09-26T14:31:39.085075: step 3643, loss 0.00363255, acc 1\n",
      "2017-09-26T14:31:39.345457: step 3644, loss 0.00148961, acc 1\n",
      "2017-09-26T14:31:39.679890: step 3645, loss 0.00173787, acc 1\n",
      "2017-09-26T14:31:39.961703: step 3646, loss 0.000361835, acc 1\n",
      "2017-09-26T14:31:40.235194: step 3647, loss 0.00200665, acc 1\n",
      "2017-09-26T14:31:40.520729: step 3648, loss 0.00041623, acc 1\n",
      "2017-09-26T14:31:40.768955: step 3649, loss 0.0010245, acc 1\n",
      "2017-09-26T14:31:41.071197: step 3650, loss 0.000436911, acc 1\n",
      "2017-09-26T14:31:41.427639: step 3651, loss 0.0197987, acc 0.984375\n",
      "2017-09-26T14:31:41.745175: step 3652, loss 0.000125715, acc 1\n",
      "2017-09-26T14:31:42.087793: step 3653, loss 0.000737592, acc 1\n",
      "2017-09-26T14:31:42.374022: step 3654, loss 0.0348008, acc 0.980769\n",
      "2017-09-26T14:31:42.707288: step 3655, loss 0.00539422, acc 1\n",
      "2017-09-26T14:31:43.031669: step 3656, loss 0.00175956, acc 1\n",
      "2017-09-26T14:31:43.370017: step 3657, loss 0.000621433, acc 1\n",
      "2017-09-26T14:31:43.619089: step 3658, loss 0.00140789, acc 1\n",
      "2017-09-26T14:31:43.861909: step 3659, loss 0.00114585, acc 1\n",
      "2017-09-26T14:31:44.188295: step 3660, loss 0.000813068, acc 1\n",
      "2017-09-26T14:31:44.502873: step 3661, loss 0.000735681, acc 1\n",
      "2017-09-26T14:31:44.839938: step 3662, loss 0.000299936, acc 1\n",
      "2017-09-26T14:31:45.125979: step 3663, loss 0.000683164, acc 1\n",
      "2017-09-26T14:31:45.483947: step 3664, loss 0.00190615, acc 1\n",
      "2017-09-26T14:31:45.746460: step 3665, loss 0.000538629, acc 1\n",
      "2017-09-26T14:31:46.016172: step 3666, loss 0.00126063, acc 1\n",
      "2017-09-26T14:31:46.305867: step 3667, loss 0.00228742, acc 1\n",
      "2017-09-26T14:31:46.572626: step 3668, loss 0.00101223, acc 1\n",
      "2017-09-26T14:31:46.820040: step 3669, loss 0.000371131, acc 1\n",
      "2017-09-26T14:31:47.081882: step 3670, loss 0.000265491, acc 1\n",
      "2017-09-26T14:31:47.324358: step 3671, loss 0.000521835, acc 1\n",
      "2017-09-26T14:31:47.572574: step 3672, loss 0.00179883, acc 1\n",
      "2017-09-26T14:31:47.819660: step 3673, loss 0.00037515, acc 1\n",
      "2017-09-26T14:31:48.088263: step 3674, loss 0.00527569, acc 1\n",
      "2017-09-26T14:31:48.372805: step 3675, loss 0.00370079, acc 1\n",
      "2017-09-26T14:31:48.615286: step 3676, loss 0.000397207, acc 1\n",
      "2017-09-26T14:31:48.868372: step 3677, loss 0.000488345, acc 1\n",
      "2017-09-26T14:31:49.135306: step 3678, loss 0.00107992, acc 1\n",
      "2017-09-26T14:31:49.382684: step 3679, loss 0.000943432, acc 1\n",
      "2017-09-26T14:31:49.653786: step 3680, loss 0.000471328, acc 1\n",
      "2017-09-26T14:31:49.900829: step 3681, loss 0.000995747, acc 1\n",
      "2017-09-26T14:31:50.157067: step 3682, loss 0.0139274, acc 0.984375\n",
      "2017-09-26T14:31:50.402105: step 3683, loss 0.00189849, acc 1\n",
      "2017-09-26T14:31:50.659709: step 3684, loss 0.00118962, acc 1\n",
      "2017-09-26T14:31:50.915112: step 3685, loss 0.0285501, acc 0.984375\n",
      "2017-09-26T14:31:51.162701: step 3686, loss 0.00320425, acc 1\n",
      "2017-09-26T14:31:51.417994: step 3687, loss 0.0035311, acc 1\n",
      "2017-09-26T14:31:51.674715: step 3688, loss 0.00065303, acc 1\n",
      "2017-09-26T14:31:51.923140: step 3689, loss 0.00220882, acc 1\n",
      "2017-09-26T14:31:52.173036: step 3690, loss 0.00401993, acc 1\n",
      "2017-09-26T14:31:52.423627: step 3691, loss 0.000329969, acc 1\n",
      "2017-09-26T14:31:52.682278: step 3692, loss 0.000583075, acc 1\n",
      "2017-09-26T14:31:52.950732: step 3693, loss 0.017977, acc 0.984375\n",
      "2017-09-26T14:31:53.205723: step 3694, loss 0.00253887, acc 1\n",
      "2017-09-26T14:31:53.456269: step 3695, loss 0.000342308, acc 1\n",
      "2017-09-26T14:31:53.689519: step 3696, loss 0.000946921, acc 1\n",
      "2017-09-26T14:31:53.951642: step 3697, loss 0.00199139, acc 1\n",
      "2017-09-26T14:31:54.203140: step 3698, loss 0.00143003, acc 1\n",
      "2017-09-26T14:31:54.485319: step 3699, loss 0.000356011, acc 1\n",
      "2017-09-26T14:31:54.763337: step 3700, loss 0.00127691, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:31:55.029323: step 3700, loss 0.566571, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3700\n",
      "\n",
      "2017-09-26T14:31:55.575607: step 3701, loss 7.11606e-05, acc 1\n",
      "2017-09-26T14:31:55.824002: step 3702, loss 0.000152348, acc 1\n",
      "2017-09-26T14:31:56.081532: step 3703, loss 0.0017084, acc 1\n",
      "2017-09-26T14:31:56.330184: step 3704, loss 0.00274351, acc 1\n",
      "2017-09-26T14:31:56.588068: step 3705, loss 0.000469325, acc 1\n",
      "2017-09-26T14:31:56.871194: step 3706, loss 0.000582291, acc 1\n",
      "2017-09-26T14:31:57.138940: step 3707, loss 0.000616969, acc 1\n",
      "2017-09-26T14:31:57.393126: step 3708, loss 0.0507222, acc 0.984375\n",
      "2017-09-26T14:31:57.649323: step 3709, loss 0.00353254, acc 1\n",
      "2017-09-26T14:31:57.906666: step 3710, loss 0.000877773, acc 1\n",
      "2017-09-26T14:31:58.167033: step 3711, loss 0.000498061, acc 1\n",
      "2017-09-26T14:31:58.416172: step 3712, loss 0.000420343, acc 1\n",
      "2017-09-26T14:31:58.672083: step 3713, loss 0.000995946, acc 1\n",
      "2017-09-26T14:31:58.927661: step 3714, loss 0.00308302, acc 1\n",
      "2017-09-26T14:31:59.182441: step 3715, loss 0.00290022, acc 1\n",
      "2017-09-26T14:31:59.442591: step 3716, loss 0.000442039, acc 1\n",
      "2017-09-26T14:31:59.697236: step 3717, loss 0.00286372, acc 1\n",
      "2017-09-26T14:31:59.951805: step 3718, loss 0.000195616, acc 1\n",
      "2017-09-26T14:32:00.210515: step 3719, loss 0.000333658, acc 1\n",
      "2017-09-26T14:32:00.463249: step 3720, loss 0.00167779, acc 1\n",
      "2017-09-26T14:32:00.717887: step 3721, loss 0.00895689, acc 1\n",
      "2017-09-26T14:32:00.993970: step 3722, loss 0.0063783, acc 1\n",
      "2017-09-26T14:32:01.247164: step 3723, loss 0.00395444, acc 1\n",
      "2017-09-26T14:32:01.502990: step 3724, loss 0.0497835, acc 0.96875\n",
      "2017-09-26T14:32:01.762414: step 3725, loss 0.00241572, acc 1\n",
      "2017-09-26T14:32:02.009689: step 3726, loss 0.000693743, acc 1\n",
      "2017-09-26T14:32:02.265792: step 3727, loss 0.00154928, acc 1\n",
      "2017-09-26T14:32:02.533752: step 3728, loss 0.000296062, acc 1\n",
      "2017-09-26T14:32:02.786105: step 3729, loss 0.000509147, acc 1\n",
      "2017-09-26T14:32:03.045394: step 3730, loss 0.00106665, acc 1\n",
      "2017-09-26T14:32:03.311367: step 3731, loss 0.000469371, acc 1\n",
      "2017-09-26T14:32:03.565020: step 3732, loss 0.000982791, acc 1\n",
      "2017-09-26T14:32:03.815655: step 3733, loss 0.000405893, acc 1\n",
      "2017-09-26T14:32:04.070713: step 3734, loss 0.000872604, acc 1\n",
      "2017-09-26T14:32:04.325780: step 3735, loss 0.000557959, acc 1\n",
      "2017-09-26T14:32:04.588401: step 3736, loss 0.000826874, acc 1\n",
      "2017-09-26T14:32:04.852240: step 3737, loss 0.000245459, acc 1\n",
      "2017-09-26T14:32:05.078022: step 3738, loss 0.000327375, acc 1\n",
      "2017-09-26T14:32:05.380471: step 3739, loss 0.0083048, acc 1\n",
      "2017-09-26T14:32:05.631398: step 3740, loss 0.00151311, acc 1\n",
      "2017-09-26T14:32:05.911570: step 3741, loss 0.00265604, acc 1\n",
      "2017-09-26T14:32:06.190534: step 3742, loss 0.000314908, acc 1\n",
      "2017-09-26T14:32:06.454851: step 3743, loss 0.000497549, acc 1\n",
      "2017-09-26T14:32:06.711355: step 3744, loss 0.060234, acc 0.984375\n",
      "2017-09-26T14:32:06.968059: step 3745, loss 0.000194209, acc 1\n",
      "2017-09-26T14:32:07.230697: step 3746, loss 0.000506766, acc 1\n",
      "2017-09-26T14:32:07.492113: step 3747, loss 0.00239905, acc 1\n",
      "2017-09-26T14:32:07.750341: step 3748, loss 0.00172745, acc 1\n",
      "2017-09-26T14:32:08.009148: step 3749, loss 0.00119405, acc 1\n",
      "2017-09-26T14:32:08.263649: step 3750, loss 0.00157921, acc 1\n",
      "2017-09-26T14:32:08.524395: step 3751, loss 0.0061128, acc 1\n",
      "2017-09-26T14:32:08.791513: step 3752, loss 0.000804817, acc 1\n",
      "2017-09-26T14:32:09.051077: step 3753, loss 0.00655269, acc 1\n",
      "2017-09-26T14:32:09.305987: step 3754, loss 0.00164364, acc 1\n",
      "2017-09-26T14:32:09.587155: step 3755, loss 0.00121197, acc 1\n",
      "2017-09-26T14:32:09.841271: step 3756, loss 0.00170616, acc 1\n",
      "2017-09-26T14:32:10.104796: step 3757, loss 0.00027343, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:32:10.369995: step 3758, loss 0.000609787, acc 1\n",
      "2017-09-26T14:32:10.627960: step 3759, loss 0.000864404, acc 1\n",
      "2017-09-26T14:32:10.872927: step 3760, loss 0.0027159, acc 1\n",
      "2017-09-26T14:32:11.123827: step 3761, loss 0.000151331, acc 1\n",
      "2017-09-26T14:32:11.379110: step 3762, loss 0.00747999, acc 1\n",
      "2017-09-26T14:32:11.621128: step 3763, loss 0.000710608, acc 1\n",
      "2017-09-26T14:32:11.866709: step 3764, loss 0.00102315, acc 1\n",
      "2017-09-26T14:32:12.117723: step 3765, loss 0.000444235, acc 1\n",
      "2017-09-26T14:32:12.363631: step 3766, loss 0.00249417, acc 1\n",
      "2017-09-26T14:32:12.610869: step 3767, loss 0.000593198, acc 1\n",
      "2017-09-26T14:32:12.852231: step 3768, loss 0.00078578, acc 1\n",
      "2017-09-26T14:32:13.102973: step 3769, loss 0.00322874, acc 1\n",
      "2017-09-26T14:32:13.352834: step 3770, loss 0.000582575, acc 1\n",
      "2017-09-26T14:32:13.603800: step 3771, loss 0.000473457, acc 1\n",
      "2017-09-26T14:32:13.884950: step 3772, loss 0.00175788, acc 1\n",
      "2017-09-26T14:32:14.134125: step 3773, loss 0.000692132, acc 1\n",
      "2017-09-26T14:32:14.393995: step 3774, loss 0.000450999, acc 1\n",
      "2017-09-26T14:32:14.631880: step 3775, loss 0.000613884, acc 1\n",
      "2017-09-26T14:32:14.884776: step 3776, loss 0.000338552, acc 1\n",
      "2017-09-26T14:32:15.140438: step 3777, loss 0.000167973, acc 1\n",
      "2017-09-26T14:32:15.395708: step 3778, loss 0.000737073, acc 1\n",
      "2017-09-26T14:32:15.642536: step 3779, loss 0.000307259, acc 1\n",
      "2017-09-26T14:32:15.861537: step 3780, loss 0.000458328, acc 1\n",
      "2017-09-26T14:32:16.132389: step 3781, loss 0.00534264, acc 1\n",
      "2017-09-26T14:32:16.401797: step 3782, loss 0.000393608, acc 1\n",
      "2017-09-26T14:32:16.654372: step 3783, loss 0.00111426, acc 1\n",
      "2017-09-26T14:32:16.901470: step 3784, loss 0.000371644, acc 1\n",
      "2017-09-26T14:32:17.163016: step 3785, loss 0.000403024, acc 1\n",
      "2017-09-26T14:32:17.420594: step 3786, loss 0.000803413, acc 1\n",
      "2017-09-26T14:32:17.667143: step 3787, loss 0.0125796, acc 1\n",
      "2017-09-26T14:32:17.940984: step 3788, loss 0.000217092, acc 1\n",
      "2017-09-26T14:32:18.211356: step 3789, loss 0.00232328, acc 1\n",
      "2017-09-26T14:32:18.490662: step 3790, loss 0.000578763, acc 1\n",
      "2017-09-26T14:32:18.738986: step 3791, loss 0.000429498, acc 1\n",
      "2017-09-26T14:32:19.006989: step 3792, loss 0.000306725, acc 1\n",
      "2017-09-26T14:32:19.317838: step 3793, loss 0.000876722, acc 1\n",
      "2017-09-26T14:32:19.591085: step 3794, loss 0.0348623, acc 0.984375\n",
      "2017-09-26T14:32:19.879349: step 3795, loss 0.000608646, acc 1\n",
      "2017-09-26T14:32:20.171623: step 3796, loss 0.00214816, acc 1\n",
      "2017-09-26T14:32:20.465510: step 3797, loss 0.000819726, acc 1\n",
      "2017-09-26T14:32:20.769854: step 3798, loss 0.00104747, acc 1\n",
      "2017-09-26T14:32:21.048366: step 3799, loss 0.000414359, acc 1\n",
      "2017-09-26T14:32:21.352018: step 3800, loss 0.000399433, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:32:21.620053: step 3800, loss 0.531184, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3800\n",
      "\n",
      "2017-09-26T14:32:22.176789: step 3801, loss 0.000128095, acc 1\n",
      "2017-09-26T14:32:22.457845: step 3802, loss 0.000741488, acc 1\n",
      "2017-09-26T14:32:22.752953: step 3803, loss 0.000254178, acc 1\n",
      "2017-09-26T14:32:23.028170: step 3804, loss 0.00209419, acc 1\n",
      "2017-09-26T14:32:23.321534: step 3805, loss 0.00141278, acc 1\n",
      "2017-09-26T14:32:23.587676: step 3806, loss 0.000574598, acc 1\n",
      "2017-09-26T14:32:23.869684: step 3807, loss 0.0657142, acc 0.984375\n",
      "2017-09-26T14:32:24.145243: step 3808, loss 0.000832061, acc 1\n",
      "2017-09-26T14:32:24.415055: step 3809, loss 0.000535844, acc 1\n",
      "2017-09-26T14:32:24.673395: step 3810, loss 0.000625673, acc 1\n",
      "2017-09-26T14:32:24.973916: step 3811, loss 0.00155223, acc 1\n",
      "2017-09-26T14:32:25.243969: step 3812, loss 0.00043358, acc 1\n",
      "2017-09-26T14:32:25.504676: step 3813, loss 0.000457312, acc 1\n",
      "2017-09-26T14:32:25.776080: step 3814, loss 0.00548878, acc 1\n",
      "2017-09-26T14:32:26.042166: step 3815, loss 0.000374833, acc 1\n",
      "2017-09-26T14:32:26.335545: step 3816, loss 0.00123229, acc 1\n",
      "2017-09-26T14:32:26.611143: step 3817, loss 0.02598, acc 0.984375\n",
      "2017-09-26T14:32:26.886418: step 3818, loss 0.000503355, acc 1\n",
      "2017-09-26T14:32:27.171304: step 3819, loss 0.000855999, acc 1\n",
      "2017-09-26T14:32:27.470355: step 3820, loss 0.000618806, acc 1\n",
      "2017-09-26T14:32:27.720684: step 3821, loss 0.000787287, acc 1\n",
      "2017-09-26T14:32:27.925390: step 3822, loss 0.00155378, acc 1\n",
      "2017-09-26T14:32:28.194219: step 3823, loss 0.000923202, acc 1\n",
      "2017-09-26T14:32:28.450495: step 3824, loss 0.00790697, acc 1\n",
      "2017-09-26T14:32:28.713225: step 3825, loss 0.000289399, acc 1\n",
      "2017-09-26T14:32:28.958813: step 3826, loss 0.0043849, acc 1\n",
      "2017-09-26T14:32:29.213718: step 3827, loss 0.000534561, acc 1\n",
      "2017-09-26T14:32:29.460338: step 3828, loss 0.00833947, acc 1\n",
      "2017-09-26T14:32:29.692423: step 3829, loss 0.00301368, acc 1\n",
      "2017-09-26T14:32:29.953697: step 3830, loss 0.00108727, acc 1\n",
      "2017-09-26T14:32:30.195707: step 3831, loss 0.00141998, acc 1\n",
      "2017-09-26T14:32:30.438692: step 3832, loss 0.0290535, acc 0.984375\n",
      "2017-09-26T14:32:30.679837: step 3833, loss 0.000926107, acc 1\n",
      "2017-09-26T14:32:30.922559: step 3834, loss 0.00137756, acc 1\n",
      "2017-09-26T14:32:31.171259: step 3835, loss 0.00424989, acc 1\n",
      "2017-09-26T14:32:31.430672: step 3836, loss 0.00117391, acc 1\n",
      "2017-09-26T14:32:31.678968: step 3837, loss 0.00184948, acc 1\n",
      "2017-09-26T14:32:31.917738: step 3838, loss 0.000691768, acc 1\n",
      "2017-09-26T14:32:32.150063: step 3839, loss 0.00382528, acc 1\n",
      "2017-09-26T14:32:32.395826: step 3840, loss 0.00123214, acc 1\n",
      "2017-09-26T14:32:32.628128: step 3841, loss 0.014931, acc 1\n",
      "2017-09-26T14:32:32.863062: step 3842, loss 0.000229752, acc 1\n",
      "2017-09-26T14:32:33.102515: step 3843, loss 0.000623323, acc 1\n",
      "2017-09-26T14:32:33.346977: step 3844, loss 0.00379203, acc 1\n",
      "2017-09-26T14:32:33.587228: step 3845, loss 0.000396774, acc 1\n",
      "2017-09-26T14:32:33.838634: step 3846, loss 0.000363219, acc 1\n",
      "2017-09-26T14:32:34.074800: step 3847, loss 0.00333692, acc 1\n",
      "2017-09-26T14:32:34.311152: step 3848, loss 0.000562343, acc 1\n",
      "2017-09-26T14:32:34.552599: step 3849, loss 0.00226854, acc 1\n",
      "2017-09-26T14:32:34.789795: step 3850, loss 0.00097405, acc 1\n",
      "2017-09-26T14:32:35.022325: step 3851, loss 0.00249833, acc 1\n",
      "2017-09-26T14:32:35.270307: step 3852, loss 0.00152452, acc 1\n",
      "2017-09-26T14:32:35.512701: step 3853, loss 0.000199339, acc 1\n",
      "2017-09-26T14:32:35.771370: step 3854, loss 0.00139127, acc 1\n",
      "2017-09-26T14:32:36.004821: step 3855, loss 0.000464373, acc 1\n",
      "2017-09-26T14:32:36.249791: step 3856, loss 0.000636922, acc 1\n",
      "2017-09-26T14:32:36.504402: step 3857, loss 0.00099171, acc 1\n",
      "2017-09-26T14:32:36.742758: step 3858, loss 0.000440589, acc 1\n",
      "2017-09-26T14:32:37.042082: step 3859, loss 0.043313, acc 0.984375\n",
      "2017-09-26T14:32:37.303718: step 3860, loss 0.0011498, acc 1\n",
      "2017-09-26T14:32:37.555479: step 3861, loss 0.000338037, acc 1\n",
      "2017-09-26T14:32:37.796895: step 3862, loss 0.00112257, acc 1\n",
      "2017-09-26T14:32:38.071087: step 3863, loss 0.000551182, acc 1\n",
      "2017-09-26T14:32:38.315401: step 3864, loss 0.0013277, acc 1\n",
      "2017-09-26T14:32:38.579264: step 3865, loss 0.000658413, acc 1\n",
      "2017-09-26T14:32:38.837682: step 3866, loss 0.000324495, acc 1\n",
      "2017-09-26T14:32:39.098162: step 3867, loss 0.000256112, acc 1\n",
      "2017-09-26T14:32:39.387982: step 3868, loss 0.000615012, acc 1\n",
      "2017-09-26T14:32:39.656007: step 3869, loss 0.000490678, acc 1\n",
      "2017-09-26T14:32:39.911120: step 3870, loss 0.000594867, acc 1\n",
      "2017-09-26T14:32:40.178301: step 3871, loss 0.00137197, acc 1\n",
      "2017-09-26T14:32:40.480648: step 3872, loss 0.00211086, acc 1\n",
      "2017-09-26T14:32:40.743059: step 3873, loss 0.000310876, acc 1\n",
      "2017-09-26T14:32:41.008801: step 3874, loss 0.0122145, acc 0.984375\n",
      "2017-09-26T14:32:41.265042: step 3875, loss 0.0776378, acc 0.984375\n",
      "2017-09-26T14:32:41.512418: step 3876, loss 0.000188729, acc 1\n",
      "2017-09-26T14:32:41.769540: step 3877, loss 0.00459116, acc 1\n",
      "2017-09-26T14:32:42.016881: step 3878, loss 0.00990369, acc 1\n",
      "2017-09-26T14:32:42.302711: step 3879, loss 0.000700207, acc 1\n",
      "2017-09-26T14:32:42.551022: step 3880, loss 0.000150412, acc 1\n",
      "2017-09-26T14:32:42.840430: step 3881, loss 0.000615579, acc 1\n",
      "2017-09-26T14:32:43.110857: step 3882, loss 0.00248779, acc 1\n",
      "2017-09-26T14:32:43.381685: step 3883, loss 0.00123797, acc 1\n",
      "2017-09-26T14:32:43.631520: step 3884, loss 0.00434184, acc 1\n",
      "2017-09-26T14:32:43.892238: step 3885, loss 0.000578043, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:32:44.141227: step 3886, loss 0.000427515, acc 1\n",
      "2017-09-26T14:32:44.392071: step 3887, loss 0.000439637, acc 1\n",
      "2017-09-26T14:32:44.632390: step 3888, loss 0.00565477, acc 1\n",
      "2017-09-26T14:32:44.885695: step 3889, loss 0.0632048, acc 0.984375\n",
      "2017-09-26T14:32:45.130409: step 3890, loss 0.000673441, acc 1\n",
      "2017-09-26T14:32:45.373751: step 3891, loss 0.000622264, acc 1\n",
      "2017-09-26T14:32:45.612591: step 3892, loss 0.000599582, acc 1\n",
      "2017-09-26T14:32:45.862596: step 3893, loss 0.00130044, acc 1\n",
      "2017-09-26T14:32:46.104603: step 3894, loss 0.000145359, acc 1\n",
      "2017-09-26T14:32:46.345237: step 3895, loss 0.000288963, acc 1\n",
      "2017-09-26T14:32:46.588035: step 3896, loss 0.00137951, acc 1\n",
      "2017-09-26T14:32:46.827842: step 3897, loss 0.000264998, acc 1\n",
      "2017-09-26T14:32:47.071682: step 3898, loss 0.00249418, acc 1\n",
      "2017-09-26T14:32:47.312128: step 3899, loss 0.000170797, acc 1\n",
      "2017-09-26T14:32:47.555656: step 3900, loss 0.0414342, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:32:47.798284: step 3900, loss 0.522533, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-3900\n",
      "\n",
      "2017-09-26T14:32:48.338098: step 3901, loss 0.000485632, acc 1\n",
      "2017-09-26T14:32:48.620065: step 3902, loss 0.000239674, acc 1\n",
      "2017-09-26T14:32:48.860301: step 3903, loss 0.000450091, acc 1\n",
      "2017-09-26T14:32:49.137400: step 3904, loss 0.00127319, acc 1\n",
      "2017-09-26T14:32:49.397355: step 3905, loss 0.00124747, acc 1\n",
      "2017-09-26T14:32:49.620699: step 3906, loss 0.00438854, acc 1\n",
      "2017-09-26T14:32:49.901674: step 3907, loss 0.000881759, acc 1\n",
      "2017-09-26T14:32:50.162511: step 3908, loss 0.000969206, acc 1\n",
      "2017-09-26T14:32:50.420089: step 3909, loss 0.00118015, acc 1\n",
      "2017-09-26T14:32:50.682637: step 3910, loss 0.000665401, acc 1\n",
      "2017-09-26T14:32:50.944888: step 3911, loss 0.0099802, acc 1\n",
      "2017-09-26T14:32:51.192716: step 3912, loss 0.00492256, acc 1\n",
      "2017-09-26T14:32:51.475968: step 3913, loss 0.000375379, acc 1\n",
      "2017-09-26T14:32:51.754529: step 3914, loss 0.00384075, acc 1\n",
      "2017-09-26T14:32:52.012193: step 3915, loss 0.0003329, acc 1\n",
      "2017-09-26T14:32:52.282927: step 3916, loss 0.00130959, acc 1\n",
      "2017-09-26T14:32:52.573650: step 3917, loss 0.0216689, acc 0.984375\n",
      "2017-09-26T14:32:52.855974: step 3918, loss 0.00129274, acc 1\n",
      "2017-09-26T14:32:53.150673: step 3919, loss 0.00100547, acc 1\n",
      "2017-09-26T14:32:53.434717: step 3920, loss 0.00120867, acc 1\n",
      "2017-09-26T14:32:53.727499: step 3921, loss 0.00121082, acc 1\n",
      "2017-09-26T14:32:54.024700: step 3922, loss 0.000421762, acc 1\n",
      "2017-09-26T14:32:54.333017: step 3923, loss 0.000696831, acc 1\n",
      "2017-09-26T14:32:54.610687: step 3924, loss 0.00443721, acc 1\n",
      "2017-09-26T14:32:54.896707: step 3925, loss 0.00109409, acc 1\n",
      "2017-09-26T14:32:55.175926: step 3926, loss 0.00443121, acc 1\n",
      "2017-09-26T14:32:55.446621: step 3927, loss 0.00144589, acc 1\n",
      "2017-09-26T14:32:55.712379: step 3928, loss 0.00113691, acc 1\n",
      "2017-09-26T14:32:55.976334: step 3929, loss 0.000262891, acc 1\n",
      "2017-09-26T14:32:56.256128: step 3930, loss 0.000177894, acc 1\n",
      "2017-09-26T14:32:56.545644: step 3931, loss 0.00308315, acc 1\n",
      "2017-09-26T14:32:56.824302: step 3932, loss 0.00182549, acc 1\n",
      "2017-09-26T14:32:57.096899: step 3933, loss 0.000446463, acc 1\n",
      "2017-09-26T14:32:57.387839: step 3934, loss 0.000494355, acc 1\n",
      "2017-09-26T14:32:57.634019: step 3935, loss 0.0026135, acc 1\n",
      "2017-09-26T14:32:57.906767: step 3936, loss 0.00732577, acc 1\n",
      "2017-09-26T14:32:58.170023: step 3937, loss 0.00217813, acc 1\n",
      "2017-09-26T14:32:58.418721: step 3938, loss 0.000371512, acc 1\n",
      "2017-09-26T14:32:58.678125: step 3939, loss 0.00109481, acc 1\n",
      "2017-09-26T14:32:58.943763: step 3940, loss 0.00130855, acc 1\n",
      "2017-09-26T14:32:59.232143: step 3941, loss 0.000854482, acc 1\n",
      "2017-09-26T14:32:59.520935: step 3942, loss 0.000770605, acc 1\n",
      "2017-09-26T14:32:59.822968: step 3943, loss 0.000968668, acc 1\n",
      "2017-09-26T14:33:00.089750: step 3944, loss 0.00171914, acc 1\n",
      "2017-09-26T14:33:00.335829: step 3945, loss 0.000480975, acc 1\n",
      "2017-09-26T14:33:00.583638: step 3946, loss 0.000878317, acc 1\n",
      "2017-09-26T14:33:00.835438: step 3947, loss 0.00793915, acc 1\n",
      "2017-09-26T14:33:01.055335: step 3948, loss 0.000821722, acc 1\n",
      "2017-09-26T14:33:01.315972: step 3949, loss 0.0112661, acc 1\n",
      "2017-09-26T14:33:01.561955: step 3950, loss 0.000624982, acc 1\n",
      "2017-09-26T14:33:01.835194: step 3951, loss 0.00119411, acc 1\n",
      "2017-09-26T14:33:02.127511: step 3952, loss 0.000761946, acc 1\n",
      "2017-09-26T14:33:02.422623: step 3953, loss 0.000302181, acc 1\n",
      "2017-09-26T14:33:02.692244: step 3954, loss 0.000446157, acc 1\n",
      "2017-09-26T14:33:02.983184: step 3955, loss 0.000444615, acc 1\n",
      "2017-09-26T14:33:03.255992: step 3956, loss 0.000363694, acc 1\n",
      "2017-09-26T14:33:03.534792: step 3957, loss 0.000947535, acc 1\n",
      "2017-09-26T14:33:03.803116: step 3958, loss 0.00176372, acc 1\n",
      "2017-09-26T14:33:04.066009: step 3959, loss 0.00666699, acc 1\n",
      "2017-09-26T14:33:04.349538: step 3960, loss 0.000318726, acc 1\n",
      "2017-09-26T14:33:04.636189: step 3961, loss 0.00121984, acc 1\n",
      "2017-09-26T14:33:04.915844: step 3962, loss 0.000273119, acc 1\n",
      "2017-09-26T14:33:05.220767: step 3963, loss 0.00107183, acc 1\n",
      "2017-09-26T14:33:05.515052: step 3964, loss 0.000143962, acc 1\n",
      "2017-09-26T14:33:05.801811: step 3965, loss 0.000328529, acc 1\n",
      "2017-09-26T14:33:06.075270: step 3966, loss 0.0133365, acc 0.984375\n",
      "2017-09-26T14:33:06.367180: step 3967, loss 0.000831749, acc 1\n",
      "2017-09-26T14:33:06.631829: step 3968, loss 0.00143602, acc 1\n",
      "2017-09-26T14:33:06.927473: step 3969, loss 0.000891019, acc 1\n",
      "2017-09-26T14:33:07.218673: step 3970, loss 0.00491349, acc 1\n",
      "2017-09-26T14:33:07.488793: step 3971, loss 0.000627422, acc 1\n",
      "2017-09-26T14:33:07.782676: step 3972, loss 0.000911204, acc 1\n",
      "2017-09-26T14:33:08.031068: step 3973, loss 0.000470686, acc 1\n",
      "2017-09-26T14:33:08.306247: step 3974, loss 0.00231829, acc 1\n",
      "2017-09-26T14:33:08.547340: step 3975, loss 0.000540692, acc 1\n",
      "2017-09-26T14:33:08.819635: step 3976, loss 0.000129282, acc 1\n",
      "2017-09-26T14:33:09.138844: step 3977, loss 0.000136361, acc 1\n",
      "2017-09-26T14:33:09.426860: step 3978, loss 0.00188523, acc 1\n",
      "2017-09-26T14:33:09.711293: step 3979, loss 0.000900547, acc 1\n",
      "2017-09-26T14:33:09.988412: step 3980, loss 0.000671946, acc 1\n",
      "2017-09-26T14:33:10.301519: step 3981, loss 0.00143752, acc 1\n",
      "2017-09-26T14:33:10.582649: step 3982, loss 0.00279442, acc 1\n",
      "2017-09-26T14:33:10.877138: step 3983, loss 0.000812102, acc 1\n",
      "2017-09-26T14:33:11.189402: step 3984, loss 0.000462543, acc 1\n",
      "2017-09-26T14:33:11.474856: step 3985, loss 0.00138185, acc 1\n",
      "2017-09-26T14:33:11.767581: step 3986, loss 0.000164273, acc 1\n",
      "2017-09-26T14:33:12.062488: step 3987, loss 0.000472387, acc 1\n",
      "2017-09-26T14:33:12.311792: step 3988, loss 0.000241977, acc 1\n",
      "2017-09-26T14:33:12.547642: step 3989, loss 0.000865497, acc 1\n",
      "2017-09-26T14:33:12.778883: step 3990, loss 0.00195454, acc 1\n",
      "2017-09-26T14:33:13.038979: step 3991, loss 0.00139486, acc 1\n",
      "2017-09-26T14:33:13.294899: step 3992, loss 0.00232029, acc 1\n",
      "2017-09-26T14:33:13.564454: step 3993, loss 0.000225857, acc 1\n",
      "2017-09-26T14:33:13.845724: step 3994, loss 0.000138126, acc 1\n",
      "2017-09-26T14:33:14.134954: step 3995, loss 0.000539942, acc 1\n",
      "2017-09-26T14:33:14.399216: step 3996, loss 0.000385005, acc 1\n",
      "2017-09-26T14:33:14.681975: step 3997, loss 0.000388905, acc 1\n",
      "2017-09-26T14:33:14.983901: step 3998, loss 0.00364553, acc 1\n",
      "2017-09-26T14:33:15.262107: step 3999, loss 0.00184595, acc 1\n",
      "2017-09-26T14:33:15.538800: step 4000, loss 0.000572653, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:33:15.841203: step 4000, loss 0.568809, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4000\n",
      "\n",
      "2017-09-26T14:33:16.423935: step 4001, loss 0.00031805, acc 1\n",
      "2017-09-26T14:33:16.700308: step 4002, loss 0.00236743, acc 1\n",
      "2017-09-26T14:33:16.999185: step 4003, loss 0.000187055, acc 1\n",
      "2017-09-26T14:33:17.289818: step 4004, loss 0.000678482, acc 1\n",
      "2017-09-26T14:33:17.587699: step 4005, loss 0.0893859, acc 0.984375\n",
      "2017-09-26T14:33:17.873058: step 4006, loss 0.000315028, acc 1\n",
      "2017-09-26T14:33:18.162589: step 4007, loss 0.000466209, acc 1\n",
      "2017-09-26T14:33:18.443283: step 4008, loss 0.000257564, acc 1\n",
      "2017-09-26T14:33:18.737398: step 4009, loss 0.000265086, acc 1\n",
      "2017-09-26T14:33:19.024337: step 4010, loss 0.000175683, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:33:19.266468: step 4011, loss 0.00111639, acc 1\n",
      "2017-09-26T14:33:19.516327: step 4012, loss 0.00127011, acc 1\n",
      "2017-09-26T14:33:19.761951: step 4013, loss 0.000726427, acc 1\n",
      "2017-09-26T14:33:20.011196: step 4014, loss 0.000219711, acc 1\n",
      "2017-09-26T14:33:20.245757: step 4015, loss 0.00114787, acc 1\n",
      "2017-09-26T14:33:20.496361: step 4016, loss 0.000255855, acc 1\n",
      "2017-09-26T14:33:20.740557: step 4017, loss 0.000534245, acc 1\n",
      "2017-09-26T14:33:20.989390: step 4018, loss 0.000473318, acc 1\n",
      "2017-09-26T14:33:21.229751: step 4019, loss 0.0236671, acc 0.984375\n",
      "2017-09-26T14:33:21.463663: step 4020, loss 0.00146692, acc 1\n",
      "2017-09-26T14:33:21.709264: step 4021, loss 0.00152905, acc 1\n",
      "2017-09-26T14:33:21.950012: step 4022, loss 0.00332112, acc 1\n",
      "2017-09-26T14:33:22.193591: step 4023, loss 0.00165145, acc 1\n",
      "2017-09-26T14:33:22.460034: step 4024, loss 0.000820347, acc 1\n",
      "2017-09-26T14:33:22.728524: step 4025, loss 0.00132441, acc 1\n",
      "2017-09-26T14:33:22.993443: step 4026, loss 0.000865976, acc 1\n",
      "2017-09-26T14:33:23.279147: step 4027, loss 0.000830071, acc 1\n",
      "2017-09-26T14:33:23.558089: step 4028, loss 0.000447993, acc 1\n",
      "2017-09-26T14:33:23.809152: step 4029, loss 0.000599626, acc 1\n",
      "2017-09-26T14:33:24.046760: step 4030, loss 0.000242599, acc 1\n",
      "2017-09-26T14:33:24.309140: step 4031, loss 0.00163876, acc 1\n",
      "2017-09-26T14:33:24.532428: step 4032, loss 0.00209432, acc 1\n",
      "2017-09-26T14:33:24.794536: step 4033, loss 0.00034138, acc 1\n",
      "2017-09-26T14:33:25.024328: step 4034, loss 0.00146071, acc 1\n",
      "2017-09-26T14:33:25.269215: step 4035, loss 0.0241224, acc 0.984375\n",
      "2017-09-26T14:33:25.503803: step 4036, loss 0.00100222, acc 1\n",
      "2017-09-26T14:33:25.739740: step 4037, loss 0.000875068, acc 1\n",
      "2017-09-26T14:33:25.982925: step 4038, loss 0.000747386, acc 1\n",
      "2017-09-26T14:33:26.235709: step 4039, loss 0.000345999, acc 1\n",
      "2017-09-26T14:33:26.479985: step 4040, loss 0.000621044, acc 1\n",
      "2017-09-26T14:33:26.764549: step 4041, loss 0.000703168, acc 1\n",
      "2017-09-26T14:33:27.022246: step 4042, loss 0.00248282, acc 1\n",
      "2017-09-26T14:33:27.265951: step 4043, loss 0.000651887, acc 1\n",
      "2017-09-26T14:33:27.503328: step 4044, loss 0.000146409, acc 1\n",
      "2017-09-26T14:33:27.741280: step 4045, loss 0.0009076, acc 1\n",
      "2017-09-26T14:33:27.982446: step 4046, loss 0.0097308, acc 1\n",
      "2017-09-26T14:33:28.215978: step 4047, loss 0.000397265, acc 1\n",
      "2017-09-26T14:33:28.449517: step 4048, loss 0.0010277, acc 1\n",
      "2017-09-26T14:33:28.689439: step 4049, loss 0.000579509, acc 1\n",
      "2017-09-26T14:33:28.963854: step 4050, loss 0.000364236, acc 1\n",
      "2017-09-26T14:33:29.213308: step 4051, loss 0.000501653, acc 1\n",
      "2017-09-26T14:33:29.443855: step 4052, loss 0.00281268, acc 1\n",
      "2017-09-26T14:33:29.685724: step 4053, loss 0.000467616, acc 1\n",
      "2017-09-26T14:33:29.958690: step 4054, loss 0.000174558, acc 1\n",
      "2017-09-26T14:33:30.265300: step 4055, loss 0.00182406, acc 1\n",
      "2017-09-26T14:33:30.525760: step 4056, loss 0.000843643, acc 1\n",
      "2017-09-26T14:33:30.782619: step 4057, loss 0.000144822, acc 1\n",
      "2017-09-26T14:33:31.020627: step 4058, loss 0.00239358, acc 1\n",
      "2017-09-26T14:33:31.263148: step 4059, loss 0.000257111, acc 1\n",
      "2017-09-26T14:33:31.510809: step 4060, loss 0.00267376, acc 1\n",
      "2017-09-26T14:33:31.750531: step 4061, loss 0.000293866, acc 1\n",
      "2017-09-26T14:33:31.988930: step 4062, loss 0.00023651, acc 1\n",
      "2017-09-26T14:33:32.267673: step 4063, loss 0.00095524, acc 1\n",
      "2017-09-26T14:33:32.509160: step 4064, loss 0.000127293, acc 1\n",
      "2017-09-26T14:33:32.754108: step 4065, loss 0.000377622, acc 1\n",
      "2017-09-26T14:33:33.001979: step 4066, loss 0.000383548, acc 1\n",
      "2017-09-26T14:33:33.244430: step 4067, loss 0.00160883, acc 1\n",
      "2017-09-26T14:33:33.522631: step 4068, loss 0.000472876, acc 1\n",
      "2017-09-26T14:33:33.814927: step 4069, loss 0.000309149, acc 1\n",
      "2017-09-26T14:33:34.110302: step 4070, loss 0.00120244, acc 1\n",
      "2017-09-26T14:33:34.351018: step 4071, loss 0.000397195, acc 1\n",
      "2017-09-26T14:33:34.584967: step 4072, loss 0.000639479, acc 1\n",
      "2017-09-26T14:33:34.845805: step 4073, loss 0.00100517, acc 1\n",
      "2017-09-26T14:33:35.043885: step 4074, loss 0.000224622, acc 1\n",
      "2017-09-26T14:33:35.315277: step 4075, loss 0.000376623, acc 1\n",
      "2017-09-26T14:33:35.546065: step 4076, loss 0.000247739, acc 1\n",
      "2017-09-26T14:33:35.806578: step 4077, loss 0.000773709, acc 1\n",
      "2017-09-26T14:33:36.047665: step 4078, loss 0.00131811, acc 1\n",
      "2017-09-26T14:33:36.284360: step 4079, loss 0.00228843, acc 1\n",
      "2017-09-26T14:33:36.524701: step 4080, loss 0.000333485, acc 1\n",
      "2017-09-26T14:33:36.773142: step 4081, loss 0.000547622, acc 1\n",
      "2017-09-26T14:33:37.013261: step 4082, loss 0.000155653, acc 1\n",
      "2017-09-26T14:33:37.251770: step 4083, loss 0.000839343, acc 1\n",
      "2017-09-26T14:33:37.525801: step 4084, loss 0.00439307, acc 1\n",
      "2017-09-26T14:33:37.804407: step 4085, loss 0.0682629, acc 0.984375\n",
      "2017-09-26T14:33:38.043585: step 4086, loss 0.000258936, acc 1\n",
      "2017-09-26T14:33:38.283356: step 4087, loss 0.000598397, acc 1\n",
      "2017-09-26T14:33:38.528138: step 4088, loss 0.00349313, acc 1\n",
      "2017-09-26T14:33:38.795877: step 4089, loss 0.00374393, acc 1\n",
      "2017-09-26T14:33:39.059332: step 4090, loss 0.000103351, acc 1\n",
      "2017-09-26T14:33:39.339795: step 4091, loss 0.00163918, acc 1\n",
      "2017-09-26T14:33:39.587042: step 4092, loss 0.000683073, acc 1\n",
      "2017-09-26T14:33:39.838680: step 4093, loss 0.000481591, acc 1\n",
      "2017-09-26T14:33:40.078174: step 4094, loss 0.000593589, acc 1\n",
      "2017-09-26T14:33:40.316843: step 4095, loss 0.000514991, acc 1\n",
      "2017-09-26T14:33:40.593655: step 4096, loss 0.000545728, acc 1\n",
      "2017-09-26T14:33:40.896752: step 4097, loss 0.00111302, acc 1\n",
      "2017-09-26T14:33:41.198312: step 4098, loss 0.000289911, acc 1\n",
      "2017-09-26T14:33:41.486591: step 4099, loss 0.000179197, acc 1\n",
      "2017-09-26T14:33:41.791377: step 4100, loss 0.000651051, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:33:42.067395: step 4100, loss 0.570593, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4100\n",
      "\n",
      "2017-09-26T14:33:42.687519: step 4101, loss 0.00100546, acc 1\n",
      "2017-09-26T14:33:43.001537: step 4102, loss 0.000885258, acc 1\n",
      "2017-09-26T14:33:43.292928: step 4103, loss 0.000877568, acc 1\n",
      "2017-09-26T14:33:43.591845: step 4104, loss 0.000788101, acc 1\n",
      "2017-09-26T14:33:43.872592: step 4105, loss 0.000638144, acc 1\n",
      "2017-09-26T14:33:44.171246: step 4106, loss 0.00185344, acc 1\n",
      "2017-09-26T14:33:44.431074: step 4107, loss 0.00303434, acc 1\n",
      "2017-09-26T14:33:44.724148: step 4108, loss 0.000259817, acc 1\n",
      "2017-09-26T14:33:45.020806: step 4109, loss 0.00104596, acc 1\n",
      "2017-09-26T14:33:45.283222: step 4110, loss 0.000589605, acc 1\n",
      "2017-09-26T14:33:45.546522: step 4111, loss 0.00131368, acc 1\n",
      "2017-09-26T14:33:45.822668: step 4112, loss 0.00106073, acc 1\n",
      "2017-09-26T14:33:46.100388: step 4113, loss 0.00285382, acc 1\n",
      "2017-09-26T14:33:46.407076: step 4114, loss 0.00216179, acc 1\n",
      "2017-09-26T14:33:46.671300: step 4115, loss 0.00119624, acc 1\n",
      "2017-09-26T14:33:46.923217: step 4116, loss 0.000670935, acc 1\n",
      "2017-09-26T14:33:47.186203: step 4117, loss 0.00155642, acc 1\n",
      "2017-09-26T14:33:47.456193: step 4118, loss 0.000526211, acc 1\n",
      "2017-09-26T14:33:47.742069: step 4119, loss 0.000621552, acc 1\n",
      "2017-09-26T14:33:48.030089: step 4120, loss 0.00111106, acc 1\n",
      "2017-09-26T14:33:48.343902: step 4121, loss 0.00204342, acc 1\n",
      "2017-09-26T14:33:48.622618: step 4122, loss 0.000269982, acc 1\n",
      "2017-09-26T14:33:48.897076: step 4123, loss 0.000413377, acc 1\n",
      "2017-09-26T14:33:49.186186: step 4124, loss 0.0047382, acc 1\n",
      "2017-09-26T14:33:49.490233: step 4125, loss 0.00130715, acc 1\n",
      "2017-09-26T14:33:49.803828: step 4126, loss 0.000812142, acc 1\n",
      "2017-09-26T14:33:50.097797: step 4127, loss 0.000312193, acc 1\n",
      "2017-09-26T14:33:50.399110: step 4128, loss 0.000956958, acc 1\n",
      "2017-09-26T14:33:50.714196: step 4129, loss 0.000736964, acc 1\n",
      "2017-09-26T14:33:51.021846: step 4130, loss 0.00178478, acc 1\n",
      "2017-09-26T14:33:51.339229: step 4131, loss 0.000116623, acc 1\n",
      "2017-09-26T14:33:51.650467: step 4132, loss 0.00248164, acc 1\n",
      "2017-09-26T14:33:51.984750: step 4133, loss 0.000451726, acc 1\n",
      "2017-09-26T14:33:52.328405: step 4134, loss 9.4435e-05, acc 1\n",
      "2017-09-26T14:33:52.674972: step 4135, loss 0.000429984, acc 1\n",
      "2017-09-26T14:33:52.986731: step 4136, loss 0.00215267, acc 1\n",
      "2017-09-26T14:33:53.272254: step 4137, loss 0.00419653, acc 1\n",
      "2017-09-26T14:33:53.568443: step 4138, loss 0.000552322, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:33:53.857493: step 4139, loss 0.000584923, acc 1\n",
      "2017-09-26T14:33:54.166307: step 4140, loss 0.000811654, acc 1\n",
      "2017-09-26T14:33:54.555014: step 4141, loss 0.000805089, acc 1\n",
      "2017-09-26T14:33:54.874900: step 4142, loss 0.000338351, acc 1\n",
      "2017-09-26T14:33:55.200956: step 4143, loss 0.00243849, acc 1\n",
      "2017-09-26T14:33:55.543381: step 4144, loss 0.00108484, acc 1\n",
      "2017-09-26T14:33:55.833330: step 4145, loss 0.0302263, acc 0.984375\n",
      "2017-09-26T14:33:56.166941: step 4146, loss 0.0258313, acc 0.984375\n",
      "2017-09-26T14:33:56.460236: step 4147, loss 0.00173006, acc 1\n",
      "2017-09-26T14:33:56.797015: step 4148, loss 0.000564873, acc 1\n",
      "2017-09-26T14:33:57.092672: step 4149, loss 0.000604475, acc 1\n",
      "2017-09-26T14:33:57.385327: step 4150, loss 0.000360039, acc 1\n",
      "2017-09-26T14:33:57.703916: step 4151, loss 0.000277489, acc 1\n",
      "2017-09-26T14:33:58.033483: step 4152, loss 0.00110571, acc 1\n",
      "2017-09-26T14:33:58.335733: step 4153, loss 0.000506938, acc 1\n",
      "2017-09-26T14:33:58.623509: step 4154, loss 0.00105252, acc 1\n",
      "2017-09-26T14:33:58.916621: step 4155, loss 0.0120647, acc 1\n",
      "2017-09-26T14:33:59.201239: step 4156, loss 0.000132085, acc 1\n",
      "2017-09-26T14:33:59.490458: step 4157, loss 0.000456958, acc 1\n",
      "2017-09-26T14:33:59.748349: step 4158, loss 0.00246222, acc 1\n",
      "2017-09-26T14:34:00.042678: step 4159, loss 0.000217849, acc 1\n",
      "2017-09-26T14:34:00.362625: step 4160, loss 0.000444934, acc 1\n",
      "2017-09-26T14:34:00.661134: step 4161, loss 0.000150968, acc 1\n",
      "2017-09-26T14:34:00.957438: step 4162, loss 0.00122214, acc 1\n",
      "2017-09-26T14:34:01.272963: step 4163, loss 0.000305966, acc 1\n",
      "2017-09-26T14:34:01.595169: step 4164, loss 0.000617752, acc 1\n",
      "2017-09-26T14:34:01.879244: step 4165, loss 0.00385978, acc 1\n",
      "2017-09-26T14:34:02.138061: step 4166, loss 0.00029176, acc 1\n",
      "2017-09-26T14:34:02.388212: step 4167, loss 0.00127346, acc 1\n",
      "2017-09-26T14:34:02.655857: step 4168, loss 0.00636084, acc 1\n",
      "2017-09-26T14:34:02.913256: step 4169, loss 0.000704447, acc 1\n",
      "2017-09-26T14:34:03.175788: step 4170, loss 0.000309606, acc 1\n",
      "2017-09-26T14:34:03.436421: step 4171, loss 0.000574784, acc 1\n",
      "2017-09-26T14:34:03.686633: step 4172, loss 0.0110575, acc 1\n",
      "2017-09-26T14:34:03.950931: step 4173, loss 0.00070798, acc 1\n",
      "2017-09-26T14:34:04.205952: step 4174, loss 0.0191662, acc 0.984375\n",
      "2017-09-26T14:34:04.453784: step 4175, loss 0.000509805, acc 1\n",
      "2017-09-26T14:34:04.703815: step 4176, loss 0.00110642, acc 1\n",
      "2017-09-26T14:34:04.953274: step 4177, loss 0.000177752, acc 1\n",
      "2017-09-26T14:34:05.228209: step 4178, loss 0.000269325, acc 1\n",
      "2017-09-26T14:34:05.514135: step 4179, loss 0.000143184, acc 1\n",
      "2017-09-26T14:34:05.765505: step 4180, loss 0.000147842, acc 1\n",
      "2017-09-26T14:34:06.019679: step 4181, loss 0.000618265, acc 1\n",
      "2017-09-26T14:34:06.275305: step 4182, loss 0.000585311, acc 1\n",
      "2017-09-26T14:34:06.543522: step 4183, loss 0.000314186, acc 1\n",
      "2017-09-26T14:34:06.831322: step 4184, loss 0.0011928, acc 1\n",
      "2017-09-26T14:34:07.095642: step 4185, loss 0.00183571, acc 1\n",
      "2017-09-26T14:34:07.390304: step 4186, loss 0.011955, acc 1\n",
      "2017-09-26T14:34:07.674857: step 4187, loss 0.000749883, acc 1\n",
      "2017-09-26T14:34:07.935990: step 4188, loss 0.000700594, acc 1\n",
      "2017-09-26T14:34:08.319243: step 4189, loss 0.000316505, acc 1\n",
      "2017-09-26T14:34:08.652709: step 4190, loss 0.00101795, acc 1\n",
      "2017-09-26T14:34:08.907248: step 4191, loss 0.000156362, acc 1\n",
      "2017-09-26T14:34:09.164117: step 4192, loss 0.00150099, acc 1\n",
      "2017-09-26T14:34:09.441162: step 4193, loss 0.000867009, acc 1\n",
      "2017-09-26T14:34:09.711725: step 4194, loss 0.00167077, acc 1\n",
      "2017-09-26T14:34:09.983553: step 4195, loss 0.00139027, acc 1\n",
      "2017-09-26T14:34:10.316440: step 4196, loss 0.00226201, acc 1\n",
      "2017-09-26T14:34:10.588152: step 4197, loss 0.000801886, acc 1\n",
      "2017-09-26T14:34:10.879650: step 4198, loss 0.000466018, acc 1\n",
      "2017-09-26T14:34:11.154878: step 4199, loss 0.000119106, acc 1\n",
      "2017-09-26T14:34:11.408232: step 4200, loss 0.000239573, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:34:11.723332: step 4200, loss 0.590513, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4200\n",
      "\n",
      "2017-09-26T14:34:12.396476: step 4201, loss 0.000656417, acc 1\n",
      "2017-09-26T14:34:12.703061: step 4202, loss 0.0002608, acc 1\n",
      "2017-09-26T14:34:12.979307: step 4203, loss 0.0023731, acc 1\n",
      "2017-09-26T14:34:13.301612: step 4204, loss 0.00126489, acc 1\n",
      "2017-09-26T14:34:13.628764: step 4205, loss 0.000367316, acc 1\n",
      "2017-09-26T14:34:13.999754: step 4206, loss 0.000560795, acc 1\n",
      "2017-09-26T14:34:14.337935: step 4207, loss 0.000171292, acc 1\n",
      "2017-09-26T14:34:14.606438: step 4208, loss 0.00232938, acc 1\n",
      "2017-09-26T14:34:14.866252: step 4209, loss 0.000712339, acc 1\n",
      "2017-09-26T14:34:15.133097: step 4210, loss 0.000574472, acc 1\n",
      "2017-09-26T14:34:15.395465: step 4211, loss 0.000850993, acc 1\n",
      "2017-09-26T14:34:15.672457: step 4212, loss 0.00212009, acc 1\n",
      "2017-09-26T14:34:15.941945: step 4213, loss 0.000822773, acc 1\n",
      "2017-09-26T14:34:16.204796: step 4214, loss 0.000461474, acc 1\n",
      "2017-09-26T14:34:16.472935: step 4215, loss 0.000271987, acc 1\n",
      "2017-09-26T14:34:16.746324: step 4216, loss 0.0135349, acc 0.984375\n",
      "2017-09-26T14:34:17.049415: step 4217, loss 0.000174383, acc 1\n",
      "2017-09-26T14:34:17.305162: step 4218, loss 0.000293248, acc 1\n",
      "2017-09-26T14:34:17.593508: step 4219, loss 0.000202201, acc 1\n",
      "2017-09-26T14:34:17.969616: step 4220, loss 0.0103263, acc 1\n",
      "2017-09-26T14:34:18.338967: step 4221, loss 0.00049257, acc 1\n",
      "2017-09-26T14:34:18.683386: step 4222, loss 0.0026147, acc 1\n",
      "2017-09-26T14:34:18.959843: step 4223, loss 0.00312273, acc 1\n",
      "2017-09-26T14:34:19.329815: step 4224, loss 0.000505939, acc 1\n",
      "2017-09-26T14:34:19.668311: step 4225, loss 0.000225768, acc 1\n",
      "2017-09-26T14:34:20.015088: step 4226, loss 0.0146237, acc 0.984375\n",
      "2017-09-26T14:34:20.374872: step 4227, loss 0.000146839, acc 1\n",
      "2017-09-26T14:34:20.727719: step 4228, loss 0.00155207, acc 1\n",
      "2017-09-26T14:34:20.983494: step 4229, loss 0.000414963, acc 1\n",
      "2017-09-26T14:34:21.250807: step 4230, loss 0.000130198, acc 1\n",
      "2017-09-26T14:34:21.513947: step 4231, loss 0.0876076, acc 0.984375\n",
      "2017-09-26T14:34:21.772288: step 4232, loss 0.00120327, acc 1\n",
      "2017-09-26T14:34:22.043855: step 4233, loss 0.000352319, acc 1\n",
      "2017-09-26T14:34:22.302245: step 4234, loss 0.000622836, acc 1\n",
      "2017-09-26T14:34:22.595555: step 4235, loss 0.000279084, acc 1\n",
      "2017-09-26T14:34:22.867814: step 4236, loss 0.000461079, acc 1\n",
      "2017-09-26T14:34:23.129156: step 4237, loss 0.00013475, acc 1\n",
      "2017-09-26T14:34:23.434653: step 4238, loss 0.00119318, acc 1\n",
      "2017-09-26T14:34:23.705823: step 4239, loss 0.000681139, acc 1\n",
      "2017-09-26T14:34:23.972620: step 4240, loss 0.000599567, acc 1\n",
      "2017-09-26T14:34:24.247215: step 4241, loss 0.0037286, acc 1\n",
      "2017-09-26T14:34:24.499500: step 4242, loss 0.000879678, acc 1\n",
      "2017-09-26T14:34:24.771245: step 4243, loss 0.000212193, acc 1\n",
      "2017-09-26T14:34:25.035957: step 4244, loss 0.00465616, acc 1\n",
      "2017-09-26T14:34:25.300303: step 4245, loss 0.000812192, acc 1\n",
      "2017-09-26T14:34:25.578426: step 4246, loss 0.000481486, acc 1\n",
      "2017-09-26T14:34:25.856725: step 4247, loss 0.000709527, acc 1\n",
      "2017-09-26T14:34:26.128620: step 4248, loss 0.000175989, acc 1\n",
      "2017-09-26T14:34:26.434316: step 4249, loss 0.00104143, acc 1\n",
      "2017-09-26T14:34:26.704840: step 4250, loss 0.000450618, acc 1\n",
      "2017-09-26T14:34:26.978719: step 4251, loss 0.000507767, acc 1\n",
      "2017-09-26T14:34:27.267452: step 4252, loss 0.000341391, acc 1\n",
      "2017-09-26T14:34:27.534275: step 4253, loss 0.00124554, acc 1\n",
      "2017-09-26T14:34:27.865639: step 4254, loss 0.0261443, acc 0.984375\n",
      "2017-09-26T14:34:28.128571: step 4255, loss 0.000239749, acc 1\n",
      "2017-09-26T14:34:28.408253: step 4256, loss 0.000573781, acc 1\n",
      "2017-09-26T14:34:28.705351: step 4257, loss 0.00789323, acc 1\n",
      "2017-09-26T14:34:28.961903: step 4258, loss 0.000533449, acc 1\n",
      "2017-09-26T14:34:29.259604: step 4259, loss 0.00151424, acc 1\n",
      "2017-09-26T14:34:29.513971: step 4260, loss 0.000347835, acc 1\n",
      "2017-09-26T14:34:29.787069: step 4261, loss 0.000570398, acc 1\n",
      "2017-09-26T14:34:30.089267: step 4262, loss 0.0334517, acc 0.984375\n",
      "2017-09-26T14:34:30.440306: step 4263, loss 0.00022337, acc 1\n",
      "2017-09-26T14:34:30.769210: step 4264, loss 0.00124083, acc 1\n",
      "2017-09-26T14:34:31.025062: step 4265, loss 0.0015609, acc 1\n",
      "2017-09-26T14:34:31.297403: step 4266, loss 0.000189875, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:34:31.555562: step 4267, loss 0.00076976, acc 1\n",
      "2017-09-26T14:34:31.886270: step 4268, loss 0.00054982, acc 1\n",
      "2017-09-26T14:34:32.192254: step 4269, loss 0.00054575, acc 1\n",
      "2017-09-26T14:34:32.508150: step 4270, loss 0.000108565, acc 1\n",
      "2017-09-26T14:34:32.762135: step 4271, loss 0.00055678, acc 1\n",
      "2017-09-26T14:34:33.028252: step 4272, loss 0.0015762, acc 1\n",
      "2017-09-26T14:34:33.304453: step 4273, loss 0.00036481, acc 1\n",
      "2017-09-26T14:34:33.562986: step 4274, loss 0.007478, acc 1\n",
      "2017-09-26T14:34:33.825961: step 4275, loss 0.00109036, acc 1\n",
      "2017-09-26T14:34:34.118421: step 4276, loss 0.000517876, acc 1\n",
      "2017-09-26T14:34:34.383346: step 4277, loss 0.000699512, acc 1\n",
      "2017-09-26T14:34:34.654937: step 4278, loss 0.00107547, acc 1\n",
      "2017-09-26T14:34:34.906379: step 4279, loss 0.00207554, acc 1\n",
      "2017-09-26T14:34:35.246993: step 4280, loss 0.00122176, acc 1\n",
      "2017-09-26T14:34:35.591787: step 4281, loss 0.000107636, acc 1\n",
      "2017-09-26T14:34:35.917836: step 4282, loss 0.0114839, acc 0.984375\n",
      "2017-09-26T14:34:36.269297: step 4283, loss 0.000396338, acc 1\n",
      "2017-09-26T14:34:36.506392: step 4284, loss 0.000870131, acc 1\n",
      "2017-09-26T14:34:36.806409: step 4285, loss 0.000474896, acc 1\n",
      "2017-09-26T14:34:37.146996: step 4286, loss 0.00114652, acc 1\n",
      "2017-09-26T14:34:37.506261: step 4287, loss 0.000531814, acc 1\n",
      "2017-09-26T14:34:37.870417: step 4288, loss 0.00111241, acc 1\n",
      "2017-09-26T14:34:38.189002: step 4289, loss 0.00065856, acc 1\n",
      "2017-09-26T14:34:38.519133: step 4290, loss 0.00101133, acc 1\n",
      "2017-09-26T14:34:38.788857: step 4291, loss 0.000294687, acc 1\n",
      "2017-09-26T14:34:39.052888: step 4292, loss 0.00133658, acc 1\n",
      "2017-09-26T14:34:39.325806: step 4293, loss 0.000393778, acc 1\n",
      "2017-09-26T14:34:39.582069: step 4294, loss 0.000802153, acc 1\n",
      "2017-09-26T14:34:39.849774: step 4295, loss 0.00179917, acc 1\n",
      "2017-09-26T14:34:40.113256: step 4296, loss 0.00190562, acc 1\n",
      "2017-09-26T14:34:40.385457: step 4297, loss 0.00151935, acc 1\n",
      "2017-09-26T14:34:40.636245: step 4298, loss 0.000319275, acc 1\n",
      "2017-09-26T14:34:40.902488: step 4299, loss 0.000583735, acc 1\n",
      "2017-09-26T14:34:41.153641: step 4300, loss 0.000811843, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:34:41.415632: step 4300, loss 0.553619, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4300\n",
      "\n",
      "2017-09-26T14:34:41.961531: step 4301, loss 0.00491136, acc 1\n",
      "2017-09-26T14:34:42.221529: step 4302, loss 0.0463529, acc 0.984375\n",
      "2017-09-26T14:34:42.479354: step 4303, loss 8.66641e-05, acc 1\n",
      "2017-09-26T14:34:42.749953: step 4304, loss 0.000289702, acc 1\n",
      "2017-09-26T14:34:43.042142: step 4305, loss 0.0025067, acc 1\n",
      "2017-09-26T14:34:43.296425: step 4306, loss 0.00117623, acc 1\n",
      "2017-09-26T14:34:43.578027: step 4307, loss 0.000440495, acc 1\n",
      "2017-09-26T14:34:43.873766: step 4308, loss 0.000611813, acc 1\n",
      "2017-09-26T14:34:44.179163: step 4309, loss 0.0018815, acc 1\n",
      "2017-09-26T14:34:44.542868: step 4310, loss 0.000201019, acc 1\n",
      "2017-09-26T14:34:44.899707: step 4311, loss 0.00140339, acc 1\n",
      "2017-09-26T14:34:45.276961: step 4312, loss 8.96261e-05, acc 1\n",
      "2017-09-26T14:34:45.578316: step 4313, loss 0.00195256, acc 1\n",
      "2017-09-26T14:34:45.954595: step 4314, loss 0.00222773, acc 1\n",
      "2017-09-26T14:34:46.290522: step 4315, loss 0.000143239, acc 1\n",
      "2017-09-26T14:34:46.614950: step 4316, loss 0.00991004, acc 1\n",
      "2017-09-26T14:34:46.958161: step 4317, loss 0.000205216, acc 1\n",
      "2017-09-26T14:34:47.257864: step 4318, loss 0.000177733, acc 1\n",
      "2017-09-26T14:34:47.528077: step 4319, loss 0.00180812, acc 1\n",
      "2017-09-26T14:34:47.800834: step 4320, loss 0.000176666, acc 1\n",
      "2017-09-26T14:34:48.159968: step 4321, loss 0.0234166, acc 0.984375\n",
      "2017-09-26T14:34:48.487103: step 4322, loss 0.000281085, acc 1\n",
      "2017-09-26T14:34:48.789219: step 4323, loss 0.000225337, acc 1\n",
      "2017-09-26T14:34:49.055389: step 4324, loss 0.000166104, acc 1\n",
      "2017-09-26T14:34:49.320482: step 4325, loss 0.000256706, acc 1\n",
      "2017-09-26T14:34:49.579403: step 4326, loss 0.00733306, acc 1\n",
      "2017-09-26T14:34:49.868503: step 4327, loss 0.000118092, acc 1\n",
      "2017-09-26T14:34:50.141678: step 4328, loss 0.000217988, acc 1\n",
      "2017-09-26T14:34:50.400233: step 4329, loss 0.00136008, acc 1\n",
      "2017-09-26T14:34:50.681844: step 4330, loss 0.00395349, acc 1\n",
      "2017-09-26T14:34:51.027272: step 4331, loss 0.00075333, acc 1\n",
      "2017-09-26T14:34:51.363876: step 4332, loss 0.0235529, acc 0.984375\n",
      "2017-09-26T14:34:51.703819: step 4333, loss 0.000598599, acc 1\n",
      "2017-09-26T14:34:52.037810: step 4334, loss 0.000235403, acc 1\n",
      "2017-09-26T14:34:52.282485: step 4335, loss 0.000967492, acc 1\n",
      "2017-09-26T14:34:52.537998: step 4336, loss 0.000291022, acc 1\n",
      "2017-09-26T14:34:52.877764: step 4337, loss 0.00068969, acc 1\n",
      "2017-09-26T14:34:53.223320: step 4338, loss 0.000218281, acc 1\n",
      "2017-09-26T14:34:53.571566: step 4339, loss 0.0148065, acc 0.984375\n",
      "2017-09-26T14:34:53.872198: step 4340, loss 0.00667893, acc 1\n",
      "2017-09-26T14:34:54.156265: step 4341, loss 0.000375258, acc 1\n",
      "2017-09-26T14:34:54.451287: step 4342, loss 0.000311655, acc 1\n",
      "2017-09-26T14:34:54.761493: step 4343, loss 0.000223176, acc 1\n",
      "2017-09-26T14:34:55.080513: step 4344, loss 0.00262935, acc 1\n",
      "2017-09-26T14:34:55.340189: step 4345, loss 0.00160263, acc 1\n",
      "2017-09-26T14:34:55.594423: step 4346, loss 0.000432958, acc 1\n",
      "2017-09-26T14:34:55.889683: step 4347, loss 0.000536851, acc 1\n",
      "2017-09-26T14:34:56.201832: step 4348, loss 0.00201599, acc 1\n",
      "2017-09-26T14:34:56.495602: step 4349, loss 0.000642841, acc 1\n",
      "2017-09-26T14:34:56.756018: step 4350, loss 0.000783338, acc 1\n",
      "2017-09-26T14:34:57.081786: step 4351, loss 0.000557271, acc 1\n",
      "2017-09-26T14:34:57.357761: step 4352, loss 0.000839777, acc 1\n",
      "2017-09-26T14:34:57.603827: step 4353, loss 0.00157386, acc 1\n",
      "2017-09-26T14:34:57.875770: step 4354, loss 0.00114722, acc 1\n",
      "2017-09-26T14:34:58.116528: step 4355, loss 0.000534567, acc 1\n",
      "2017-09-26T14:34:58.357754: step 4356, loss 0.000422769, acc 1\n",
      "2017-09-26T14:34:58.612486: step 4357, loss 0.000563201, acc 1\n",
      "2017-09-26T14:34:58.958437: step 4358, loss 0.00329657, acc 1\n",
      "2017-09-26T14:34:59.283255: step 4359, loss 0.000125205, acc 1\n",
      "2017-09-26T14:34:59.600986: step 4360, loss 0.00165254, acc 1\n",
      "2017-09-26T14:34:59.921869: step 4361, loss 0.000336801, acc 1\n",
      "2017-09-26T14:35:00.190823: step 4362, loss 0.000412562, acc 1\n",
      "2017-09-26T14:35:00.444135: step 4363, loss 0.000166766, acc 1\n",
      "2017-09-26T14:35:00.760359: step 4364, loss 0.0011415, acc 1\n",
      "2017-09-26T14:35:01.090033: step 4365, loss 0.00119815, acc 1\n",
      "2017-09-26T14:35:01.397872: step 4366, loss 0.000350148, acc 1\n",
      "2017-09-26T14:35:01.693951: step 4367, loss 0.000763706, acc 1\n",
      "2017-09-26T14:35:01.920843: step 4368, loss 0.000128453, acc 1\n",
      "2017-09-26T14:35:02.170542: step 4369, loss 0.0279868, acc 0.984375\n",
      "2017-09-26T14:35:02.420625: step 4370, loss 0.000362015, acc 1\n",
      "2017-09-26T14:35:02.691352: step 4371, loss 0.00226126, acc 1\n",
      "2017-09-26T14:35:02.934809: step 4372, loss 0.00036676, acc 1\n",
      "2017-09-26T14:35:03.200352: step 4373, loss 0.00078938, acc 1\n",
      "2017-09-26T14:35:03.448895: step 4374, loss 0.00444185, acc 1\n",
      "2017-09-26T14:35:03.702282: step 4375, loss 0.00017684, acc 1\n",
      "2017-09-26T14:35:03.963609: step 4376, loss 0.00026324, acc 1\n",
      "2017-09-26T14:35:04.267822: step 4377, loss 0.000219072, acc 1\n",
      "2017-09-26T14:35:04.542360: step 4378, loss 0.00194519, acc 1\n",
      "2017-09-26T14:35:04.801009: step 4379, loss 0.000648041, acc 1\n",
      "2017-09-26T14:35:05.055741: step 4380, loss 0.00630206, acc 1\n",
      "2017-09-26T14:35:05.292494: step 4381, loss 0.000202482, acc 1\n",
      "2017-09-26T14:35:05.522232: step 4382, loss 0.00190138, acc 1\n",
      "2017-09-26T14:35:05.764214: step 4383, loss 0.000416147, acc 1\n",
      "2017-09-26T14:35:06.004727: step 4384, loss 0.0286843, acc 0.984375\n",
      "2017-09-26T14:35:06.234976: step 4385, loss 9.23718e-05, acc 1\n",
      "2017-09-26T14:35:06.466964: step 4386, loss 0.000366705, acc 1\n",
      "2017-09-26T14:35:06.769039: step 4387, loss 0.000248737, acc 1\n",
      "2017-09-26T14:35:07.039247: step 4388, loss 0.000193017, acc 1\n",
      "2017-09-26T14:35:07.297881: step 4389, loss 0.000933291, acc 1\n",
      "2017-09-26T14:35:07.542342: step 4390, loss 0.000685205, acc 1\n",
      "2017-09-26T14:35:07.791474: step 4391, loss 0.000247535, acc 1\n",
      "2017-09-26T14:35:08.041889: step 4392, loss 0.000252247, acc 1\n",
      "2017-09-26T14:35:08.290727: step 4393, loss 0.000688234, acc 1\n",
      "2017-09-26T14:35:08.538013: step 4394, loss 0.000284503, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:35:08.783800: step 4395, loss 0.00116979, acc 1\n",
      "2017-09-26T14:35:09.086063: step 4396, loss 0.000444654, acc 1\n",
      "2017-09-26T14:35:09.387894: step 4397, loss 0.00353901, acc 1\n",
      "2017-09-26T14:35:09.693156: step 4398, loss 0.00104205, acc 1\n",
      "2017-09-26T14:35:09.993647: step 4399, loss 0.000924747, acc 1\n",
      "2017-09-26T14:35:10.311682: step 4400, loss 0.0150627, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:35:10.641416: step 4400, loss 0.636679, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4400\n",
      "\n",
      "2017-09-26T14:35:11.202160: step 4401, loss 0.00158542, acc 1\n",
      "2017-09-26T14:35:11.443799: step 4402, loss 0.0016535, acc 1\n",
      "2017-09-26T14:35:11.677645: step 4403, loss 0.00155885, acc 1\n",
      "2017-09-26T14:35:11.914221: step 4404, loss 0.000162079, acc 1\n",
      "2017-09-26T14:35:12.160513: step 4405, loss 0.000947669, acc 1\n",
      "2017-09-26T14:35:12.409239: step 4406, loss 9.47707e-05, acc 1\n",
      "2017-09-26T14:35:12.654349: step 4407, loss 0.000454956, acc 1\n",
      "2017-09-26T14:35:12.902360: step 4408, loss 0.00043121, acc 1\n",
      "2017-09-26T14:35:13.138588: step 4409, loss 0.00248114, acc 1\n",
      "2017-09-26T14:35:13.354444: step 4410, loss 0.000468307, acc 1\n",
      "2017-09-26T14:35:13.714019: step 4411, loss 0.000341539, acc 1\n",
      "2017-09-26T14:35:14.001889: step 4412, loss 0.00544544, acc 1\n",
      "2017-09-26T14:35:14.256637: step 4413, loss 0.000467988, acc 1\n",
      "2017-09-26T14:35:14.491984: step 4414, loss 0.0129656, acc 0.984375\n",
      "2017-09-26T14:35:14.809392: step 4415, loss 0.00026491, acc 1\n",
      "2017-09-26T14:35:15.091029: step 4416, loss 0.000463309, acc 1\n",
      "2017-09-26T14:35:15.329672: step 4417, loss 0.000186153, acc 1\n",
      "2017-09-26T14:35:15.563876: step 4418, loss 0.00056065, acc 1\n",
      "2017-09-26T14:35:15.804985: step 4419, loss 0.000489822, acc 1\n",
      "2017-09-26T14:35:16.041063: step 4420, loss 0.00669209, acc 1\n",
      "2017-09-26T14:35:16.282205: step 4421, loss 0.000811496, acc 1\n",
      "2017-09-26T14:35:16.515700: step 4422, loss 0.00178012, acc 1\n",
      "2017-09-26T14:35:16.753976: step 4423, loss 0.00351861, acc 1\n",
      "2017-09-26T14:35:17.009509: step 4424, loss 0.00217155, acc 1\n",
      "2017-09-26T14:35:17.248607: step 4425, loss 0.00251279, acc 1\n",
      "2017-09-26T14:35:17.496524: step 4426, loss 0.000276303, acc 1\n",
      "2017-09-26T14:35:17.746448: step 4427, loss 0.00141279, acc 1\n",
      "2017-09-26T14:35:17.982472: step 4428, loss 0.00174346, acc 1\n",
      "2017-09-26T14:35:18.278257: step 4429, loss 0.00251368, acc 1\n",
      "2017-09-26T14:35:18.622582: step 4430, loss 0.00149196, acc 1\n",
      "2017-09-26T14:35:18.913885: step 4431, loss 0.000570057, acc 1\n",
      "2017-09-26T14:35:19.169310: step 4432, loss 0.00522964, acc 1\n",
      "2017-09-26T14:35:19.474306: step 4433, loss 0.00103767, acc 1\n",
      "2017-09-26T14:35:19.781746: step 4434, loss 0.0280734, acc 0.984375\n",
      "2017-09-26T14:35:20.019351: step 4435, loss 0.000238132, acc 1\n",
      "2017-09-26T14:35:20.346691: step 4436, loss 0.00190057, acc 1\n",
      "2017-09-26T14:35:20.669609: step 4437, loss 0.00124156, acc 1\n",
      "2017-09-26T14:35:20.951940: step 4438, loss 0.00243121, acc 1\n",
      "2017-09-26T14:35:21.267261: step 4439, loss 0.000437785, acc 1\n",
      "2017-09-26T14:35:21.493822: step 4440, loss 0.000823566, acc 1\n",
      "2017-09-26T14:35:21.738073: step 4441, loss 0.000894827, acc 1\n",
      "2017-09-26T14:35:22.004373: step 4442, loss 0.000263697, acc 1\n",
      "2017-09-26T14:35:22.248486: step 4443, loss 0.000384641, acc 1\n",
      "2017-09-26T14:35:22.493623: step 4444, loss 0.000483673, acc 1\n",
      "2017-09-26T14:35:22.737937: step 4445, loss 0.00354873, acc 1\n",
      "2017-09-26T14:35:22.988850: step 4446, loss 0.000955514, acc 1\n",
      "2017-09-26T14:35:23.227688: step 4447, loss 0.000441499, acc 1\n",
      "2017-09-26T14:35:23.492433: step 4448, loss 0.000703725, acc 1\n",
      "2017-09-26T14:35:23.734686: step 4449, loss 0.000363247, acc 1\n",
      "2017-09-26T14:35:24.044088: step 4450, loss 0.000600812, acc 1\n",
      "2017-09-26T14:35:24.358151: step 4451, loss 0.00226136, acc 1\n",
      "2017-09-26T14:35:24.652905: step 4452, loss 0.00293057, acc 1\n",
      "2017-09-26T14:35:24.980885: step 4453, loss 0.00306318, acc 1\n",
      "2017-09-26T14:35:25.238592: step 4454, loss 0.000221689, acc 1\n",
      "2017-09-26T14:35:25.501443: step 4455, loss 0.00280518, acc 1\n",
      "2017-09-26T14:35:25.741166: step 4456, loss 0.000103683, acc 1\n",
      "2017-09-26T14:35:26.006271: step 4457, loss 0.000161426, acc 1\n",
      "2017-09-26T14:35:26.252736: step 4458, loss 0.000209895, acc 1\n",
      "2017-09-26T14:35:26.492901: step 4459, loss 0.00058354, acc 1\n",
      "2017-09-26T14:35:26.735637: step 4460, loss 0.000424691, acc 1\n",
      "2017-09-26T14:35:26.975281: step 4461, loss 0.000231938, acc 1\n",
      "2017-09-26T14:35:27.218812: step 4462, loss 0.00010987, acc 1\n",
      "2017-09-26T14:35:27.467953: step 4463, loss 0.00521796, acc 1\n",
      "2017-09-26T14:35:27.706028: step 4464, loss 0.000473363, acc 1\n",
      "2017-09-26T14:35:27.952269: step 4465, loss 0.000401434, acc 1\n",
      "2017-09-26T14:35:28.196001: step 4466, loss 0.00142967, acc 1\n",
      "2017-09-26T14:35:28.453096: step 4467, loss 0.0012862, acc 1\n",
      "2017-09-26T14:35:28.697119: step 4468, loss 0.000207866, acc 1\n",
      "2017-09-26T14:35:28.972093: step 4469, loss 0.000318169, acc 1\n",
      "2017-09-26T14:35:29.281824: step 4470, loss 0.00757359, acc 1\n",
      "2017-09-26T14:35:29.608427: step 4471, loss 0.000275445, acc 1\n",
      "2017-09-26T14:35:29.928761: step 4472, loss 0.000255691, acc 1\n",
      "2017-09-26T14:35:30.248279: step 4473, loss 0.000589628, acc 1\n",
      "2017-09-26T14:35:30.482818: step 4474, loss 0.00037751, acc 1\n",
      "2017-09-26T14:35:30.769998: step 4475, loss 0.000113033, acc 1\n",
      "2017-09-26T14:35:31.039254: step 4476, loss 0.000605817, acc 1\n",
      "2017-09-26T14:35:31.285755: step 4477, loss 0.00310816, acc 1\n",
      "2017-09-26T14:35:31.539982: step 4478, loss 0.00031894, acc 1\n",
      "2017-09-26T14:35:31.778201: step 4479, loss 0.000187204, acc 1\n",
      "2017-09-26T14:35:32.018451: step 4480, loss 0.000484064, acc 1\n",
      "2017-09-26T14:35:32.261160: step 4481, loss 0.000861486, acc 1\n",
      "2017-09-26T14:35:32.505360: step 4482, loss 0.000659792, acc 1\n",
      "2017-09-26T14:35:32.744865: step 4483, loss 0.000267802, acc 1\n",
      "2017-09-26T14:35:32.982948: step 4484, loss 0.00102446, acc 1\n",
      "2017-09-26T14:35:33.229233: step 4485, loss 0.000217103, acc 1\n",
      "2017-09-26T14:35:33.481119: step 4486, loss 0.000210098, acc 1\n",
      "2017-09-26T14:35:33.727699: step 4487, loss 0.000822891, acc 1\n",
      "2017-09-26T14:35:33.973377: step 4488, loss 0.00040929, acc 1\n",
      "2017-09-26T14:35:34.220159: step 4489, loss 0.000913438, acc 1\n",
      "2017-09-26T14:35:34.482756: step 4490, loss 0.000144565, acc 1\n",
      "2017-09-26T14:35:34.726024: step 4491, loss 0.000805033, acc 1\n",
      "2017-09-26T14:35:34.997410: step 4492, loss 0.0786961, acc 0.984375\n",
      "2017-09-26T14:35:35.243714: step 4493, loss 0.0015718, acc 1\n",
      "2017-09-26T14:35:35.513645: step 4494, loss 0.00466277, acc 1\n",
      "2017-09-26T14:35:35.816902: step 4495, loss 0.000150415, acc 1\n",
      "2017-09-26T14:35:36.067207: step 4496, loss 0.000176516, acc 1\n",
      "2017-09-26T14:35:36.360811: step 4497, loss 0.00180867, acc 1\n",
      "2017-09-26T14:35:36.661495: step 4498, loss 0.015421, acc 0.984375\n",
      "2017-09-26T14:35:36.942268: step 4499, loss 0.000480189, acc 1\n",
      "2017-09-26T14:35:37.256885: step 4500, loss 0.00110557, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:35:37.506073: step 4500, loss 0.617393, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4500\n",
      "\n",
      "2017-09-26T14:35:38.040528: step 4501, loss 0.000593736, acc 1\n",
      "2017-09-26T14:35:38.294480: step 4502, loss 8.56754e-05, acc 1\n",
      "2017-09-26T14:35:38.534973: step 4503, loss 0.000411376, acc 1\n",
      "2017-09-26T14:35:38.783820: step 4504, loss 0.000329118, acc 1\n",
      "2017-09-26T14:35:39.031461: step 4505, loss 0.00188875, acc 1\n",
      "2017-09-26T14:35:39.320650: step 4506, loss 0.000427333, acc 1\n",
      "2017-09-26T14:35:39.633629: step 4507, loss 0.000221461, acc 1\n",
      "2017-09-26T14:35:39.945665: step 4508, loss 0.00054235, acc 1\n",
      "2017-09-26T14:35:40.268199: step 4509, loss 0.000382789, acc 1\n",
      "2017-09-26T14:35:40.528872: step 4510, loss 0.00246433, acc 1\n",
      "2017-09-26T14:35:40.818277: step 4511, loss 0.000711832, acc 1\n",
      "2017-09-26T14:35:41.083683: step 4512, loss 0.000234982, acc 1\n",
      "2017-09-26T14:35:41.353035: step 4513, loss 0.00141804, acc 1\n",
      "2017-09-26T14:35:41.626449: step 4514, loss 0.000509497, acc 1\n",
      "2017-09-26T14:35:41.930827: step 4515, loss 0.000740306, acc 1\n",
      "2017-09-26T14:35:42.195613: step 4516, loss 0.00049732, acc 1\n",
      "2017-09-26T14:35:42.459668: step 4517, loss 0.00362522, acc 1\n",
      "2017-09-26T14:35:42.731791: step 4518, loss 0.00488403, acc 1\n",
      "2017-09-26T14:35:43.035966: step 4519, loss 0.000391573, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:35:43.348740: step 4520, loss 0.00493147, acc 1\n",
      "2017-09-26T14:35:43.616019: step 4521, loss 0.00209555, acc 1\n",
      "2017-09-26T14:35:43.847518: step 4522, loss 0.000435672, acc 1\n",
      "2017-09-26T14:35:44.081149: step 4523, loss 0.000524268, acc 1\n",
      "2017-09-26T14:35:44.343987: step 4524, loss 0.0113115, acc 1\n",
      "2017-09-26T14:35:44.615356: step 4525, loss 0.000270588, acc 1\n",
      "2017-09-26T14:35:44.865620: step 4526, loss 0.000116023, acc 1\n",
      "2017-09-26T14:35:45.114476: step 4527, loss 0.00250439, acc 1\n",
      "2017-09-26T14:35:45.377178: step 4528, loss 0.000169953, acc 1\n",
      "2017-09-26T14:35:45.628046: step 4529, loss 0.00428042, acc 1\n",
      "2017-09-26T14:35:45.877933: step 4530, loss 0.000353292, acc 1\n",
      "2017-09-26T14:35:46.112060: step 4531, loss 0.00122558, acc 1\n",
      "2017-09-26T14:35:46.344351: step 4532, loss 0.00247891, acc 1\n",
      "2017-09-26T14:35:46.578004: step 4533, loss 0.000173037, acc 1\n",
      "2017-09-26T14:35:46.816965: step 4534, loss 0.00721518, acc 1\n",
      "2017-09-26T14:35:47.055378: step 4535, loss 0.00103159, acc 1\n",
      "2017-09-26T14:35:47.260222: step 4536, loss 0.00105936, acc 1\n",
      "2017-09-26T14:35:47.504215: step 4537, loss 0.000559474, acc 1\n",
      "2017-09-26T14:35:47.734761: step 4538, loss 0.00102022, acc 1\n",
      "2017-09-26T14:35:47.971182: step 4539, loss 0.00113605, acc 1\n",
      "2017-09-26T14:35:48.203775: step 4540, loss 0.00141749, acc 1\n",
      "2017-09-26T14:35:48.435356: step 4541, loss 0.00142464, acc 1\n",
      "2017-09-26T14:35:48.667099: step 4542, loss 0.000326767, acc 1\n",
      "2017-09-26T14:35:48.908512: step 4543, loss 0.00158684, acc 1\n",
      "2017-09-26T14:35:49.142674: step 4544, loss 0.00141668, acc 1\n",
      "2017-09-26T14:35:49.374410: step 4545, loss 0.000954004, acc 1\n",
      "2017-09-26T14:35:49.621379: step 4546, loss 0.00024837, acc 1\n",
      "2017-09-26T14:35:49.852457: step 4547, loss 0.000512341, acc 1\n",
      "2017-09-26T14:35:50.083713: step 4548, loss 0.000344372, acc 1\n",
      "2017-09-26T14:35:50.323162: step 4549, loss 0.000379339, acc 1\n",
      "2017-09-26T14:35:50.577434: step 4550, loss 0.000389414, acc 1\n",
      "2017-09-26T14:35:50.807383: step 4551, loss 0.0761469, acc 0.984375\n",
      "2017-09-26T14:35:51.040776: step 4552, loss 0.000133176, acc 1\n",
      "2017-09-26T14:35:51.269969: step 4553, loss 0.0013267, acc 1\n",
      "2017-09-26T14:35:51.506435: step 4554, loss 0.0443578, acc 0.984375\n",
      "2017-09-26T14:35:51.744437: step 4555, loss 0.0067226, acc 1\n",
      "2017-09-26T14:35:51.993227: step 4556, loss 0.0155519, acc 0.984375\n",
      "2017-09-26T14:35:52.222295: step 4557, loss 0.000108691, acc 1\n",
      "2017-09-26T14:35:52.461092: step 4558, loss 0.00226042, acc 1\n",
      "2017-09-26T14:35:52.697706: step 4559, loss 0.000719563, acc 1\n",
      "2017-09-26T14:35:52.932448: step 4560, loss 0.000296975, acc 1\n",
      "2017-09-26T14:35:53.179954: step 4561, loss 0.000479936, acc 1\n",
      "2017-09-26T14:35:53.413427: step 4562, loss 0.00100877, acc 1\n",
      "2017-09-26T14:35:53.648614: step 4563, loss 0.000638046, acc 1\n",
      "2017-09-26T14:35:53.885063: step 4564, loss 0.000123057, acc 1\n",
      "2017-09-26T14:35:54.119423: step 4565, loss 0.000229596, acc 1\n",
      "2017-09-26T14:35:54.370575: step 4566, loss 0.000141684, acc 1\n",
      "2017-09-26T14:35:54.617968: step 4567, loss 0.000503116, acc 1\n",
      "2017-09-26T14:35:54.883479: step 4568, loss 0.000498219, acc 1\n",
      "2017-09-26T14:35:55.114383: step 4569, loss 0.00317364, acc 1\n",
      "2017-09-26T14:35:55.352500: step 4570, loss 0.000471696, acc 1\n",
      "2017-09-26T14:35:55.585482: step 4571, loss 0.000162589, acc 1\n",
      "2017-09-26T14:35:55.824050: step 4572, loss 0.00274003, acc 1\n",
      "2017-09-26T14:35:56.070830: step 4573, loss 0.000707238, acc 1\n",
      "2017-09-26T14:35:56.306200: step 4574, loss 0.00171477, acc 1\n",
      "2017-09-26T14:35:56.541440: step 4575, loss 0.00782405, acc 1\n",
      "2017-09-26T14:35:56.778746: step 4576, loss 0.000703829, acc 1\n",
      "2017-09-26T14:35:57.020951: step 4577, loss 0.00025454, acc 1\n",
      "2017-09-26T14:35:57.234348: step 4578, loss 0.000268059, acc 1\n",
      "2017-09-26T14:35:57.472676: step 4579, loss 0.000817621, acc 1\n",
      "2017-09-26T14:35:57.709102: step 4580, loss 0.000432777, acc 1\n",
      "2017-09-26T14:35:57.955576: step 4581, loss 0.000259002, acc 1\n",
      "2017-09-26T14:35:58.193154: step 4582, loss 0.000339056, acc 1\n",
      "2017-09-26T14:35:58.428185: step 4583, loss 0.00074662, acc 1\n",
      "2017-09-26T14:35:58.664412: step 4584, loss 0.000592089, acc 1\n",
      "2017-09-26T14:35:58.904115: step 4585, loss 0.00179158, acc 1\n",
      "2017-09-26T14:35:59.136545: step 4586, loss 0.00936926, acc 1\n",
      "2017-09-26T14:35:59.368847: step 4587, loss 0.000601664, acc 1\n",
      "2017-09-26T14:35:59.606425: step 4588, loss 0.000137001, acc 1\n",
      "2017-09-26T14:35:59.852858: step 4589, loss 0.000469308, acc 1\n",
      "2017-09-26T14:36:00.090366: step 4590, loss 0.00135644, acc 1\n",
      "2017-09-26T14:36:00.328824: step 4591, loss 0.00479161, acc 1\n",
      "2017-09-26T14:36:00.563447: step 4592, loss 0.000854035, acc 1\n",
      "2017-09-26T14:36:00.807511: step 4593, loss 0.000193274, acc 1\n",
      "2017-09-26T14:36:01.043649: step 4594, loss 0.000423737, acc 1\n",
      "2017-09-26T14:36:01.278922: step 4595, loss 0.000305459, acc 1\n",
      "2017-09-26T14:36:01.517788: step 4596, loss 0.000161934, acc 1\n",
      "2017-09-26T14:36:01.751679: step 4597, loss 0.0681351, acc 0.984375\n",
      "2017-09-26T14:36:01.989450: step 4598, loss 0.000357738, acc 1\n",
      "2017-09-26T14:36:02.221987: step 4599, loss 0.000575435, acc 1\n",
      "2017-09-26T14:36:02.457324: step 4600, loss 0.000548675, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:36:02.689277: step 4600, loss 0.605112, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4600\n",
      "\n",
      "2017-09-26T14:36:03.179911: step 4601, loss 0.00161854, acc 1\n",
      "2017-09-26T14:36:03.413018: step 4602, loss 0.00014896, acc 1\n",
      "2017-09-26T14:36:03.649542: step 4603, loss 0.000251812, acc 1\n",
      "2017-09-26T14:36:03.882513: step 4604, loss 0.000801067, acc 1\n",
      "2017-09-26T14:36:04.117699: step 4605, loss 0.000351708, acc 1\n",
      "2017-09-26T14:36:04.359270: step 4606, loss 0.00324551, acc 1\n",
      "2017-09-26T14:36:04.596729: step 4607, loss 0.00063192, acc 1\n",
      "2017-09-26T14:36:04.840184: step 4608, loss 0.000317161, acc 1\n",
      "2017-09-26T14:36:05.079320: step 4609, loss 0.00166209, acc 1\n",
      "2017-09-26T14:36:05.330948: step 4610, loss 0.00027896, acc 1\n",
      "2017-09-26T14:36:05.567459: step 4611, loss 0.00228013, acc 1\n",
      "2017-09-26T14:36:05.799351: step 4612, loss 0.000424033, acc 1\n",
      "2017-09-26T14:36:06.043661: step 4613, loss 0.000963217, acc 1\n",
      "2017-09-26T14:36:06.277396: step 4614, loss 0.00022797, acc 1\n",
      "2017-09-26T14:36:06.512851: step 4615, loss 0.000134851, acc 1\n",
      "2017-09-26T14:36:06.751999: step 4616, loss 0.00107186, acc 1\n",
      "2017-09-26T14:36:06.986773: step 4617, loss 0.000152186, acc 1\n",
      "2017-09-26T14:36:07.223089: step 4618, loss 0.000587658, acc 1\n",
      "2017-09-26T14:36:07.461470: step 4619, loss 0.00343763, acc 1\n",
      "2017-09-26T14:36:07.669735: step 4620, loss 5.91757e-05, acc 1\n",
      "2017-09-26T14:36:07.914772: step 4621, loss 0.00202455, acc 1\n",
      "2017-09-26T14:36:08.151099: step 4622, loss 0.000509816, acc 1\n",
      "2017-09-26T14:36:08.389800: step 4623, loss 0.0336333, acc 0.984375\n",
      "2017-09-26T14:36:08.629019: step 4624, loss 0.000921678, acc 1\n",
      "2017-09-26T14:36:08.866769: step 4625, loss 0.00178697, acc 1\n",
      "2017-09-26T14:36:09.116705: step 4626, loss 0.000754236, acc 1\n",
      "2017-09-26T14:36:09.367914: step 4627, loss 0.00425797, acc 1\n",
      "2017-09-26T14:36:09.608868: step 4628, loss 0.00148994, acc 1\n",
      "2017-09-26T14:36:09.845906: step 4629, loss 0.000884916, acc 1\n",
      "2017-09-26T14:36:10.080925: step 4630, loss 0.000247513, acc 1\n",
      "2017-09-26T14:36:10.320288: step 4631, loss 0.000239423, acc 1\n",
      "2017-09-26T14:36:10.558464: step 4632, loss 0.000546242, acc 1\n",
      "2017-09-26T14:36:10.795317: step 4633, loss 0.000492544, acc 1\n",
      "2017-09-26T14:36:11.032651: step 4634, loss 0.000479289, acc 1\n",
      "2017-09-26T14:36:11.269111: step 4635, loss 0.00132164, acc 1\n",
      "2017-09-26T14:36:11.509158: step 4636, loss 0.00100399, acc 1\n",
      "2017-09-26T14:36:11.752309: step 4637, loss 0.00116728, acc 1\n",
      "2017-09-26T14:36:12.018779: step 4638, loss 0.000955142, acc 1\n",
      "2017-09-26T14:36:12.256509: step 4639, loss 0.000767328, acc 1\n",
      "2017-09-26T14:36:12.492864: step 4640, loss 0.00075529, acc 1\n",
      "2017-09-26T14:36:12.738011: step 4641, loss 0.000912457, acc 1\n",
      "2017-09-26T14:36:12.978146: step 4642, loss 0.000352152, acc 1\n",
      "2017-09-26T14:36:13.228958: step 4643, loss 0.00016203, acc 1\n",
      "2017-09-26T14:36:13.464453: step 4644, loss 0.000131524, acc 1\n",
      "2017-09-26T14:36:13.706440: step 4645, loss 0.000842862, acc 1\n",
      "2017-09-26T14:36:13.945303: step 4646, loss 0.000160143, acc 1\n",
      "2017-09-26T14:36:14.182598: step 4647, loss 0.000131734, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:36:14.421413: step 4648, loss 0.00841842, acc 1\n",
      "2017-09-26T14:36:14.663304: step 4649, loss 0.000873081, acc 1\n",
      "2017-09-26T14:36:14.892075: step 4650, loss 0.00164513, acc 1\n",
      "2017-09-26T14:36:15.131346: step 4651, loss 0.000496892, acc 1\n",
      "2017-09-26T14:36:15.362752: step 4652, loss 0.000572166, acc 1\n",
      "2017-09-26T14:36:15.597265: step 4653, loss 0.000808141, acc 1\n",
      "2017-09-26T14:36:15.838030: step 4654, loss 0.000135061, acc 1\n",
      "2017-09-26T14:36:16.076538: step 4655, loss 0.000141433, acc 1\n",
      "2017-09-26T14:36:16.305265: step 4656, loss 0.000220979, acc 1\n",
      "2017-09-26T14:36:16.603649: step 4657, loss 0.00017104, acc 1\n",
      "2017-09-26T14:36:16.938524: step 4658, loss 0.000418655, acc 1\n",
      "2017-09-26T14:36:17.247636: step 4659, loss 0.00109064, acc 1\n",
      "2017-09-26T14:36:17.578021: step 4660, loss 0.000391176, acc 1\n",
      "2017-09-26T14:36:17.920332: step 4661, loss 0.000302561, acc 1\n",
      "2017-09-26T14:36:18.130442: step 4662, loss 0.000308509, acc 1\n",
      "2017-09-26T14:36:18.374649: step 4663, loss 0.000271128, acc 1\n",
      "2017-09-26T14:36:18.649667: step 4664, loss 0.00225937, acc 1\n",
      "2017-09-26T14:36:18.996271: step 4665, loss 0.000104724, acc 1\n",
      "2017-09-26T14:36:19.329110: step 4666, loss 0.00050556, acc 1\n",
      "2017-09-26T14:36:19.639836: step 4667, loss 0.000259413, acc 1\n",
      "2017-09-26T14:36:19.964046: step 4668, loss 0.000292043, acc 1\n",
      "2017-09-26T14:36:20.196591: step 4669, loss 0.000211871, acc 1\n",
      "2017-09-26T14:36:20.430375: step 4670, loss 0.000296289, acc 1\n",
      "2017-09-26T14:36:20.673128: step 4671, loss 0.00101293, acc 1\n",
      "2017-09-26T14:36:20.906701: step 4672, loss 0.0196253, acc 0.984375\n",
      "2017-09-26T14:36:21.139738: step 4673, loss 0.000319963, acc 1\n",
      "2017-09-26T14:36:21.389391: step 4674, loss 0.000746482, acc 1\n",
      "2017-09-26T14:36:21.622129: step 4675, loss 0.00100185, acc 1\n",
      "2017-09-26T14:36:21.866081: step 4676, loss 0.000851256, acc 1\n",
      "2017-09-26T14:36:22.097371: step 4677, loss 0.000958879, acc 1\n",
      "2017-09-26T14:36:22.437071: step 4678, loss 0.000329852, acc 1\n",
      "2017-09-26T14:36:22.793103: step 4679, loss 0.00051939, acc 1\n",
      "2017-09-26T14:36:23.119040: step 4680, loss 0.000244463, acc 1\n",
      "2017-09-26T14:36:23.448055: step 4681, loss 0.000487937, acc 1\n",
      "2017-09-26T14:36:23.770333: step 4682, loss 0.000574712, acc 1\n",
      "2017-09-26T14:36:24.070793: step 4683, loss 0.000259589, acc 1\n",
      "2017-09-26T14:36:24.314351: step 4684, loss 0.0161878, acc 0.984375\n",
      "2017-09-26T14:36:24.550012: step 4685, loss 0.000804586, acc 1\n",
      "2017-09-26T14:36:24.787797: step 4686, loss 0.000274499, acc 1\n",
      "2017-09-26T14:36:25.029059: step 4687, loss 0.000526137, acc 1\n",
      "2017-09-26T14:36:25.274752: step 4688, loss 0.000470158, acc 1\n",
      "2017-09-26T14:36:25.619636: step 4689, loss 0.000125042, acc 1\n",
      "2017-09-26T14:36:25.941311: step 4690, loss 5.36426e-05, acc 1\n",
      "2017-09-26T14:36:26.238576: step 4691, loss 0.000504563, acc 1\n",
      "2017-09-26T14:36:26.466303: step 4692, loss 0.000131403, acc 1\n",
      "2017-09-26T14:36:26.753217: step 4693, loss 0.000546143, acc 1\n",
      "2017-09-26T14:36:27.077690: step 4694, loss 0.0261049, acc 0.984375\n",
      "2017-09-26T14:36:27.377075: step 4695, loss 0.000362388, acc 1\n",
      "2017-09-26T14:36:27.669399: step 4696, loss 0.000187586, acc 1\n",
      "2017-09-26T14:36:28.001732: step 4697, loss 0.00483387, acc 1\n",
      "2017-09-26T14:36:28.297278: step 4698, loss 0.000177252, acc 1\n",
      "2017-09-26T14:36:28.544569: step 4699, loss 0.000385597, acc 1\n",
      "2017-09-26T14:36:28.785897: step 4700, loss 0.00141192, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:36:29.086955: step 4700, loss 0.638267, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4700\n",
      "\n",
      "2017-09-26T14:36:29.682360: step 4701, loss 0.00040702, acc 1\n",
      "2017-09-26T14:36:29.920580: step 4702, loss 0.00083674, acc 1\n",
      "2017-09-26T14:36:30.195114: step 4703, loss 0.000431621, acc 1\n",
      "2017-09-26T14:36:30.458702: step 4704, loss 0.000213653, acc 1\n",
      "2017-09-26T14:36:30.759711: step 4705, loss 0.00230124, acc 1\n",
      "2017-09-26T14:36:31.033838: step 4706, loss 0.0330758, acc 0.984375\n",
      "2017-09-26T14:36:31.270391: step 4707, loss 0.000579479, acc 1\n",
      "2017-09-26T14:36:31.512719: step 4708, loss 0.000797642, acc 1\n",
      "2017-09-26T14:36:31.771391: step 4709, loss 0.000406355, acc 1\n",
      "2017-09-26T14:36:32.014297: step 4710, loss 0.000623473, acc 1\n",
      "2017-09-26T14:36:32.253353: step 4711, loss 0.000307637, acc 1\n",
      "2017-09-26T14:36:32.511026: step 4712, loss 9.63799e-05, acc 1\n",
      "2017-09-26T14:36:32.749455: step 4713, loss 0.00058701, acc 1\n",
      "2017-09-26T14:36:32.989986: step 4714, loss 0.000296265, acc 1\n",
      "2017-09-26T14:36:33.230429: step 4715, loss 7.2485e-05, acc 1\n",
      "2017-09-26T14:36:33.500229: step 4716, loss 0.00177616, acc 1\n",
      "2017-09-26T14:36:33.752303: step 4717, loss 0.00069485, acc 1\n",
      "2017-09-26T14:36:33.993241: step 4718, loss 0.00216802, acc 1\n",
      "2017-09-26T14:36:34.247390: step 4719, loss 0.000618236, acc 1\n",
      "2017-09-26T14:36:34.485687: step 4720, loss 0.000622679, acc 1\n",
      "2017-09-26T14:36:34.730413: step 4721, loss 0.000370394, acc 1\n",
      "2017-09-26T14:36:34.969678: step 4722, loss 0.000471741, acc 1\n",
      "2017-09-26T14:36:35.213154: step 4723, loss 0.00600642, acc 1\n",
      "2017-09-26T14:36:35.452544: step 4724, loss 0.000406139, acc 1\n",
      "2017-09-26T14:36:35.702733: step 4725, loss 0.000588241, acc 1\n",
      "2017-09-26T14:36:36.017461: step 4726, loss 0.039944, acc 0.984375\n",
      "2017-09-26T14:36:36.342779: step 4727, loss 0.00677377, acc 1\n",
      "2017-09-26T14:36:36.654556: step 4728, loss 0.000313809, acc 1\n",
      "2017-09-26T14:36:36.913493: step 4729, loss 0.000287709, acc 1\n",
      "2017-09-26T14:36:37.152768: step 4730, loss 0.000152426, acc 1\n",
      "2017-09-26T14:36:37.430460: step 4731, loss 0.000382898, acc 1\n",
      "2017-09-26T14:36:37.717360: step 4732, loss 0.000175471, acc 1\n",
      "2017-09-26T14:36:37.966505: step 4733, loss 0.000717488, acc 1\n",
      "2017-09-26T14:36:38.212192: step 4734, loss 0.000525922, acc 1\n",
      "2017-09-26T14:36:38.480466: step 4735, loss 0.000472162, acc 1\n",
      "2017-09-26T14:36:38.727442: step 4736, loss 0.0040241, acc 1\n",
      "2017-09-26T14:36:38.978403: step 4737, loss 0.0054164, acc 1\n",
      "2017-09-26T14:36:39.231198: step 4738, loss 0.00013191, acc 1\n",
      "2017-09-26T14:36:39.484043: step 4739, loss 0.000334477, acc 1\n",
      "2017-09-26T14:36:39.731222: step 4740, loss 0.000301579, acc 1\n",
      "2017-09-26T14:36:39.982393: step 4741, loss 0.000694896, acc 1\n",
      "2017-09-26T14:36:40.240975: step 4742, loss 0.00220236, acc 1\n",
      "2017-09-26T14:36:40.490729: step 4743, loss 0.000628683, acc 1\n",
      "2017-09-26T14:36:40.741760: step 4744, loss 0.000114153, acc 1\n",
      "2017-09-26T14:36:41.063454: step 4745, loss 0.00119654, acc 1\n",
      "2017-09-26T14:36:41.316629: step 4746, loss 8.8807e-05, acc 1\n",
      "2017-09-26T14:36:41.632534: step 4747, loss 0.0903978, acc 0.984375\n",
      "2017-09-26T14:36:41.960878: step 4748, loss 0.000485506, acc 1\n",
      "2017-09-26T14:36:42.274578: step 4749, loss 0.000301437, acc 1\n",
      "2017-09-26T14:36:42.574473: step 4750, loss 0.000476977, acc 1\n",
      "2017-09-26T14:36:42.864662: step 4751, loss 0.000634884, acc 1\n",
      "2017-09-26T14:36:43.130232: step 4752, loss 0.00145399, acc 1\n",
      "2017-09-26T14:36:43.409024: step 4753, loss 0.00419956, acc 1\n",
      "2017-09-26T14:36:43.655820: step 4754, loss 0.0005236, acc 1\n",
      "2017-09-26T14:36:43.956208: step 4755, loss 0.000369626, acc 1\n",
      "2017-09-26T14:36:44.211321: step 4756, loss 0.000216509, acc 1\n",
      "2017-09-26T14:36:44.468644: step 4757, loss 0.000171971, acc 1\n",
      "2017-09-26T14:36:44.733379: step 4758, loss 0.00553898, acc 1\n",
      "2017-09-26T14:36:45.008923: step 4759, loss 0.000651992, acc 1\n",
      "2017-09-26T14:36:45.343201: step 4760, loss 0.000575264, acc 1\n",
      "2017-09-26T14:36:45.609631: step 4761, loss 0.000754017, acc 1\n",
      "2017-09-26T14:36:45.897586: step 4762, loss 0.00370388, acc 1\n",
      "2017-09-26T14:36:46.231326: step 4763, loss 0.00174021, acc 1\n",
      "2017-09-26T14:36:46.539772: step 4764, loss 0.000785852, acc 1\n",
      "2017-09-26T14:36:46.862564: step 4765, loss 0.000380435, acc 1\n",
      "2017-09-26T14:36:47.172657: step 4766, loss 0.0029935, acc 1\n",
      "2017-09-26T14:36:47.480759: step 4767, loss 0.00261617, acc 1\n",
      "2017-09-26T14:36:47.748621: step 4768, loss 0.000168, acc 1\n",
      "2017-09-26T14:36:48.034236: step 4769, loss 0.00118993, acc 1\n",
      "2017-09-26T14:36:48.317870: step 4770, loss 0.000415667, acc 1\n",
      "2017-09-26T14:36:48.586122: step 4771, loss 0.00119797, acc 1\n",
      "2017-09-26T14:36:48.856203: step 4772, loss 0.00043896, acc 1\n",
      "2017-09-26T14:36:49.126002: step 4773, loss 0.000772385, acc 1\n",
      "2017-09-26T14:36:49.386327: step 4774, loss 0.000283742, acc 1\n",
      "2017-09-26T14:36:49.645340: step 4775, loss 0.000712259, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:36:49.897261: step 4776, loss 0.00820466, acc 1\n",
      "2017-09-26T14:36:50.195386: step 4777, loss 0.000296993, acc 1\n",
      "2017-09-26T14:36:50.442000: step 4778, loss 0.00031756, acc 1\n",
      "2017-09-26T14:36:50.695985: step 4779, loss 0.000342041, acc 1\n",
      "2017-09-26T14:36:51.007073: step 4780, loss 0.000588542, acc 1\n",
      "2017-09-26T14:36:51.264874: step 4781, loss 0.000157845, acc 1\n",
      "2017-09-26T14:36:51.492199: step 4782, loss 0.000784551, acc 1\n",
      "2017-09-26T14:36:51.743347: step 4783, loss 0.000181304, acc 1\n",
      "2017-09-26T14:36:51.989012: step 4784, loss 0.000162607, acc 1\n",
      "2017-09-26T14:36:52.226085: step 4785, loss 0.00175221, acc 1\n",
      "2017-09-26T14:36:52.540960: step 4786, loss 0.00124334, acc 1\n",
      "2017-09-26T14:36:52.792107: step 4787, loss 0.00127917, acc 1\n",
      "2017-09-26T14:36:53.032018: step 4788, loss 9.88117e-05, acc 1\n",
      "2017-09-26T14:36:53.287901: step 4789, loss 0.000343361, acc 1\n",
      "2017-09-26T14:36:53.551163: step 4790, loss 0.000140371, acc 1\n",
      "2017-09-26T14:36:53.902108: step 4791, loss 0.000199787, acc 1\n",
      "2017-09-26T14:36:54.249498: step 4792, loss 0.000324628, acc 1\n",
      "2017-09-26T14:36:54.532169: step 4793, loss 0.000499809, acc 1\n",
      "2017-09-26T14:36:54.831091: step 4794, loss 0.00110725, acc 1\n",
      "2017-09-26T14:36:55.126144: step 4795, loss 0.000683045, acc 1\n",
      "2017-09-26T14:36:55.401811: step 4796, loss 0.000319474, acc 1\n",
      "2017-09-26T14:36:55.669331: step 4797, loss 0.000480766, acc 1\n",
      "2017-09-26T14:36:55.923866: step 4798, loss 0.00235782, acc 1\n",
      "2017-09-26T14:36:56.181141: step 4799, loss 0.000145378, acc 1\n",
      "2017-09-26T14:36:56.448451: step 4800, loss 0.000139791, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:36:56.773747: step 4800, loss 0.61144, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4800\n",
      "\n",
      "2017-09-26T14:36:57.311247: step 4801, loss 0.00020312, acc 1\n",
      "2017-09-26T14:36:57.562642: step 4802, loss 0.000517338, acc 1\n",
      "2017-09-26T14:36:57.823965: step 4803, loss 0.00348412, acc 1\n",
      "2017-09-26T14:36:58.141321: step 4804, loss 0.000321677, acc 1\n",
      "2017-09-26T14:36:58.402804: step 4805, loss 0.000348966, acc 1\n",
      "2017-09-26T14:36:58.661218: step 4806, loss 0.000351603, acc 1\n",
      "2017-09-26T14:36:58.952976: step 4807, loss 0.000107651, acc 1\n",
      "2017-09-26T14:36:59.213910: step 4808, loss 0.00269421, acc 1\n",
      "2017-09-26T14:36:59.479739: step 4809, loss 0.00138513, acc 1\n",
      "2017-09-26T14:36:59.751572: step 4810, loss 0.000417893, acc 1\n",
      "2017-09-26T14:37:00.033757: step 4811, loss 0.000134482, acc 1\n",
      "2017-09-26T14:37:00.327663: step 4812, loss 0.00252224, acc 1\n",
      "2017-09-26T14:37:00.588352: step 4813, loss 0.00132504, acc 1\n",
      "2017-09-26T14:37:00.846090: step 4814, loss 0.000205486, acc 1\n",
      "2017-09-26T14:37:01.137102: step 4815, loss 0.000621198, acc 1\n",
      "2017-09-26T14:37:01.439316: step 4816, loss 0.000202958, acc 1\n",
      "2017-09-26T14:37:01.744347: step 4817, loss 0.00214248, acc 1\n",
      "2017-09-26T14:37:02.040687: step 4818, loss 0.00061018, acc 1\n",
      "2017-09-26T14:37:02.314253: step 4819, loss 0.000122858, acc 1\n",
      "2017-09-26T14:37:02.589278: step 4820, loss 0.00320265, acc 1\n",
      "2017-09-26T14:37:02.886951: step 4821, loss 0.000473483, acc 1\n",
      "2017-09-26T14:37:03.172603: step 4822, loss 0.000784873, acc 1\n",
      "2017-09-26T14:37:03.471102: step 4823, loss 0.000405163, acc 1\n",
      "2017-09-26T14:37:03.751802: step 4824, loss 0.000366456, acc 1\n",
      "2017-09-26T14:37:04.021773: step 4825, loss 0.000602185, acc 1\n",
      "2017-09-26T14:37:04.291145: step 4826, loss 0.0115482, acc 1\n",
      "2017-09-26T14:37:04.560323: step 4827, loss 0.000827565, acc 1\n",
      "2017-09-26T14:37:04.824688: step 4828, loss 0.000653443, acc 1\n",
      "2017-09-26T14:37:05.131017: step 4829, loss 0.00328731, acc 1\n",
      "2017-09-26T14:37:05.381794: step 4830, loss 8.43513e-05, acc 1\n",
      "2017-09-26T14:37:05.654745: step 4831, loss 0.000228825, acc 1\n",
      "2017-09-26T14:37:05.931215: step 4832, loss 0.00488481, acc 1\n",
      "2017-09-26T14:37:06.208995: step 4833, loss 0.000309682, acc 1\n",
      "2017-09-26T14:37:06.493262: step 4834, loss 0.000154003, acc 1\n",
      "2017-09-26T14:37:06.794782: step 4835, loss 0.00025098, acc 1\n",
      "2017-09-26T14:37:07.095848: step 4836, loss 0.000545311, acc 1\n",
      "2017-09-26T14:37:07.379658: step 4837, loss 0.000162175, acc 1\n",
      "2017-09-26T14:37:07.645773: step 4838, loss 0.000874114, acc 1\n",
      "2017-09-26T14:37:07.926213: step 4839, loss 0.000138994, acc 1\n",
      "2017-09-26T14:37:08.226221: step 4840, loss 0.00124422, acc 1\n",
      "2017-09-26T14:37:08.508119: step 4841, loss 0.000861002, acc 1\n",
      "2017-09-26T14:37:08.795481: step 4842, loss 0.00128468, acc 1\n",
      "2017-09-26T14:37:09.089427: step 4843, loss 4.67996e-05, acc 1\n",
      "2017-09-26T14:37:09.384557: step 4844, loss 0.0021885, acc 1\n",
      "2017-09-26T14:37:09.675340: step 4845, loss 0.000378958, acc 1\n",
      "2017-09-26T14:37:09.953472: step 4846, loss 0.012392, acc 0.984375\n",
      "2017-09-26T14:37:10.250836: step 4847, loss 0.000135782, acc 1\n",
      "2017-09-26T14:37:10.536404: step 4848, loss 7.08945e-05, acc 1\n",
      "2017-09-26T14:37:10.801540: step 4849, loss 0.000303758, acc 1\n",
      "2017-09-26T14:37:11.104195: step 4850, loss 0.0302436, acc 0.984375\n",
      "2017-09-26T14:37:11.383913: step 4851, loss 0.000506751, acc 1\n",
      "2017-09-26T14:37:11.661497: step 4852, loss 0.000263982, acc 1\n",
      "2017-09-26T14:37:11.949992: step 4853, loss 9.61139e-05, acc 1\n",
      "2017-09-26T14:37:12.239561: step 4854, loss 0.000920309, acc 1\n",
      "2017-09-26T14:37:12.506637: step 4855, loss 0.00135213, acc 1\n",
      "2017-09-26T14:37:12.781422: step 4856, loss 0.00112354, acc 1\n",
      "2017-09-26T14:37:13.060184: step 4857, loss 7.93013e-05, acc 1\n",
      "2017-09-26T14:37:13.416102: step 4858, loss 0.000394764, acc 1\n",
      "2017-09-26T14:37:13.679312: step 4859, loss 0.00030505, acc 1\n",
      "2017-09-26T14:37:13.959929: step 4860, loss 0.000152491, acc 1\n",
      "2017-09-26T14:37:14.225026: step 4861, loss 0.00185827, acc 1\n",
      "2017-09-26T14:37:14.479850: step 4862, loss 0.000420985, acc 1\n",
      "2017-09-26T14:37:14.773149: step 4863, loss 0.0131911, acc 1\n",
      "2017-09-26T14:37:15.036361: step 4864, loss 0.000392692, acc 1\n",
      "2017-09-26T14:37:15.340263: step 4865, loss 0.108113, acc 0.984375\n",
      "2017-09-26T14:37:15.607598: step 4866, loss 0.000488056, acc 1\n",
      "2017-09-26T14:37:15.925384: step 4867, loss 0.000607037, acc 1\n",
      "2017-09-26T14:37:16.258632: step 4868, loss 0.00052613, acc 1\n",
      "2017-09-26T14:37:16.527797: step 4869, loss 0.00017872, acc 1\n",
      "2017-09-26T14:37:16.807787: step 4870, loss 0.000579393, acc 1\n",
      "2017-09-26T14:37:17.102129: step 4871, loss 0.00112117, acc 1\n",
      "2017-09-26T14:37:17.338582: step 4872, loss 0.0013327, acc 1\n",
      "2017-09-26T14:37:17.655062: step 4873, loss 0.000109449, acc 1\n",
      "2017-09-26T14:37:17.936354: step 4874, loss 0.0208708, acc 0.984375\n",
      "2017-09-26T14:37:18.192760: step 4875, loss 0.000954321, acc 1\n",
      "2017-09-26T14:37:18.447205: step 4876, loss 0.000792385, acc 1\n",
      "2017-09-26T14:37:18.710020: step 4877, loss 0.00098005, acc 1\n",
      "2017-09-26T14:37:19.010595: step 4878, loss 0.000742566, acc 1\n",
      "2017-09-26T14:37:19.292068: step 4879, loss 0.0014024, acc 1\n",
      "2017-09-26T14:37:19.564701: step 4880, loss 0.00119881, acc 1\n",
      "2017-09-26T14:37:19.845042: step 4881, loss 0.000528577, acc 1\n",
      "2017-09-26T14:37:20.127653: step 4882, loss 0.000110827, acc 1\n",
      "2017-09-26T14:37:20.383912: step 4883, loss 0.0221971, acc 0.984375\n",
      "2017-09-26T14:37:20.707082: step 4884, loss 0.000708129, acc 1\n",
      "2017-09-26T14:37:20.992595: step 4885, loss 0.000735018, acc 1\n",
      "2017-09-26T14:37:21.288694: step 4886, loss 0.00192988, acc 1\n",
      "2017-09-26T14:37:21.580418: step 4887, loss 0.000488329, acc 1\n",
      "2017-09-26T14:37:21.903645: step 4888, loss 0.000126082, acc 1\n",
      "2017-09-26T14:37:22.224849: step 4889, loss 0.0203246, acc 0.984375\n",
      "2017-09-26T14:37:22.587001: step 4890, loss 0.000784359, acc 1\n",
      "2017-09-26T14:37:22.897049: step 4891, loss 0.000457695, acc 1\n",
      "2017-09-26T14:37:23.204254: step 4892, loss 0.000276895, acc 1\n",
      "2017-09-26T14:37:23.495009: step 4893, loss 7.58444e-05, acc 1\n",
      "2017-09-26T14:37:23.810929: step 4894, loss 0.000183611, acc 1\n",
      "2017-09-26T14:37:24.120734: step 4895, loss 0.0654671, acc 0.984375\n",
      "2017-09-26T14:37:24.426811: step 4896, loss 0.00012507, acc 1\n",
      "2017-09-26T14:37:24.732328: step 4897, loss 0.000132565, acc 1\n",
      "2017-09-26T14:37:25.049923: step 4898, loss 0.000183248, acc 1\n",
      "2017-09-26T14:37:25.441500: step 4899, loss 0.000271889, acc 1\n",
      "2017-09-26T14:37:25.888323: step 4900, loss 0.0010985, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:37:26.257247: step 4900, loss 0.636547, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-4900\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:37:27.078300: step 4901, loss 0.00117404, acc 1\n",
      "2017-09-26T14:37:27.501399: step 4902, loss 0.0475556, acc 0.984375\n",
      "2017-09-26T14:37:27.821933: step 4903, loss 0.000620686, acc 1\n",
      "2017-09-26T14:37:28.238900: step 4904, loss 0.000789169, acc 1\n",
      "2017-09-26T14:37:28.668348: step 4905, loss 0.00183884, acc 1\n",
      "2017-09-26T14:37:29.068948: step 4906, loss 0.00117739, acc 1\n",
      "2017-09-26T14:37:29.416241: step 4907, loss 0.000221644, acc 1\n",
      "2017-09-26T14:37:29.692700: step 4908, loss 0.00086777, acc 1\n",
      "2017-09-26T14:37:30.068984: step 4909, loss 0.000524018, acc 1\n",
      "2017-09-26T14:37:30.327887: step 4910, loss 0.0733394, acc 0.984375\n",
      "2017-09-26T14:37:30.624938: step 4911, loss 0.000706631, acc 1\n",
      "2017-09-26T14:37:30.930994: step 4912, loss 0.000599361, acc 1\n",
      "2017-09-26T14:37:31.258170: step 4913, loss 0.00515198, acc 1\n",
      "2017-09-26T14:37:31.587428: step 4914, loss 0.00168244, acc 1\n",
      "2017-09-26T14:37:31.947860: step 4915, loss 0.000436867, acc 1\n",
      "2017-09-26T14:37:32.315337: step 4916, loss 0.000115695, acc 1\n",
      "2017-09-26T14:37:32.698141: step 4917, loss 0.000530843, acc 1\n",
      "2017-09-26T14:37:33.097513: step 4918, loss 0.000355166, acc 1\n",
      "2017-09-26T14:37:33.475784: step 4919, loss 0.000267457, acc 1\n",
      "2017-09-26T14:37:33.842810: step 4920, loss 0.00019291, acc 1\n",
      "2017-09-26T14:37:34.142788: step 4921, loss 0.000270931, acc 1\n",
      "2017-09-26T14:37:34.443559: step 4922, loss 0.00495517, acc 1\n",
      "2017-09-26T14:37:34.715835: step 4923, loss 0.000646964, acc 1\n",
      "2017-09-26T14:37:34.976735: step 4924, loss 0.000185201, acc 1\n",
      "2017-09-26T14:37:35.237510: step 4925, loss 0.00154627, acc 1\n",
      "2017-09-26T14:37:35.502308: step 4926, loss 0.000187936, acc 1\n",
      "2017-09-26T14:37:35.756420: step 4927, loss 0.000547782, acc 1\n",
      "2017-09-26T14:37:36.004483: step 4928, loss 0.00456581, acc 1\n",
      "2017-09-26T14:37:36.244032: step 4929, loss 0.000379626, acc 1\n",
      "2017-09-26T14:37:36.482783: step 4930, loss 0.00310461, acc 1\n",
      "2017-09-26T14:37:36.715804: step 4931, loss 0.00147328, acc 1\n",
      "2017-09-26T14:37:36.996701: step 4932, loss 0.00119092, acc 1\n",
      "2017-09-26T14:37:37.259909: step 4933, loss 0.000345777, acc 1\n",
      "2017-09-26T14:37:37.525087: step 4934, loss 0.000123399, acc 1\n",
      "2017-09-26T14:37:37.792042: step 4935, loss 0.00154282, acc 1\n",
      "2017-09-26T14:37:38.066247: step 4936, loss 0.00134011, acc 1\n",
      "2017-09-26T14:37:38.397163: step 4937, loss 0.00174761, acc 1\n",
      "2017-09-26T14:37:38.726640: step 4938, loss 0.00243676, acc 1\n",
      "2017-09-26T14:37:39.036350: step 4939, loss 0.00417876, acc 1\n",
      "2017-09-26T14:37:39.367331: step 4940, loss 0.000667512, acc 1\n",
      "2017-09-26T14:37:39.679205: step 4941, loss 0.00331264, acc 1\n",
      "2017-09-26T14:37:39.928322: step 4942, loss 0.000404577, acc 1\n",
      "2017-09-26T14:37:40.187025: step 4943, loss 0.00128963, acc 1\n",
      "2017-09-26T14:37:40.419563: step 4944, loss 0.000649947, acc 1\n",
      "2017-09-26T14:37:40.705118: step 4945, loss 0.00315149, acc 1\n",
      "2017-09-26T14:37:41.022521: step 4946, loss 0.00396063, acc 1\n",
      "2017-09-26T14:37:41.339552: step 4947, loss 0.000440689, acc 1\n",
      "2017-09-26T14:37:41.643337: step 4948, loss 0.00105757, acc 1\n",
      "2017-09-26T14:37:41.892075: step 4949, loss 0.00152014, acc 1\n",
      "2017-09-26T14:37:42.216990: step 4950, loss 0.000206096, acc 1\n",
      "2017-09-26T14:37:42.528195: step 4951, loss 0.000530941, acc 1\n",
      "2017-09-26T14:37:42.799774: step 4952, loss 0.000444251, acc 1\n",
      "2017-09-26T14:37:43.129999: step 4953, loss 0.000123618, acc 1\n",
      "2017-09-26T14:37:43.377341: step 4954, loss 0.000268759, acc 1\n",
      "2017-09-26T14:37:43.694609: step 4955, loss 0.00182382, acc 1\n",
      "2017-09-26T14:37:43.955070: step 4956, loss 0.000769324, acc 1\n",
      "2017-09-26T14:37:44.258405: step 4957, loss 0.000335345, acc 1\n",
      "2017-09-26T14:37:44.566356: step 4958, loss 0.00157639, acc 1\n",
      "2017-09-26T14:37:44.873438: step 4959, loss 0.000693585, acc 1\n",
      "2017-09-26T14:37:45.174393: step 4960, loss 0.000358452, acc 1\n",
      "2017-09-26T14:37:45.470937: step 4961, loss 0.000159541, acc 1\n",
      "2017-09-26T14:37:45.792301: step 4962, loss 0.000668017, acc 1\n",
      "2017-09-26T14:37:46.135902: step 4963, loss 0.0001077, acc 1\n",
      "2017-09-26T14:37:46.448287: step 4964, loss 0.002425, acc 1\n",
      "2017-09-26T14:37:46.746725: step 4965, loss 8.54688e-05, acc 1\n",
      "2017-09-26T14:37:47.023541: step 4966, loss 0.00036903, acc 1\n",
      "2017-09-26T14:37:47.344477: step 4967, loss 0.000909233, acc 1\n",
      "2017-09-26T14:37:47.647495: step 4968, loss 0.000286416, acc 1\n",
      "2017-09-26T14:37:47.948399: step 4969, loss 0.000731428, acc 1\n",
      "2017-09-26T14:37:48.261260: step 4970, loss 0.00169615, acc 1\n",
      "2017-09-26T14:37:48.566085: step 4971, loss 0.000365257, acc 1\n",
      "2017-09-26T14:37:48.861821: step 4972, loss 0.00180933, acc 1\n",
      "2017-09-26T14:37:49.181264: step 4973, loss 0.000124059, acc 1\n",
      "2017-09-26T14:37:49.409471: step 4974, loss 0.000195551, acc 1\n",
      "2017-09-26T14:37:49.743910: step 4975, loss 0.000229194, acc 1\n",
      "2017-09-26T14:37:50.059513: step 4976, loss 0.00117301, acc 1\n",
      "2017-09-26T14:37:50.370931: step 4977, loss 0.000102264, acc 1\n",
      "2017-09-26T14:37:50.655276: step 4978, loss 0.000542957, acc 1\n",
      "2017-09-26T14:37:50.976539: step 4979, loss 0.000374714, acc 1\n",
      "2017-09-26T14:37:51.232182: step 4980, loss 0.00350678, acc 1\n",
      "2017-09-26T14:37:51.551289: step 4981, loss 0.00098362, acc 1\n",
      "2017-09-26T14:37:51.873872: step 4982, loss 0.000687559, acc 1\n",
      "2017-09-26T14:37:52.128051: step 4983, loss 0.000270218, acc 1\n",
      "2017-09-26T14:37:52.434314: step 4984, loss 0.000585188, acc 1\n",
      "2017-09-26T14:37:52.746262: step 4985, loss 0.000851453, acc 1\n",
      "2017-09-26T14:37:52.992165: step 4986, loss 0.000593645, acc 1\n",
      "2017-09-26T14:37:53.303722: step 4987, loss 0.0154035, acc 0.984375\n",
      "2017-09-26T14:37:53.595394: step 4988, loss 0.00217578, acc 1\n",
      "2017-09-26T14:37:53.890486: step 4989, loss 0.000222669, acc 1\n",
      "2017-09-26T14:37:54.203287: step 4990, loss 0.000251233, acc 1\n",
      "2017-09-26T14:37:54.524053: step 4991, loss 0.000761147, acc 1\n",
      "2017-09-26T14:37:54.869630: step 4992, loss 0.00015168, acc 1\n",
      "2017-09-26T14:37:55.134120: step 4993, loss 0.00254234, acc 1\n",
      "2017-09-26T14:37:55.451609: step 4994, loss 0.000138915, acc 1\n",
      "2017-09-26T14:37:55.714231: step 4995, loss 0.000597286, acc 1\n",
      "2017-09-26T14:37:56.029281: step 4996, loss 0.00492641, acc 1\n",
      "2017-09-26T14:37:56.344562: step 4997, loss 0.00999312, acc 1\n",
      "2017-09-26T14:37:56.581123: step 4998, loss 6.27079e-05, acc 1\n",
      "2017-09-26T14:37:56.923512: step 4999, loss 0.000568167, acc 1\n",
      "2017-09-26T14:37:57.252129: step 5000, loss 0.000588976, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:37:57.545129: step 5000, loss 0.621302, acc 0.878788\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5000\n",
      "\n",
      "2017-09-26T14:37:58.145867: step 5001, loss 0.000134662, acc 1\n",
      "2017-09-26T14:37:58.453727: step 5002, loss 0.000151003, acc 1\n",
      "2017-09-26T14:37:58.749401: step 5003, loss 0.000440215, acc 1\n",
      "2017-09-26T14:37:58.989614: step 5004, loss 0.000203859, acc 1\n",
      "2017-09-26T14:37:59.236332: step 5005, loss 0.00171295, acc 1\n",
      "2017-09-26T14:37:59.483943: step 5006, loss 0.00133706, acc 1\n",
      "2017-09-26T14:37:59.736405: step 5007, loss 0.00363102, acc 1\n",
      "2017-09-26T14:37:59.991943: step 5008, loss 0.000392844, acc 1\n",
      "2017-09-26T14:38:00.305130: step 5009, loss 0.00120886, acc 1\n",
      "2017-09-26T14:38:00.612949: step 5010, loss 0.000319026, acc 1\n",
      "2017-09-26T14:38:00.871928: step 5011, loss 0.000372315, acc 1\n",
      "2017-09-26T14:38:01.156071: step 5012, loss 0.00268173, acc 1\n",
      "2017-09-26T14:38:01.449594: step 5013, loss 0.000378733, acc 1\n",
      "2017-09-26T14:38:01.688516: step 5014, loss 0.000195161, acc 1\n",
      "2017-09-26T14:38:01.940977: step 5015, loss 0.000205449, acc 1\n",
      "2017-09-26T14:38:02.192986: step 5016, loss 0.000359325, acc 1\n",
      "2017-09-26T14:38:02.433241: step 5017, loss 0.00464071, acc 1\n",
      "2017-09-26T14:38:02.675824: step 5018, loss 0.0097694, acc 1\n",
      "2017-09-26T14:38:02.928543: step 5019, loss 0.000151271, acc 1\n",
      "2017-09-26T14:38:03.220276: step 5020, loss 0.0031834, acc 1\n",
      "2017-09-26T14:38:03.471564: step 5021, loss 0.000157473, acc 1\n",
      "2017-09-26T14:38:03.715740: step 5022, loss 0.000233375, acc 1\n",
      "2017-09-26T14:38:04.018907: step 5023, loss 0.000771571, acc 1\n",
      "2017-09-26T14:38:04.323246: step 5024, loss 0.000443855, acc 1\n",
      "2017-09-26T14:38:04.564381: step 5025, loss 0.00066082, acc 1\n",
      "2017-09-26T14:38:04.825023: step 5026, loss 0.00299424, acc 1\n",
      "2017-09-26T14:38:05.105077: step 5027, loss 0.00462924, acc 1\n",
      "2017-09-26T14:38:05.346791: step 5028, loss 0.00251271, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:38:05.591833: step 5029, loss 0.000269847, acc 1\n",
      "2017-09-26T14:38:05.835733: step 5030, loss 0.000321813, acc 1\n",
      "2017-09-26T14:38:06.067539: step 5031, loss 0.0130018, acc 0.984375\n",
      "2017-09-26T14:38:06.340646: step 5032, loss 0.000215115, acc 1\n",
      "2017-09-26T14:38:06.607389: step 5033, loss 0.000376711, acc 1\n",
      "2017-09-26T14:38:06.896913: step 5034, loss 0.00107499, acc 1\n",
      "2017-09-26T14:38:07.236629: step 5035, loss 0.000148157, acc 1\n",
      "2017-09-26T14:38:07.545962: step 5036, loss 0.0238786, acc 0.984375\n",
      "2017-09-26T14:38:07.827138: step 5037, loss 0.000286574, acc 1\n",
      "2017-09-26T14:38:08.189244: step 5038, loss 0.000227351, acc 1\n",
      "2017-09-26T14:38:08.520871: step 5039, loss 0.000460064, acc 1\n",
      "2017-09-26T14:38:08.757665: step 5040, loss 0.000480811, acc 1\n",
      "2017-09-26T14:38:08.989605: step 5041, loss 0.000202843, acc 1\n",
      "2017-09-26T14:38:09.224984: step 5042, loss 0.000171238, acc 1\n",
      "2017-09-26T14:38:09.465219: step 5043, loss 0.0908476, acc 0.984375\n",
      "2017-09-26T14:38:09.788805: step 5044, loss 0.000353413, acc 1\n",
      "2017-09-26T14:38:10.105765: step 5045, loss 0.000196472, acc 1\n",
      "2017-09-26T14:38:10.392496: step 5046, loss 0.00273913, acc 1\n",
      "2017-09-26T14:38:10.709682: step 5047, loss 0.00557698, acc 1\n",
      "2017-09-26T14:38:11.032576: step 5048, loss 6.10019e-05, acc 1\n",
      "2017-09-26T14:38:11.344706: step 5049, loss 0.000997769, acc 1\n",
      "2017-09-26T14:38:11.653724: step 5050, loss 0.000898693, acc 1\n",
      "2017-09-26T14:38:11.993512: step 5051, loss 0.000225776, acc 1\n",
      "2017-09-26T14:38:12.321919: step 5052, loss 0.000191877, acc 1\n",
      "2017-09-26T14:38:12.656066: step 5053, loss 0.000287303, acc 1\n",
      "2017-09-26T14:38:12.981896: step 5054, loss 0.00101277, acc 1\n",
      "2017-09-26T14:38:13.310695: step 5055, loss 0.000156399, acc 1\n",
      "2017-09-26T14:38:13.573784: step 5056, loss 0.000341341, acc 1\n",
      "2017-09-26T14:38:13.809261: step 5057, loss 0.00123335, acc 1\n",
      "2017-09-26T14:38:14.039097: step 5058, loss 0.000368095, acc 1\n",
      "2017-09-26T14:38:14.274126: step 5059, loss 0.000648922, acc 1\n",
      "2017-09-26T14:38:14.506040: step 5060, loss 0.00104577, acc 1\n",
      "2017-09-26T14:38:14.739203: step 5061, loss 0.000223804, acc 1\n",
      "2017-09-26T14:38:14.971795: step 5062, loss 0.000556063, acc 1\n",
      "2017-09-26T14:38:15.215070: step 5063, loss 0.000308038, acc 1\n",
      "2017-09-26T14:38:15.511260: step 5064, loss 0.000376693, acc 1\n",
      "2017-09-26T14:38:15.798916: step 5065, loss 0.0167048, acc 0.984375\n",
      "2017-09-26T14:38:16.042279: step 5066, loss 0.00150278, acc 1\n",
      "2017-09-26T14:38:16.312922: step 5067, loss 0.0193622, acc 0.984375\n",
      "2017-09-26T14:38:16.596839: step 5068, loss 0.000657098, acc 1\n",
      "2017-09-26T14:38:16.875172: step 5069, loss 0.000387842, acc 1\n",
      "2017-09-26T14:38:17.220107: step 5070, loss 0.00114612, acc 1\n",
      "2017-09-26T14:38:17.561147: step 5071, loss 0.00162682, acc 1\n",
      "2017-09-26T14:38:17.835061: step 5072, loss 0.000231635, acc 1\n",
      "2017-09-26T14:38:18.210530: step 5073, loss 0.00109644, acc 1\n",
      "2017-09-26T14:38:18.553191: step 5074, loss 0.000705008, acc 1\n",
      "2017-09-26T14:38:18.903523: step 5075, loss 0.000301667, acc 1\n",
      "2017-09-26T14:38:19.215004: step 5076, loss 0.000595757, acc 1\n",
      "2017-09-26T14:38:19.553015: step 5077, loss 0.0171399, acc 0.984375\n",
      "2017-09-26T14:38:19.884935: step 5078, loss 0.000116256, acc 1\n",
      "2017-09-26T14:38:20.154165: step 5079, loss 0.0011202, acc 1\n",
      "2017-09-26T14:38:20.488647: step 5080, loss 0.00123064, acc 1\n",
      "2017-09-26T14:38:20.825833: step 5081, loss 0.000361314, acc 1\n",
      "2017-09-26T14:38:21.126484: step 5082, loss 0.00117966, acc 1\n",
      "2017-09-26T14:38:21.393081: step 5083, loss 0.0042221, acc 1\n",
      "2017-09-26T14:38:21.714905: step 5084, loss 0.000209231, acc 1\n",
      "2017-09-26T14:38:22.030846: step 5085, loss 0.000442544, acc 1\n",
      "2017-09-26T14:38:22.308210: step 5086, loss 0.00010147, acc 1\n",
      "2017-09-26T14:38:22.630179: step 5087, loss 0.000462672, acc 1\n",
      "2017-09-26T14:38:22.939058: step 5088, loss 0.000273501, acc 1\n",
      "2017-09-26T14:38:23.223921: step 5089, loss 0.000265137, acc 1\n",
      "2017-09-26T14:38:23.548734: step 5090, loss 0.00626536, acc 1\n",
      "2017-09-26T14:38:23.865999: step 5091, loss 0.00263636, acc 1\n",
      "2017-09-26T14:38:24.190968: step 5092, loss 0.000121563, acc 1\n",
      "2017-09-26T14:38:24.458715: step 5093, loss 0.000778704, acc 1\n",
      "2017-09-26T14:38:24.698557: step 5094, loss 9.95449e-05, acc 1\n",
      "2017-09-26T14:38:24.930942: step 5095, loss 0.00196464, acc 1\n",
      "2017-09-26T14:38:25.166060: step 5096, loss 0.000482529, acc 1\n",
      "2017-09-26T14:38:25.413115: step 5097, loss 0.00287049, acc 1\n",
      "2017-09-26T14:38:25.651447: step 5098, loss 0.000132875, acc 1\n",
      "2017-09-26T14:38:25.885417: step 5099, loss 0.000170468, acc 1\n",
      "2017-09-26T14:38:26.131944: step 5100, loss 0.00042291, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:38:26.384027: step 5100, loss 0.601538, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5100\n",
      "\n",
      "2017-09-26T14:38:27.081234: step 5101, loss 0.0101044, acc 1\n",
      "2017-09-26T14:38:27.415618: step 5102, loss 6.0976e-05, acc 1\n",
      "2017-09-26T14:38:27.767487: step 5103, loss 0.000865388, acc 1\n",
      "2017-09-26T14:38:28.085901: step 5104, loss 0.000235965, acc 1\n",
      "2017-09-26T14:38:28.398389: step 5105, loss 0.000675533, acc 1\n",
      "2017-09-26T14:38:28.719915: step 5106, loss 0.00085344, acc 1\n",
      "2017-09-26T14:38:29.008456: step 5107, loss 0.000443892, acc 1\n",
      "2017-09-26T14:38:29.250411: step 5108, loss 0.000489123, acc 1\n",
      "2017-09-26T14:38:29.495176: step 5109, loss 0.000753901, acc 1\n",
      "2017-09-26T14:38:29.732535: step 5110, loss 0.000174311, acc 1\n",
      "2017-09-26T14:38:29.970931: step 5111, loss 0.00100706, acc 1\n",
      "2017-09-26T14:38:30.211631: step 5112, loss 0.0433641, acc 0.96875\n",
      "2017-09-26T14:38:30.447626: step 5113, loss 0.00301449, acc 1\n",
      "2017-09-26T14:38:30.686136: step 5114, loss 0.00039706, acc 1\n",
      "2017-09-26T14:38:30.928800: step 5115, loss 0.000100132, acc 1\n",
      "2017-09-26T14:38:31.165992: step 5116, loss 0.000414192, acc 1\n",
      "2017-09-26T14:38:31.403715: step 5117, loss 0.035943, acc 0.984375\n",
      "2017-09-26T14:38:31.642956: step 5118, loss 0.000809431, acc 1\n",
      "2017-09-26T14:38:31.899327: step 5119, loss 0.000418782, acc 1\n",
      "2017-09-26T14:38:32.148623: step 5120, loss 0.000946746, acc 1\n",
      "2017-09-26T14:38:32.394649: step 5121, loss 0.000449215, acc 1\n",
      "2017-09-26T14:38:32.630884: step 5122, loss 0.000667007, acc 1\n",
      "2017-09-26T14:38:32.887381: step 5123, loss 0.000975267, acc 1\n",
      "2017-09-26T14:38:33.106078: step 5124, loss 0.000757508, acc 1\n",
      "2017-09-26T14:38:33.351696: step 5125, loss 0.0246999, acc 0.984375\n",
      "2017-09-26T14:38:33.599410: step 5126, loss 0.00257806, acc 1\n",
      "2017-09-26T14:38:33.851425: step 5127, loss 0.00128655, acc 1\n",
      "2017-09-26T14:38:34.093595: step 5128, loss 0.00569855, acc 1\n",
      "2017-09-26T14:38:34.331221: step 5129, loss 0.00211666, acc 1\n",
      "2017-09-26T14:38:34.567452: step 5130, loss 0.000377065, acc 1\n",
      "2017-09-26T14:38:34.808927: step 5131, loss 0.00437081, acc 1\n",
      "2017-09-26T14:38:35.045789: step 5132, loss 0.000174399, acc 1\n",
      "2017-09-26T14:38:35.303723: step 5133, loss 0.0535225, acc 0.96875\n",
      "2017-09-26T14:38:35.554859: step 5134, loss 0.00090427, acc 1\n",
      "2017-09-26T14:38:35.807350: step 5135, loss 0.0178337, acc 0.984375\n",
      "2017-09-26T14:38:36.045444: step 5136, loss 0.000253008, acc 1\n",
      "2017-09-26T14:38:36.280704: step 5137, loss 0.000638724, acc 1\n",
      "2017-09-26T14:38:36.522630: step 5138, loss 0.000840949, acc 1\n",
      "2017-09-26T14:38:36.755886: step 5139, loss 0.00129252, acc 1\n",
      "2017-09-26T14:38:36.996924: step 5140, loss 0.00172825, acc 1\n",
      "2017-09-26T14:38:37.234646: step 5141, loss 0.00582679, acc 1\n",
      "2017-09-26T14:38:37.472353: step 5142, loss 0.00318256, acc 1\n",
      "2017-09-26T14:38:37.710684: step 5143, loss 0.000420769, acc 1\n",
      "2017-09-26T14:38:37.955273: step 5144, loss 0.00144169, acc 1\n",
      "2017-09-26T14:38:38.206555: step 5145, loss 0.000451126, acc 1\n",
      "2017-09-26T14:38:38.441764: step 5146, loss 0.000390481, acc 1\n",
      "2017-09-26T14:38:38.678911: step 5147, loss 0.000129542, acc 1\n",
      "2017-09-26T14:38:38.932163: step 5148, loss 0.000179764, acc 1\n",
      "2017-09-26T14:38:39.209453: step 5149, loss 0.00116952, acc 1\n",
      "2017-09-26T14:38:39.488791: step 5150, loss 0.000491482, acc 1\n",
      "2017-09-26T14:38:39.795940: step 5151, loss 0.000969665, acc 1\n",
      "2017-09-26T14:38:40.085600: step 5152, loss 0.00125206, acc 1\n",
      "2017-09-26T14:38:40.348434: step 5153, loss 0.000265915, acc 1\n",
      "2017-09-26T14:38:40.619661: step 5154, loss 0.000214438, acc 1\n",
      "2017-09-26T14:38:40.896116: step 5155, loss 0.000611528, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:38:41.199524: step 5156, loss 0.00954455, acc 1\n",
      "2017-09-26T14:38:41.487747: step 5157, loss 0.0270721, acc 0.984375\n",
      "2017-09-26T14:38:41.803011: step 5158, loss 0.00126926, acc 1\n",
      "2017-09-26T14:38:42.070679: step 5159, loss 0.02804, acc 0.984375\n",
      "2017-09-26T14:38:42.299950: step 5160, loss 0.00448861, acc 1\n",
      "2017-09-26T14:38:42.564970: step 5161, loss 0.00216461, acc 1\n",
      "2017-09-26T14:38:42.878044: step 5162, loss 0.00099916, acc 1\n",
      "2017-09-26T14:38:43.172641: step 5163, loss 0.0311569, acc 0.984375\n",
      "2017-09-26T14:38:43.477231: step 5164, loss 0.000434499, acc 1\n",
      "2017-09-26T14:38:43.714795: step 5165, loss 0.000934774, acc 1\n",
      "2017-09-26T14:38:43.919560: step 5166, loss 0.000576904, acc 1\n",
      "2017-09-26T14:38:44.174631: step 5167, loss 0.000146691, acc 1\n",
      "2017-09-26T14:38:44.406151: step 5168, loss 0.000192359, acc 1\n",
      "2017-09-26T14:38:44.636928: step 5169, loss 0.00362417, acc 1\n",
      "2017-09-26T14:38:44.872370: step 5170, loss 0.000689292, acc 1\n",
      "2017-09-26T14:38:45.105604: step 5171, loss 0.000210996, acc 1\n",
      "2017-09-26T14:38:45.358162: step 5172, loss 0.00160061, acc 1\n",
      "2017-09-26T14:38:45.659231: step 5173, loss 0.013701, acc 0.984375\n",
      "2017-09-26T14:38:45.953997: step 5174, loss 0.00114761, acc 1\n",
      "2017-09-26T14:38:46.184777: step 5175, loss 0.00020496, acc 1\n",
      "2017-09-26T14:38:46.433925: step 5176, loss 0.000189076, acc 1\n",
      "2017-09-26T14:38:46.781844: step 5177, loss 0.00131492, acc 1\n",
      "2017-09-26T14:38:47.082743: step 5178, loss 0.000988962, acc 1\n",
      "2017-09-26T14:38:47.345387: step 5179, loss 0.000275919, acc 1\n",
      "2017-09-26T14:38:47.668182: step 5180, loss 0.000210045, acc 1\n",
      "2017-09-26T14:38:47.946646: step 5181, loss 0.00533324, acc 1\n",
      "2017-09-26T14:38:48.176148: step 5182, loss 0.00331187, acc 1\n",
      "2017-09-26T14:38:48.408556: step 5183, loss 0.00259707, acc 1\n",
      "2017-09-26T14:38:48.643539: step 5184, loss 0.00101766, acc 1\n",
      "2017-09-26T14:38:48.878654: step 5185, loss 0.00268281, acc 1\n",
      "2017-09-26T14:38:49.162158: step 5186, loss 0.000262227, acc 1\n",
      "2017-09-26T14:38:49.439210: step 5187, loss 0.000545289, acc 1\n",
      "2017-09-26T14:38:49.721263: step 5188, loss 0.00392253, acc 1\n",
      "2017-09-26T14:38:50.006094: step 5189, loss 0.0168936, acc 0.984375\n",
      "2017-09-26T14:38:50.279434: step 5190, loss 0.000307133, acc 1\n",
      "2017-09-26T14:38:50.570474: step 5191, loss 0.000468272, acc 1\n",
      "2017-09-26T14:38:50.850185: step 5192, loss 0.000323192, acc 1\n",
      "2017-09-26T14:38:51.152043: step 5193, loss 0.00189878, acc 1\n",
      "2017-09-26T14:38:51.438532: step 5194, loss 0.000380904, acc 1\n",
      "2017-09-26T14:38:51.717745: step 5195, loss 0.00122904, acc 1\n",
      "2017-09-26T14:38:52.009733: step 5196, loss 0.0021069, acc 1\n",
      "2017-09-26T14:38:52.254851: step 5197, loss 0.00128297, acc 1\n",
      "2017-09-26T14:38:52.539932: step 5198, loss 0.0001222, acc 1\n",
      "2017-09-26T14:38:52.822271: step 5199, loss 0.000615258, acc 1\n",
      "2017-09-26T14:38:53.088642: step 5200, loss 0.00254551, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:38:53.368105: step 5200, loss 0.617929, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5200\n",
      "\n",
      "2017-09-26T14:38:53.934027: step 5201, loss 0.000289294, acc 1\n",
      "2017-09-26T14:38:54.232021: step 5202, loss 0.000726098, acc 1\n",
      "2017-09-26T14:38:54.510173: step 5203, loss 0.000260067, acc 1\n",
      "2017-09-26T14:38:54.781021: step 5204, loss 0.00191197, acc 1\n",
      "2017-09-26T14:38:55.051698: step 5205, loss 0.00191769, acc 1\n",
      "2017-09-26T14:38:55.338479: step 5206, loss 0.000277962, acc 1\n",
      "2017-09-26T14:38:55.631133: step 5207, loss 0.000759608, acc 1\n",
      "2017-09-26T14:38:55.829800: step 5208, loss 0.00154048, acc 1\n",
      "2017-09-26T14:38:56.107585: step 5209, loss 0.000802453, acc 1\n",
      "2017-09-26T14:38:56.397268: step 5210, loss 0.000758949, acc 1\n",
      "2017-09-26T14:38:56.676305: step 5211, loss 0.000422984, acc 1\n",
      "2017-09-26T14:38:56.923389: step 5212, loss 0.000215962, acc 1\n",
      "2017-09-26T14:38:57.186040: step 5213, loss 0.000485202, acc 1\n",
      "2017-09-26T14:38:57.465310: step 5214, loss 0.000522404, acc 1\n",
      "2017-09-26T14:38:57.743925: step 5215, loss 0.000364383, acc 1\n",
      "2017-09-26T14:38:58.018556: step 5216, loss 0.00116585, acc 1\n",
      "2017-09-26T14:38:58.252097: step 5217, loss 0.000420768, acc 1\n",
      "2017-09-26T14:38:58.486365: step 5218, loss 0.0134549, acc 0.984375\n",
      "2017-09-26T14:38:58.723998: step 5219, loss 0.00047502, acc 1\n",
      "2017-09-26T14:38:58.962941: step 5220, loss 0.000165447, acc 1\n",
      "2017-09-26T14:38:59.207736: step 5221, loss 0.000876316, acc 1\n",
      "2017-09-26T14:38:59.438637: step 5222, loss 0.000622165, acc 1\n",
      "2017-09-26T14:38:59.677193: step 5223, loss 0.00131155, acc 1\n",
      "2017-09-26T14:38:59.908771: step 5224, loss 0.00141982, acc 1\n",
      "2017-09-26T14:39:00.164675: step 5225, loss 0.000168434, acc 1\n",
      "2017-09-26T14:39:00.419077: step 5226, loss 0.00013171, acc 1\n",
      "2017-09-26T14:39:00.672620: step 5227, loss 0.00103734, acc 1\n",
      "2017-09-26T14:39:00.928518: step 5228, loss 0.000354655, acc 1\n",
      "2017-09-26T14:39:01.185113: step 5229, loss 0.00521602, acc 1\n",
      "2017-09-26T14:39:01.414456: step 5230, loss 0.000186969, acc 1\n",
      "2017-09-26T14:39:01.651128: step 5231, loss 0.0158208, acc 0.984375\n",
      "2017-09-26T14:39:01.886472: step 5232, loss 0.00184931, acc 1\n",
      "2017-09-26T14:39:02.126016: step 5233, loss 0.0246385, acc 0.984375\n",
      "2017-09-26T14:39:02.358985: step 5234, loss 0.00128019, acc 1\n",
      "2017-09-26T14:39:02.651975: step 5235, loss 0.000244991, acc 1\n",
      "2017-09-26T14:39:02.910528: step 5236, loss 0.000179389, acc 1\n",
      "2017-09-26T14:39:03.187967: step 5237, loss 0.00273464, acc 1\n",
      "2017-09-26T14:39:03.481903: step 5238, loss 0.0723305, acc 0.96875\n",
      "2017-09-26T14:39:03.753506: step 5239, loss 0.00620276, acc 1\n",
      "2017-09-26T14:39:04.034561: step 5240, loss 0.00018397, acc 1\n",
      "2017-09-26T14:39:04.311288: step 5241, loss 0.00150468, acc 1\n",
      "2017-09-26T14:39:04.567884: step 5242, loss 0.000795377, acc 1\n",
      "2017-09-26T14:39:04.839850: step 5243, loss 0.00494922, acc 1\n",
      "2017-09-26T14:39:05.146018: step 5244, loss 0.00169371, acc 1\n",
      "2017-09-26T14:39:05.427863: step 5245, loss 0.000456755, acc 1\n",
      "2017-09-26T14:39:05.721637: step 5246, loss 0.000555836, acc 1\n",
      "2017-09-26T14:39:05.967208: step 5247, loss 0.0182102, acc 0.984375\n",
      "2017-09-26T14:39:06.202838: step 5248, loss 0.00112064, acc 1\n",
      "2017-09-26T14:39:06.466591: step 5249, loss 7.71505e-05, acc 1\n",
      "2017-09-26T14:39:06.753828: step 5250, loss 0.000384303, acc 1\n",
      "2017-09-26T14:39:07.047524: step 5251, loss 0.000199894, acc 1\n",
      "2017-09-26T14:39:07.288302: step 5252, loss 0.0101301, acc 1\n",
      "2017-09-26T14:39:07.525909: step 5253, loss 0.000701158, acc 1\n",
      "2017-09-26T14:39:07.763395: step 5254, loss 0.0320763, acc 0.984375\n",
      "2017-09-26T14:39:08.000641: step 5255, loss 0.000408066, acc 1\n",
      "2017-09-26T14:39:08.248394: step 5256, loss 0.00530756, acc 1\n",
      "2017-09-26T14:39:08.498903: step 5257, loss 0.00060829, acc 1\n",
      "2017-09-26T14:39:08.752333: step 5258, loss 0.00107237, acc 1\n",
      "2017-09-26T14:39:08.987686: step 5259, loss 0.000335273, acc 1\n",
      "2017-09-26T14:39:09.224779: step 5260, loss 0.00642136, acc 1\n",
      "2017-09-26T14:39:09.493066: step 5261, loss 0.000169809, acc 1\n",
      "2017-09-26T14:39:09.726318: step 5262, loss 0.00019159, acc 1\n",
      "2017-09-26T14:39:09.966698: step 5263, loss 0.00163926, acc 1\n",
      "2017-09-26T14:39:10.224699: step 5264, loss 0.0821068, acc 0.984375\n",
      "2017-09-26T14:39:10.466631: step 5265, loss 0.000120351, acc 1\n",
      "2017-09-26T14:39:10.704953: step 5266, loss 0.0022301, acc 1\n",
      "2017-09-26T14:39:10.942740: step 5267, loss 0.00691954, acc 1\n",
      "2017-09-26T14:39:11.197862: step 5268, loss 0.00127082, acc 1\n",
      "2017-09-26T14:39:11.433069: step 5269, loss 0.000188418, acc 1\n",
      "2017-09-26T14:39:11.670076: step 5270, loss 0.00137268, acc 1\n",
      "2017-09-26T14:39:11.907278: step 5271, loss 0.0277906, acc 0.984375\n",
      "2017-09-26T14:39:12.146982: step 5272, loss 0.000874598, acc 1\n",
      "2017-09-26T14:39:12.389009: step 5273, loss 0.000352754, acc 1\n",
      "2017-09-26T14:39:12.627263: step 5274, loss 0.00916207, acc 1\n",
      "2017-09-26T14:39:12.868970: step 5275, loss 0.000225941, acc 1\n",
      "2017-09-26T14:39:13.111081: step 5276, loss 0.000452608, acc 1\n",
      "2017-09-26T14:39:13.347402: step 5277, loss 0.000696556, acc 1\n",
      "2017-09-26T14:39:13.588402: step 5278, loss 0.00216482, acc 1\n",
      "2017-09-26T14:39:13.826638: step 5279, loss 0.0194499, acc 0.984375\n",
      "2017-09-26T14:39:14.064650: step 5280, loss 0.00367409, acc 1\n",
      "2017-09-26T14:39:14.307901: step 5281, loss 0.00140368, acc 1\n",
      "2017-09-26T14:39:14.544055: step 5282, loss 0.0116552, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:39:14.790442: step 5283, loss 0.00539583, acc 1\n",
      "2017-09-26T14:39:15.027528: step 5284, loss 0.000428681, acc 1\n",
      "2017-09-26T14:39:15.258756: step 5285, loss 0.0003675, acc 1\n",
      "2017-09-26T14:39:15.498181: step 5286, loss 0.000220519, acc 1\n",
      "2017-09-26T14:39:15.727166: step 5287, loss 0.0501703, acc 0.984375\n",
      "2017-09-26T14:39:15.964530: step 5288, loss 0.00066232, acc 1\n",
      "2017-09-26T14:39:16.196442: step 5289, loss 0.000569731, acc 1\n",
      "2017-09-26T14:39:16.428972: step 5290, loss 0.00622043, acc 1\n",
      "2017-09-26T14:39:16.659502: step 5291, loss 0.000958726, acc 1\n",
      "2017-09-26T14:39:16.863556: step 5292, loss 0.000796377, acc 1\n",
      "2017-09-26T14:39:17.106331: step 5293, loss 0.00195545, acc 1\n",
      "2017-09-26T14:39:17.338254: step 5294, loss 0.00197195, acc 1\n",
      "2017-09-26T14:39:17.570100: step 5295, loss 0.00113048, acc 1\n",
      "2017-09-26T14:39:17.797989: step 5296, loss 0.00225727, acc 1\n",
      "2017-09-26T14:39:18.030325: step 5297, loss 0.0165606, acc 0.984375\n",
      "2017-09-26T14:39:18.279686: step 5298, loss 0.0044044, acc 1\n",
      "2017-09-26T14:39:18.508102: step 5299, loss 0.00756516, acc 1\n",
      "2017-09-26T14:39:18.736815: step 5300, loss 0.00643613, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:39:18.967322: step 5300, loss 0.58157, acc 0.872054\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5300\n",
      "\n",
      "2017-09-26T14:39:19.456440: step 5301, loss 0.000243047, acc 1\n",
      "2017-09-26T14:39:19.688204: step 5302, loss 0.000296918, acc 1\n",
      "2017-09-26T14:39:19.922192: step 5303, loss 0.000159115, acc 1\n",
      "2017-09-26T14:39:20.154268: step 5304, loss 0.00861761, acc 1\n",
      "2017-09-26T14:39:20.387757: step 5305, loss 0.000830021, acc 1\n",
      "2017-09-26T14:39:20.622849: step 5306, loss 0.000969538, acc 1\n",
      "2017-09-26T14:39:20.854939: step 5307, loss 0.000560763, acc 1\n",
      "2017-09-26T14:39:21.087051: step 5308, loss 0.000488459, acc 1\n",
      "2017-09-26T14:39:21.324564: step 5309, loss 0.00110245, acc 1\n",
      "2017-09-26T14:39:21.552514: step 5310, loss 0.000624666, acc 1\n",
      "2017-09-26T14:39:21.787510: step 5311, loss 0.000685485, acc 1\n",
      "2017-09-26T14:39:22.031135: step 5312, loss 0.00362628, acc 1\n",
      "2017-09-26T14:39:22.266087: step 5313, loss 0.00107626, acc 1\n",
      "2017-09-26T14:39:22.497334: step 5314, loss 0.00310088, acc 1\n",
      "2017-09-26T14:39:22.727976: step 5315, loss 0.0020252, acc 1\n",
      "2017-09-26T14:39:22.956520: step 5316, loss 0.000200651, acc 1\n",
      "2017-09-26T14:39:23.193972: step 5317, loss 0.0043161, acc 1\n",
      "2017-09-26T14:39:23.426900: step 5318, loss 0.00289823, acc 1\n",
      "2017-09-26T14:39:23.669437: step 5319, loss 0.0309191, acc 0.984375\n",
      "2017-09-26T14:39:23.905802: step 5320, loss 0.00118529, acc 1\n",
      "2017-09-26T14:39:24.170713: step 5321, loss 0.00496446, acc 1\n",
      "2017-09-26T14:39:24.426052: step 5322, loss 0.000152219, acc 1\n",
      "2017-09-26T14:39:24.676935: step 5323, loss 0.00149011, acc 1\n",
      "2017-09-26T14:39:24.950005: step 5324, loss 0.000412866, acc 1\n",
      "2017-09-26T14:39:25.201383: step 5325, loss 0.000943056, acc 1\n",
      "2017-09-26T14:39:25.457100: step 5326, loss 0.00024182, acc 1\n",
      "2017-09-26T14:39:25.707097: step 5327, loss 0.000443204, acc 1\n",
      "2017-09-26T14:39:25.976420: step 5328, loss 0.00178205, acc 1\n",
      "2017-09-26T14:39:26.238764: step 5329, loss 0.00192262, acc 1\n",
      "2017-09-26T14:39:26.520656: step 5330, loss 0.00211296, acc 1\n",
      "2017-09-26T14:39:26.800610: step 5331, loss 0.00199564, acc 1\n",
      "2017-09-26T14:39:27.063650: step 5332, loss 0.000226132, acc 1\n",
      "2017-09-26T14:39:27.330156: step 5333, loss 0.00064488, acc 1\n",
      "2017-09-26T14:39:27.558843: step 5334, loss 0.001543, acc 1\n",
      "2017-09-26T14:39:27.841314: step 5335, loss 0.00202559, acc 1\n",
      "2017-09-26T14:39:28.104636: step 5336, loss 0.00675613, acc 1\n",
      "2017-09-26T14:39:28.392497: step 5337, loss 0.00190344, acc 1\n",
      "2017-09-26T14:39:28.662563: step 5338, loss 0.00146758, acc 1\n",
      "2017-09-26T14:39:28.923673: step 5339, loss 0.000497454, acc 1\n",
      "2017-09-26T14:39:29.173278: step 5340, loss 0.000755635, acc 1\n",
      "2017-09-26T14:39:29.466271: step 5341, loss 0.000215666, acc 1\n",
      "2017-09-26T14:39:29.764010: step 5342, loss 0.000597662, acc 1\n",
      "2017-09-26T14:39:30.011157: step 5343, loss 0.000596385, acc 1\n",
      "2017-09-26T14:39:30.268167: step 5344, loss 0.000143451, acc 1\n",
      "2017-09-26T14:39:30.524579: step 5345, loss 0.00246924, acc 1\n",
      "2017-09-26T14:39:30.792486: step 5346, loss 0.00049004, acc 1\n",
      "2017-09-26T14:39:31.044490: step 5347, loss 0.000832031, acc 1\n",
      "2017-09-26T14:39:31.331428: step 5348, loss 0.000148348, acc 1\n",
      "2017-09-26T14:39:31.606524: step 5349, loss 0.000162875, acc 1\n",
      "2017-09-26T14:39:31.875862: step 5350, loss 0.00659605, acc 1\n",
      "2017-09-26T14:39:32.160053: step 5351, loss 0.000397473, acc 1\n",
      "2017-09-26T14:39:32.425944: step 5352, loss 0.000197054, acc 1\n",
      "2017-09-26T14:39:32.710404: step 5353, loss 0.000767466, acc 1\n",
      "2017-09-26T14:39:32.993484: step 5354, loss 0.000604424, acc 1\n",
      "2017-09-26T14:39:33.252994: step 5355, loss 0.000569819, acc 1\n",
      "2017-09-26T14:39:33.530096: step 5356, loss 0.000174479, acc 1\n",
      "2017-09-26T14:39:33.830668: step 5357, loss 0.00109788, acc 1\n",
      "2017-09-26T14:39:34.118484: step 5358, loss 0.000432033, acc 1\n",
      "2017-09-26T14:39:34.418819: step 5359, loss 0.000328157, acc 1\n",
      "2017-09-26T14:39:34.708587: step 5360, loss 0.00252237, acc 1\n",
      "2017-09-26T14:39:34.996142: step 5361, loss 0.000279539, acc 1\n",
      "2017-09-26T14:39:35.260723: step 5362, loss 0.000856133, acc 1\n",
      "2017-09-26T14:39:35.522671: step 5363, loss 0.00253609, acc 1\n",
      "2017-09-26T14:39:35.783676: step 5364, loss 0.00069536, acc 1\n",
      "2017-09-26T14:39:36.039434: step 5365, loss 0.00193499, acc 1\n",
      "2017-09-26T14:39:36.305220: step 5366, loss 0.000187616, acc 1\n",
      "2017-09-26T14:39:36.575254: step 5367, loss 0.000184433, acc 1\n",
      "2017-09-26T14:39:36.826923: step 5368, loss 0.00140953, acc 1\n",
      "2017-09-26T14:39:37.092826: step 5369, loss 0.00113168, acc 1\n",
      "2017-09-26T14:39:37.401647: step 5370, loss 0.000725951, acc 1\n",
      "2017-09-26T14:39:37.694164: step 5371, loss 0.000551424, acc 1\n",
      "2017-09-26T14:39:37.962378: step 5372, loss 0.000244799, acc 1\n",
      "2017-09-26T14:39:38.227665: step 5373, loss 0.0002768, acc 1\n",
      "2017-09-26T14:39:38.477137: step 5374, loss 0.000798352, acc 1\n",
      "2017-09-26T14:39:38.758118: step 5375, loss 8.80588e-05, acc 1\n",
      "2017-09-26T14:39:38.981987: step 5376, loss 0.000214075, acc 1\n",
      "2017-09-26T14:39:39.262699: step 5377, loss 0.000627401, acc 1\n",
      "2017-09-26T14:39:39.549989: step 5378, loss 0.0493761, acc 0.984375\n",
      "2017-09-26T14:39:39.801815: step 5379, loss 0.000193756, acc 1\n",
      "2017-09-26T14:39:40.055294: step 5380, loss 0.002372, acc 1\n",
      "2017-09-26T14:39:40.313521: step 5381, loss 0.00141959, acc 1\n",
      "2017-09-26T14:39:40.564878: step 5382, loss 0.000217373, acc 1\n",
      "2017-09-26T14:39:40.824746: step 5383, loss 0.000647257, acc 1\n",
      "2017-09-26T14:39:41.077860: step 5384, loss 0.000173049, acc 1\n",
      "2017-09-26T14:39:41.387559: step 5385, loss 0.000172007, acc 1\n",
      "2017-09-26T14:39:41.663484: step 5386, loss 0.000207763, acc 1\n",
      "2017-09-26T14:39:41.915004: step 5387, loss 0.000164852, acc 1\n",
      "2017-09-26T14:39:42.175925: step 5388, loss 0.00017069, acc 1\n",
      "2017-09-26T14:39:42.462495: step 5389, loss 0.000437888, acc 1\n",
      "2017-09-26T14:39:42.709628: step 5390, loss 0.00125666, acc 1\n",
      "2017-09-26T14:39:42.968059: step 5391, loss 0.0032525, acc 1\n",
      "2017-09-26T14:39:43.227054: step 5392, loss 0.000859314, acc 1\n",
      "2017-09-26T14:39:43.553994: step 5393, loss 0.000393165, acc 1\n",
      "2017-09-26T14:39:43.821139: step 5394, loss 0.000217933, acc 1\n",
      "2017-09-26T14:39:44.103608: step 5395, loss 0.000801772, acc 1\n",
      "2017-09-26T14:39:44.375748: step 5396, loss 0.00082795, acc 1\n",
      "2017-09-26T14:39:44.667505: step 5397, loss 0.000804404, acc 1\n",
      "2017-09-26T14:39:44.995886: step 5398, loss 0.000146273, acc 1\n",
      "2017-09-26T14:39:45.334274: step 5399, loss 0.000265883, acc 1\n",
      "2017-09-26T14:39:45.713309: step 5400, loss 0.00294854, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:39:46.062822: step 5400, loss 0.623495, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5400\n",
      "\n",
      "2017-09-26T14:39:46.669021: step 5401, loss 0.000541075, acc 1\n",
      "2017-09-26T14:39:46.996474: step 5402, loss 0.00065586, acc 1\n",
      "2017-09-26T14:39:47.325675: step 5403, loss 0.000322902, acc 1\n",
      "2017-09-26T14:39:47.646066: step 5404, loss 0.000314421, acc 1\n",
      "2017-09-26T14:39:47.921520: step 5405, loss 0.00117626, acc 1\n",
      "2017-09-26T14:39:48.221048: step 5406, loss 0.000879138, acc 1\n",
      "2017-09-26T14:39:48.537315: step 5407, loss 0.000693893, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:39:48.842623: step 5408, loss 0.000416499, acc 1\n",
      "2017-09-26T14:39:49.152366: step 5409, loss 0.0527167, acc 0.984375\n",
      "2017-09-26T14:39:49.446084: step 5410, loss 0.0013737, acc 1\n",
      "2017-09-26T14:39:49.693100: step 5411, loss 0.000486158, acc 1\n",
      "2017-09-26T14:39:49.935298: step 5412, loss 0.0354831, acc 0.984375\n",
      "2017-09-26T14:39:50.172806: step 5413, loss 0.000638076, acc 1\n",
      "2017-09-26T14:39:50.418097: step 5414, loss 0.000249113, acc 1\n",
      "2017-09-26T14:39:50.656858: step 5415, loss 0.00442772, acc 1\n",
      "2017-09-26T14:39:50.927061: step 5416, loss 0.000186683, acc 1\n",
      "2017-09-26T14:39:51.209286: step 5417, loss 0.000410589, acc 1\n",
      "2017-09-26T14:39:51.408339: step 5418, loss 0.000367973, acc 1\n",
      "2017-09-26T14:39:51.696778: step 5419, loss 0.00451714, acc 1\n",
      "2017-09-26T14:39:51.986200: step 5420, loss 0.0010235, acc 1\n",
      "2017-09-26T14:39:52.226695: step 5421, loss 0.00163568, acc 1\n",
      "2017-09-26T14:39:52.469015: step 5422, loss 0.0178279, acc 0.984375\n",
      "2017-09-26T14:39:52.712734: step 5423, loss 0.00321539, acc 1\n",
      "2017-09-26T14:39:52.966232: step 5424, loss 0.000282957, acc 1\n",
      "2017-09-26T14:39:53.198762: step 5425, loss 0.000782896, acc 1\n",
      "2017-09-26T14:39:53.516902: step 5426, loss 0.00135918, acc 1\n",
      "2017-09-26T14:39:53.844945: step 5427, loss 0.000474854, acc 1\n",
      "2017-09-26T14:39:54.088217: step 5428, loss 0.000208518, acc 1\n",
      "2017-09-26T14:39:54.437494: step 5429, loss 0.000594383, acc 1\n",
      "2017-09-26T14:39:54.742636: step 5430, loss 0.00374774, acc 1\n",
      "2017-09-26T14:39:55.042116: step 5431, loss 0.00210644, acc 1\n",
      "2017-09-26T14:39:55.422383: step 5432, loss 0.00366927, acc 1\n",
      "2017-09-26T14:39:55.729969: step 5433, loss 0.0019093, acc 1\n",
      "2017-09-26T14:39:56.033640: step 5434, loss 0.0112486, acc 1\n",
      "2017-09-26T14:39:56.351393: step 5435, loss 0.000451325, acc 1\n",
      "2017-09-26T14:39:56.608485: step 5436, loss 0.0012081, acc 1\n",
      "2017-09-26T14:39:56.873153: step 5437, loss 0.00120452, acc 1\n",
      "2017-09-26T14:39:57.105122: step 5438, loss 0.000773042, acc 1\n",
      "2017-09-26T14:39:57.361867: step 5439, loss 0.000402761, acc 1\n",
      "2017-09-26T14:39:57.592161: step 5440, loss 0.000903239, acc 1\n",
      "2017-09-26T14:39:57.835887: step 5441, loss 0.0013908, acc 1\n",
      "2017-09-26T14:39:58.078051: step 5442, loss 0.000659841, acc 1\n",
      "2017-09-26T14:39:58.325758: step 5443, loss 6.76487e-05, acc 1\n",
      "2017-09-26T14:39:58.611882: step 5444, loss 0.000937497, acc 1\n",
      "2017-09-26T14:39:58.961740: step 5445, loss 0.000684208, acc 1\n",
      "2017-09-26T14:39:59.308391: step 5446, loss 0.00112086, acc 1\n",
      "2017-09-26T14:39:59.701737: step 5447, loss 0.000735884, acc 1\n",
      "2017-09-26T14:40:00.083353: step 5448, loss 0.00103471, acc 1\n",
      "2017-09-26T14:40:00.474202: step 5449, loss 0.053857, acc 0.984375\n",
      "2017-09-26T14:40:00.828202: step 5450, loss 0.000623096, acc 1\n",
      "2017-09-26T14:40:01.091019: step 5451, loss 0.000199029, acc 1\n",
      "2017-09-26T14:40:01.326760: step 5452, loss 0.000171507, acc 1\n",
      "2017-09-26T14:40:01.568296: step 5453, loss 0.00041111, acc 1\n",
      "2017-09-26T14:40:01.848617: step 5454, loss 0.000539833, acc 1\n",
      "2017-09-26T14:40:02.163598: step 5455, loss 0.000275325, acc 1\n",
      "2017-09-26T14:40:02.485381: step 5456, loss 0.00110054, acc 1\n",
      "2017-09-26T14:40:02.809043: step 5457, loss 0.000210564, acc 1\n",
      "2017-09-26T14:40:03.124002: step 5458, loss 0.000437075, acc 1\n",
      "2017-09-26T14:40:03.465880: step 5459, loss 0.000294613, acc 1\n",
      "2017-09-26T14:40:03.740247: step 5460, loss 0.0365616, acc 0.980769\n",
      "2017-09-26T14:40:04.081481: step 5461, loss 0.00111056, acc 1\n",
      "2017-09-26T14:40:04.410178: step 5462, loss 0.000430691, acc 1\n",
      "2017-09-26T14:40:04.734052: step 5463, loss 0.000107073, acc 1\n",
      "2017-09-26T14:40:05.065518: step 5464, loss 0.00654474, acc 1\n",
      "2017-09-26T14:40:05.358530: step 5465, loss 0.0176632, acc 0.984375\n",
      "2017-09-26T14:40:05.682382: step 5466, loss 0.00136739, acc 1\n",
      "2017-09-26T14:40:05.990177: step 5467, loss 0.0276937, acc 0.984375\n",
      "2017-09-26T14:40:06.292997: step 5468, loss 0.000156893, acc 1\n",
      "2017-09-26T14:40:06.605134: step 5469, loss 0.00328394, acc 1\n",
      "2017-09-26T14:40:06.856289: step 5470, loss 0.000181146, acc 1\n",
      "2017-09-26T14:40:07.134888: step 5471, loss 0.000107912, acc 1\n",
      "2017-09-26T14:40:07.448462: step 5472, loss 0.000677528, acc 1\n",
      "2017-09-26T14:40:07.766291: step 5473, loss 0.000403874, acc 1\n",
      "2017-09-26T14:40:08.015287: step 5474, loss 0.000352009, acc 1\n",
      "2017-09-26T14:40:08.262103: step 5475, loss 0.00392069, acc 1\n",
      "2017-09-26T14:40:08.503146: step 5476, loss 0.000456356, acc 1\n",
      "2017-09-26T14:40:08.771826: step 5477, loss 0.0757544, acc 0.984375\n",
      "2017-09-26T14:40:09.090361: step 5478, loss 0.00598214, acc 1\n",
      "2017-09-26T14:40:09.401389: step 5479, loss 0.000629947, acc 1\n",
      "2017-09-26T14:40:09.710151: step 5480, loss 0.000107696, acc 1\n",
      "2017-09-26T14:40:10.026454: step 5481, loss 0.00113814, acc 1\n",
      "2017-09-26T14:40:10.346456: step 5482, loss 0.000235011, acc 1\n",
      "2017-09-26T14:40:10.595053: step 5483, loss 0.000403657, acc 1\n",
      "2017-09-26T14:40:10.856186: step 5484, loss 0.00143893, acc 1\n",
      "2017-09-26T14:40:11.171001: step 5485, loss 0.000341723, acc 1\n",
      "2017-09-26T14:40:11.478415: step 5486, loss 0.00103474, acc 1\n",
      "2017-09-26T14:40:11.710835: step 5487, loss 0.000346653, acc 1\n",
      "2017-09-26T14:40:11.951296: step 5488, loss 0.000428455, acc 1\n",
      "2017-09-26T14:40:12.189162: step 5489, loss 0.00232754, acc 1\n",
      "2017-09-26T14:40:12.484463: step 5490, loss 0.000887653, acc 1\n",
      "2017-09-26T14:40:12.814741: step 5491, loss 0.000467489, acc 1\n",
      "2017-09-26T14:40:13.098274: step 5492, loss 4.61128e-05, acc 1\n",
      "2017-09-26T14:40:13.410651: step 5493, loss 9.69332e-05, acc 1\n",
      "2017-09-26T14:40:13.688366: step 5494, loss 0.000201545, acc 1\n",
      "2017-09-26T14:40:13.953894: step 5495, loss 0.0168504, acc 0.984375\n",
      "2017-09-26T14:40:14.264863: step 5496, loss 0.00286078, acc 1\n",
      "2017-09-26T14:40:14.498523: step 5497, loss 0.000609034, acc 1\n",
      "2017-09-26T14:40:14.736399: step 5498, loss 0.0225537, acc 0.984375\n",
      "2017-09-26T14:40:14.974402: step 5499, loss 0.000947485, acc 1\n",
      "2017-09-26T14:40:15.294637: step 5500, loss 0.000898511, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:40:15.588541: step 5500, loss 0.680925, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5500\n",
      "\n",
      "2017-09-26T14:40:16.305799: step 5501, loss 0.000270018, acc 1\n",
      "2017-09-26T14:40:16.583577: step 5502, loss 0.00034992, acc 1\n",
      "2017-09-26T14:40:16.894525: step 5503, loss 4.62867e-05, acc 1\n",
      "2017-09-26T14:40:17.164932: step 5504, loss 0.00367474, acc 1\n",
      "2017-09-26T14:40:17.490197: step 5505, loss 0.000920866, acc 1\n",
      "2017-09-26T14:40:17.822490: step 5506, loss 0.000100073, acc 1\n",
      "2017-09-26T14:40:18.158127: step 5507, loss 9.54047e-05, acc 1\n",
      "2017-09-26T14:40:18.491577: step 5508, loss 0.00760582, acc 1\n",
      "2017-09-26T14:40:18.830485: step 5509, loss 0.00983116, acc 1\n",
      "2017-09-26T14:40:19.142440: step 5510, loss 0.00114854, acc 1\n",
      "2017-09-26T14:40:19.463453: step 5511, loss 0.000393526, acc 1\n",
      "2017-09-26T14:40:19.768764: step 5512, loss 0.000633957, acc 1\n",
      "2017-09-26T14:40:20.079795: step 5513, loss 0.000430169, acc 1\n",
      "2017-09-26T14:40:20.390085: step 5514, loss 0.000438488, acc 1\n",
      "2017-09-26T14:40:20.696188: step 5515, loss 0.000225191, acc 1\n",
      "2017-09-26T14:40:21.023357: step 5516, loss 0.000684301, acc 1\n",
      "2017-09-26T14:40:21.254520: step 5517, loss 4.15839e-05, acc 1\n",
      "2017-09-26T14:40:21.502238: step 5518, loss 0.000571671, acc 1\n",
      "2017-09-26T14:40:21.737751: step 5519, loss 0.000684595, acc 1\n",
      "2017-09-26T14:40:21.977600: step 5520, loss 8.09002e-05, acc 1\n",
      "2017-09-26T14:40:22.223859: step 5521, loss 0.0118233, acc 1\n",
      "2017-09-26T14:40:22.459329: step 5522, loss 0.000288705, acc 1\n",
      "2017-09-26T14:40:22.699237: step 5523, loss 0.000315901, acc 1\n",
      "2017-09-26T14:40:22.944458: step 5524, loss 0.000131279, acc 1\n",
      "2017-09-26T14:40:23.188029: step 5525, loss 0.000957851, acc 1\n",
      "2017-09-26T14:40:23.425470: step 5526, loss 6.6091e-05, acc 1\n",
      "2017-09-26T14:40:23.672639: step 5527, loss 0.00217144, acc 1\n",
      "2017-09-26T14:40:23.957960: step 5528, loss 0.000180755, acc 1\n",
      "2017-09-26T14:40:24.248649: step 5529, loss 0.000435363, acc 1\n",
      "2017-09-26T14:40:24.594548: step 5530, loss 0.00523403, acc 1\n",
      "2017-09-26T14:40:24.921952: step 5531, loss 4.96929e-05, acc 1\n",
      "2017-09-26T14:40:25.248546: step 5532, loss 0.000775511, acc 1\n",
      "2017-09-26T14:40:25.572925: step 5533, loss 0.000425561, acc 1\n",
      "2017-09-26T14:40:25.829032: step 5534, loss 0.0106253, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:40:26.069497: step 5535, loss 0.00248969, acc 1\n",
      "2017-09-26T14:40:26.345809: step 5536, loss 0.00569777, acc 1\n",
      "2017-09-26T14:40:26.694868: step 5537, loss 0.00162151, acc 1\n",
      "2017-09-26T14:40:27.013666: step 5538, loss 0.000924503, acc 1\n",
      "2017-09-26T14:40:27.328108: step 5539, loss 0.000610113, acc 1\n",
      "2017-09-26T14:40:27.629151: step 5540, loss 0.0182707, acc 0.984375\n",
      "2017-09-26T14:40:27.934836: step 5541, loss 0.000638937, acc 1\n",
      "2017-09-26T14:40:28.239808: step 5542, loss 0.000661723, acc 1\n",
      "2017-09-26T14:40:28.487598: step 5543, loss 0.00998452, acc 1\n",
      "2017-09-26T14:40:28.692271: step 5544, loss 0.00018451, acc 1\n",
      "2017-09-26T14:40:29.079209: step 5545, loss 0.000126893, acc 1\n",
      "2017-09-26T14:40:29.434984: step 5546, loss 0.000155749, acc 1\n",
      "2017-09-26T14:40:29.760080: step 5547, loss 6.02108e-05, acc 1\n",
      "2017-09-26T14:40:30.084262: step 5548, loss 0.000818844, acc 1\n",
      "2017-09-26T14:40:30.425222: step 5549, loss 0.000796691, acc 1\n",
      "2017-09-26T14:40:30.690260: step 5550, loss 0.000162606, acc 1\n",
      "2017-09-26T14:40:30.940925: step 5551, loss 0.00117458, acc 1\n",
      "2017-09-26T14:40:31.191025: step 5552, loss 0.00103012, acc 1\n",
      "2017-09-26T14:40:31.466183: step 5553, loss 0.000418516, acc 1\n",
      "2017-09-26T14:40:31.850008: step 5554, loss 8.36117e-05, acc 1\n",
      "2017-09-26T14:40:32.165426: step 5555, loss 0.000619594, acc 1\n",
      "2017-09-26T14:40:32.404611: step 5556, loss 0.000107128, acc 1\n",
      "2017-09-26T14:40:32.741695: step 5557, loss 0.000548795, acc 1\n",
      "2017-09-26T14:40:33.038003: step 5558, loss 0.00753834, acc 1\n",
      "2017-09-26T14:40:33.374863: step 5559, loss 0.000509966, acc 1\n",
      "2017-09-26T14:40:33.714928: step 5560, loss 0.000621554, acc 1\n",
      "2017-09-26T14:40:34.023383: step 5561, loss 0.00108513, acc 1\n",
      "2017-09-26T14:40:34.261028: step 5562, loss 0.000201452, acc 1\n",
      "2017-09-26T14:40:34.582061: step 5563, loss 0.000696682, acc 1\n",
      "2017-09-26T14:40:34.894534: step 5564, loss 0.000154254, acc 1\n",
      "2017-09-26T14:40:35.223447: step 5565, loss 0.000632855, acc 1\n",
      "2017-09-26T14:40:35.538273: step 5566, loss 0.000266093, acc 1\n",
      "2017-09-26T14:40:35.779717: step 5567, loss 0.000398527, acc 1\n",
      "2017-09-26T14:40:36.010629: step 5568, loss 0.000102617, acc 1\n",
      "2017-09-26T14:40:36.319930: step 5569, loss 0.000448695, acc 1\n",
      "2017-09-26T14:40:36.637947: step 5570, loss 0.000129463, acc 1\n",
      "2017-09-26T14:40:36.942152: step 5571, loss 0.000586167, acc 1\n",
      "2017-09-26T14:40:37.265893: step 5572, loss 0.0046692, acc 1\n",
      "2017-09-26T14:40:37.580663: step 5573, loss 0.000584275, acc 1\n",
      "2017-09-26T14:40:37.923089: step 5574, loss 0.000540178, acc 1\n",
      "2017-09-26T14:40:38.226408: step 5575, loss 0.000412736, acc 1\n",
      "2017-09-26T14:40:38.540947: step 5576, loss 0.00292573, acc 1\n",
      "2017-09-26T14:40:38.879918: step 5577, loss 0.00241875, acc 1\n",
      "2017-09-26T14:40:39.227483: step 5578, loss 0.000931489, acc 1\n",
      "2017-09-26T14:40:39.568763: step 5579, loss 0.000173538, acc 1\n",
      "2017-09-26T14:40:39.956634: step 5580, loss 0.000783986, acc 1\n",
      "2017-09-26T14:40:40.352810: step 5581, loss 9.47918e-05, acc 1\n",
      "2017-09-26T14:40:40.735008: step 5582, loss 0.000291917, acc 1\n",
      "2017-09-26T14:40:40.990818: step 5583, loss 0.00017792, acc 1\n",
      "2017-09-26T14:40:41.265925: step 5584, loss 0.018326, acc 0.984375\n",
      "2017-09-26T14:40:41.532177: step 5585, loss 0.00095491, acc 1\n",
      "2017-09-26T14:40:41.802671: step 5586, loss 0.02227, acc 0.980769\n",
      "2017-09-26T14:40:42.118245: step 5587, loss 0.000499485, acc 1\n",
      "2017-09-26T14:40:42.432530: step 5588, loss 0.000325435, acc 1\n",
      "2017-09-26T14:40:42.719977: step 5589, loss 0.000982377, acc 1\n",
      "2017-09-26T14:40:42.950747: step 5590, loss 0.00671528, acc 1\n",
      "2017-09-26T14:40:43.183556: step 5591, loss 0.0008096, acc 1\n",
      "2017-09-26T14:40:43.422038: step 5592, loss 0.000227342, acc 1\n",
      "2017-09-26T14:40:43.731903: step 5593, loss 0.00614935, acc 1\n",
      "2017-09-26T14:40:44.050524: step 5594, loss 0.000232606, acc 1\n",
      "2017-09-26T14:40:44.369283: step 5595, loss 0.000177065, acc 1\n",
      "2017-09-26T14:40:44.675889: step 5596, loss 0.000275636, acc 1\n",
      "2017-09-26T14:40:44.986003: step 5597, loss 0.000483963, acc 1\n",
      "2017-09-26T14:40:45.317074: step 5598, loss 0.00435391, acc 1\n",
      "2017-09-26T14:40:45.632052: step 5599, loss 0.0813159, acc 0.984375\n",
      "2017-09-26T14:40:45.947497: step 5600, loss 0.00100407, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:40:46.258528: step 5600, loss 0.674917, acc 0.878788\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5600\n",
      "\n",
      "2017-09-26T14:40:46.931059: step 5601, loss 0.00040895, acc 1\n",
      "2017-09-26T14:40:47.206845: step 5602, loss 0.000188954, acc 1\n",
      "2017-09-26T14:40:47.436384: step 5603, loss 0.000316951, acc 1\n",
      "2017-09-26T14:40:47.723301: step 5604, loss 0.000189862, acc 1\n",
      "2017-09-26T14:40:48.042128: step 5605, loss 0.00396927, acc 1\n",
      "2017-09-26T14:40:48.365234: step 5606, loss 0.000372103, acc 1\n",
      "2017-09-26T14:40:48.705020: step 5607, loss 9.91398e-05, acc 1\n",
      "2017-09-26T14:40:49.021230: step 5608, loss 9.70216e-05, acc 1\n",
      "2017-09-26T14:40:49.342139: step 5609, loss 0.000783971, acc 1\n",
      "2017-09-26T14:40:49.655753: step 5610, loss 0.000303981, acc 1\n",
      "2017-09-26T14:40:49.934406: step 5611, loss 0.000126629, acc 1\n",
      "2017-09-26T14:40:50.241942: step 5612, loss 0.000455992, acc 1\n",
      "2017-09-26T14:40:50.479987: step 5613, loss 0.00101587, acc 1\n",
      "2017-09-26T14:40:50.786277: step 5614, loss 0.000155032, acc 1\n",
      "2017-09-26T14:40:51.056507: step 5615, loss 0.00179977, acc 1\n",
      "2017-09-26T14:40:51.374161: step 5616, loss 0.000233744, acc 1\n",
      "2017-09-26T14:40:51.631299: step 5617, loss 0.00328238, acc 1\n",
      "2017-09-26T14:40:51.949987: step 5618, loss 0.000396162, acc 1\n",
      "2017-09-26T14:40:52.270168: step 5619, loss 0.00783254, acc 1\n",
      "2017-09-26T14:40:52.576475: step 5620, loss 0.00047234, acc 1\n",
      "2017-09-26T14:40:52.807983: step 5621, loss 0.000244928, acc 1\n",
      "2017-09-26T14:40:53.049865: step 5622, loss 0.00243163, acc 1\n",
      "2017-09-26T14:40:53.296451: step 5623, loss 0.00124477, acc 1\n",
      "2017-09-26T14:40:53.536236: step 5624, loss 0.0185382, acc 0.984375\n",
      "2017-09-26T14:40:53.827130: step 5625, loss 0.00169327, acc 1\n",
      "2017-09-26T14:40:54.133481: step 5626, loss 0.00271796, acc 1\n",
      "2017-09-26T14:40:54.477150: step 5627, loss 0.000168457, acc 1\n",
      "2017-09-26T14:40:54.808581: step 5628, loss 0.000236315, acc 1\n",
      "2017-09-26T14:40:55.116568: step 5629, loss 0.000782441, acc 1\n",
      "2017-09-26T14:40:55.462173: step 5630, loss 0.0018125, acc 1\n",
      "2017-09-26T14:40:55.730722: step 5631, loss 0.00125114, acc 1\n",
      "2017-09-26T14:40:55.985202: step 5632, loss 0.000645076, acc 1\n",
      "2017-09-26T14:40:56.350488: step 5633, loss 0.000319017, acc 1\n",
      "2017-09-26T14:40:56.720309: step 5634, loss 0.00216587, acc 1\n",
      "2017-09-26T14:40:57.072158: step 5635, loss 0.000207707, acc 1\n",
      "2017-09-26T14:40:57.383183: step 5636, loss 0.0002633, acc 1\n",
      "2017-09-26T14:40:57.692444: step 5637, loss 0.000315339, acc 1\n",
      "2017-09-26T14:40:58.006985: step 5638, loss 0.00660601, acc 1\n",
      "2017-09-26T14:40:58.249143: step 5639, loss 0.000113877, acc 1\n",
      "2017-09-26T14:40:58.497237: step 5640, loss 0.000188057, acc 1\n",
      "2017-09-26T14:40:58.755703: step 5641, loss 0.00238819, acc 1\n",
      "2017-09-26T14:40:59.003229: step 5642, loss 0.00234836, acc 1\n",
      "2017-09-26T14:40:59.330933: step 5643, loss 7.01187e-05, acc 1\n",
      "2017-09-26T14:40:59.646947: step 5644, loss 0.00034422, acc 1\n",
      "2017-09-26T14:40:59.946055: step 5645, loss 0.000389435, acc 1\n",
      "2017-09-26T14:41:00.247344: step 5646, loss 0.000165087, acc 1\n",
      "2017-09-26T14:41:00.497657: step 5647, loss 0.000828453, acc 1\n",
      "2017-09-26T14:41:00.743657: step 5648, loss 0.00190435, acc 1\n",
      "2017-09-26T14:41:01.009458: step 5649, loss 0.0222681, acc 0.984375\n",
      "2017-09-26T14:41:01.253786: step 5650, loss 5.29313e-05, acc 1\n",
      "2017-09-26T14:41:01.501006: step 5651, loss 0.000535378, acc 1\n",
      "2017-09-26T14:41:01.749139: step 5652, loss 0.000214424, acc 1\n",
      "2017-09-26T14:41:02.000042: step 5653, loss 0.000480496, acc 1\n",
      "2017-09-26T14:41:02.279913: step 5654, loss 0.00334719, acc 1\n",
      "2017-09-26T14:41:02.583379: step 5655, loss 0.000448587, acc 1\n",
      "2017-09-26T14:41:02.843831: step 5656, loss 0.0167541, acc 0.984375\n",
      "2017-09-26T14:41:03.085867: step 5657, loss 0.00038403, acc 1\n",
      "2017-09-26T14:41:03.350565: step 5658, loss 0.00114091, acc 1\n",
      "2017-09-26T14:41:03.632241: step 5659, loss 0.000261621, acc 1\n",
      "2017-09-26T14:41:03.914489: step 5660, loss 0.000154859, acc 1\n",
      "2017-09-26T14:41:04.197079: step 5661, loss 0.000365114, acc 1\n",
      "2017-09-26T14:41:04.483941: step 5662, loss 0.000329994, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:41:04.778024: step 5663, loss 0.000454624, acc 1\n",
      "2017-09-26T14:41:05.060721: step 5664, loss 0.001498, acc 1\n",
      "2017-09-26T14:41:05.382700: step 5665, loss 0.000433851, acc 1\n",
      "2017-09-26T14:41:05.636681: step 5666, loss 0.00152558, acc 1\n",
      "2017-09-26T14:41:05.882285: step 5667, loss 0.00492478, acc 1\n",
      "2017-09-26T14:41:06.187094: step 5668, loss 0.00031608, acc 1\n",
      "2017-09-26T14:41:06.485282: step 5669, loss 0.0300897, acc 0.984375\n",
      "2017-09-26T14:41:06.728290: step 5670, loss 0.000194178, acc 1\n",
      "2017-09-26T14:41:06.985095: step 5671, loss 0.00093137, acc 1\n",
      "2017-09-26T14:41:07.258587: step 5672, loss 0.000570629, acc 1\n",
      "2017-09-26T14:41:07.571195: step 5673, loss 0.000236162, acc 1\n",
      "2017-09-26T14:41:07.859852: step 5674, loss 0.000803729, acc 1\n",
      "2017-09-26T14:41:08.133597: step 5675, loss 0.000757059, acc 1\n",
      "2017-09-26T14:41:08.416993: step 5676, loss 0.000707756, acc 1\n",
      "2017-09-26T14:41:08.698828: step 5677, loss 0.00218575, acc 1\n",
      "2017-09-26T14:41:08.992895: step 5678, loss 0.00160685, acc 1\n",
      "2017-09-26T14:41:09.238374: step 5679, loss 0.00189092, acc 1\n",
      "2017-09-26T14:41:09.471684: step 5680, loss 0.000172705, acc 1\n",
      "2017-09-26T14:41:09.704555: step 5681, loss 0.00115277, acc 1\n",
      "2017-09-26T14:41:09.937375: step 5682, loss 9.00571e-05, acc 1\n",
      "2017-09-26T14:41:10.174152: step 5683, loss 0.000865847, acc 1\n",
      "2017-09-26T14:41:10.406237: step 5684, loss 0.000233261, acc 1\n",
      "2017-09-26T14:41:10.639828: step 5685, loss 0.00655204, acc 1\n",
      "2017-09-26T14:41:10.870185: step 5686, loss 0.000310651, acc 1\n",
      "2017-09-26T14:41:11.100967: step 5687, loss 0.000985306, acc 1\n",
      "2017-09-26T14:41:11.340521: step 5688, loss 0.0307695, acc 0.984375\n",
      "2017-09-26T14:41:11.574346: step 5689, loss 0.0124697, acc 1\n",
      "2017-09-26T14:41:11.808691: step 5690, loss 0.000173706, acc 1\n",
      "2017-09-26T14:41:12.038631: step 5691, loss 0.000271516, acc 1\n",
      "2017-09-26T14:41:12.284749: step 5692, loss 9.57946e-05, acc 1\n",
      "2017-09-26T14:41:12.517390: step 5693, loss 0.000275681, acc 1\n",
      "2017-09-26T14:41:12.750709: step 5694, loss 0.00148298, acc 1\n",
      "2017-09-26T14:41:12.983322: step 5695, loss 0.000166342, acc 1\n",
      "2017-09-26T14:41:13.219694: step 5696, loss 0.000916363, acc 1\n",
      "2017-09-26T14:41:13.458145: step 5697, loss 0.000293673, acc 1\n",
      "2017-09-26T14:41:13.689384: step 5698, loss 0.000493473, acc 1\n",
      "2017-09-26T14:41:13.921614: step 5699, loss 0.000338484, acc 1\n",
      "2017-09-26T14:41:14.164134: step 5700, loss 0.000241458, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:41:14.391056: step 5700, loss 0.764186, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5700\n",
      "\n",
      "2017-09-26T14:41:15.165970: step 5701, loss 0.00694475, acc 1\n",
      "2017-09-26T14:41:15.449136: step 5702, loss 0.0292377, acc 0.984375\n",
      "2017-09-26T14:41:15.733600: step 5703, loss 0.0009973, acc 1\n",
      "2017-09-26T14:41:16.014869: step 5704, loss 0.000320818, acc 1\n",
      "2017-09-26T14:41:16.296456: step 5705, loss 0.00804733, acc 1\n",
      "2017-09-26T14:41:16.546740: step 5706, loss 0.000911282, acc 1\n",
      "2017-09-26T14:41:16.782016: step 5707, loss 0.000573939, acc 1\n",
      "2017-09-26T14:41:17.014476: step 5708, loss 0.000922889, acc 1\n",
      "2017-09-26T14:41:17.246274: step 5709, loss 0.00240571, acc 1\n",
      "2017-09-26T14:41:17.479256: step 5710, loss 0.00182479, acc 1\n",
      "2017-09-26T14:41:17.710181: step 5711, loss 0.000108014, acc 1\n",
      "2017-09-26T14:41:17.911982: step 5712, loss 0.000343839, acc 1\n",
      "2017-09-26T14:41:18.151916: step 5713, loss 0.000621581, acc 1\n",
      "2017-09-26T14:41:18.387344: step 5714, loss 0.00926845, acc 1\n",
      "2017-09-26T14:41:18.632640: step 5715, loss 0.000217409, acc 1\n",
      "2017-09-26T14:41:18.886535: step 5716, loss 0.000114242, acc 1\n",
      "2017-09-26T14:41:19.160591: step 5717, loss 0.000881151, acc 1\n",
      "2017-09-26T14:41:19.418861: step 5718, loss 0.000171259, acc 1\n",
      "2017-09-26T14:41:19.653300: step 5719, loss 0.000562054, acc 1\n",
      "2017-09-26T14:41:19.916179: step 5720, loss 0.000835607, acc 1\n",
      "2017-09-26T14:41:20.161579: step 5721, loss 0.00156347, acc 1\n",
      "2017-09-26T14:41:20.414326: step 5722, loss 3.6497e-05, acc 1\n",
      "2017-09-26T14:41:20.657075: step 5723, loss 0.000683708, acc 1\n",
      "2017-09-26T14:41:20.908873: step 5724, loss 0.000172043, acc 1\n",
      "2017-09-26T14:41:21.141420: step 5725, loss 0.000295188, acc 1\n",
      "2017-09-26T14:41:21.383789: step 5726, loss 0.000328527, acc 1\n",
      "2017-09-26T14:41:21.617917: step 5727, loss 0.0203557, acc 0.984375\n",
      "2017-09-26T14:41:21.855632: step 5728, loss 0.000514392, acc 1\n",
      "2017-09-26T14:41:22.105249: step 5729, loss 0.000100467, acc 1\n",
      "2017-09-26T14:41:22.363523: step 5730, loss 0.0217211, acc 0.984375\n",
      "2017-09-26T14:41:22.615490: step 5731, loss 3.47793e-05, acc 1\n",
      "2017-09-26T14:41:22.863840: step 5732, loss 0.000168461, acc 1\n",
      "2017-09-26T14:41:23.103513: step 5733, loss 0.000170748, acc 1\n",
      "2017-09-26T14:41:23.354006: step 5734, loss 0.000120567, acc 1\n",
      "2017-09-26T14:41:23.589041: step 5735, loss 0.00142179, acc 1\n",
      "2017-09-26T14:41:23.833998: step 5736, loss 0.000512268, acc 1\n",
      "2017-09-26T14:41:24.080245: step 5737, loss 9.29169e-05, acc 1\n",
      "2017-09-26T14:41:24.315851: step 5738, loss 0.00124736, acc 1\n",
      "2017-09-26T14:41:24.567517: step 5739, loss 0.000397505, acc 1\n",
      "2017-09-26T14:41:24.813648: step 5740, loss 0.000411089, acc 1\n",
      "2017-09-26T14:41:25.053770: step 5741, loss 0.00111991, acc 1\n",
      "2017-09-26T14:41:25.294202: step 5742, loss 0.0163273, acc 0.984375\n",
      "2017-09-26T14:41:25.538040: step 5743, loss 0.000213098, acc 1\n",
      "2017-09-26T14:41:25.805230: step 5744, loss 0.00084698, acc 1\n",
      "2017-09-26T14:41:26.133742: step 5745, loss 0.006806, acc 1\n",
      "2017-09-26T14:41:26.454478: step 5746, loss 0.0423917, acc 0.984375\n",
      "2017-09-26T14:41:26.748264: step 5747, loss 0.0010335, acc 1\n",
      "2017-09-26T14:41:27.038540: step 5748, loss 0.00128322, acc 1\n",
      "2017-09-26T14:41:27.277208: step 5749, loss 0.00181954, acc 1\n",
      "2017-09-26T14:41:27.527856: step 5750, loss 1.56648e-05, acc 1\n",
      "2017-09-26T14:41:27.770324: step 5751, loss 0.00113068, acc 1\n",
      "2017-09-26T14:41:28.004157: step 5752, loss 0.000945882, acc 1\n",
      "2017-09-26T14:41:28.297480: step 5753, loss 0.00434995, acc 1\n",
      "2017-09-26T14:41:28.581688: step 5754, loss 0.000195424, acc 1\n",
      "2017-09-26T14:41:28.893210: step 5755, loss 9.88889e-05, acc 1\n",
      "2017-09-26T14:41:29.128945: step 5756, loss 0.00013202, acc 1\n",
      "2017-09-26T14:41:29.380054: step 5757, loss 0.00196997, acc 1\n",
      "2017-09-26T14:41:29.632853: step 5758, loss 0.0167153, acc 0.984375\n",
      "2017-09-26T14:41:29.897483: step 5759, loss 0.200472, acc 0.984375\n",
      "2017-09-26T14:41:30.162739: step 5760, loss 0.00124669, acc 1\n",
      "2017-09-26T14:41:30.445197: step 5761, loss 0.000184607, acc 1\n",
      "2017-09-26T14:41:30.696540: step 5762, loss 0.00058252, acc 1\n",
      "2017-09-26T14:41:30.946762: step 5763, loss 0.000213079, acc 1\n",
      "2017-09-26T14:41:31.199803: step 5764, loss 0.000586417, acc 1\n",
      "2017-09-26T14:41:31.438827: step 5765, loss 0.000866201, acc 1\n",
      "2017-09-26T14:41:31.695372: step 5766, loss 0.000237442, acc 1\n",
      "2017-09-26T14:41:31.948041: step 5767, loss 0.000697887, acc 1\n",
      "2017-09-26T14:41:32.203954: step 5768, loss 0.000131768, acc 1\n",
      "2017-09-26T14:41:32.450910: step 5769, loss 0.00012152, acc 1\n",
      "2017-09-26T14:41:32.735328: step 5770, loss 0.000163026, acc 1\n",
      "2017-09-26T14:41:32.991322: step 5771, loss 0.000161429, acc 1\n",
      "2017-09-26T14:41:33.272847: step 5772, loss 0.0039171, acc 1\n",
      "2017-09-26T14:41:33.559315: step 5773, loss 0.0819148, acc 0.984375\n",
      "2017-09-26T14:41:33.810298: step 5774, loss 0.00201949, acc 1\n",
      "2017-09-26T14:41:34.083954: step 5775, loss 0.000235451, acc 1\n",
      "2017-09-26T14:41:34.369925: step 5776, loss 0.00826012, acc 1\n",
      "2017-09-26T14:41:34.622012: step 5777, loss 0.00035056, acc 1\n",
      "2017-09-26T14:41:34.876686: step 5778, loss 0.000898015, acc 1\n",
      "2017-09-26T14:41:35.127329: step 5779, loss 0.000352603, acc 1\n",
      "2017-09-26T14:41:35.386511: step 5780, loss 0.00143071, acc 1\n",
      "2017-09-26T14:41:35.630977: step 5781, loss 0.000183751, acc 1\n",
      "2017-09-26T14:41:35.880708: step 5782, loss 0.000493887, acc 1\n",
      "2017-09-26T14:41:36.143873: step 5783, loss 0.00148687, acc 1\n",
      "2017-09-26T14:41:36.401878: step 5784, loss 0.00447486, acc 1\n",
      "2017-09-26T14:41:36.671983: step 5785, loss 0.00223186, acc 1\n",
      "2017-09-26T14:41:36.923411: step 5786, loss 0.00122515, acc 1\n",
      "2017-09-26T14:41:37.173320: step 5787, loss 0.000371915, acc 1\n",
      "2017-09-26T14:41:37.428493: step 5788, loss 0.0159093, acc 0.984375\n",
      "2017-09-26T14:41:37.687275: step 5789, loss 0.164554, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:41:37.939300: step 5790, loss 0.00103503, acc 1\n",
      "2017-09-26T14:41:38.197960: step 5791, loss 0.0112644, acc 1\n",
      "2017-09-26T14:41:38.449208: step 5792, loss 0.000210682, acc 1\n",
      "2017-09-26T14:41:38.686067: step 5793, loss 0.000215847, acc 1\n",
      "2017-09-26T14:41:38.915087: step 5794, loss 0.000250668, acc 1\n",
      "2017-09-26T14:41:39.161210: step 5795, loss 0.000340717, acc 1\n",
      "2017-09-26T14:41:39.369592: step 5796, loss 0.0058154, acc 1\n",
      "2017-09-26T14:41:39.634082: step 5797, loss 0.00495218, acc 1\n",
      "2017-09-26T14:41:39.870672: step 5798, loss 0.000334753, acc 1\n",
      "2017-09-26T14:41:40.124950: step 5799, loss 0.000247368, acc 1\n",
      "2017-09-26T14:41:40.365777: step 5800, loss 0.000194661, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:41:40.605481: step 5800, loss 0.680173, acc 0.878788\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5800\n",
      "\n",
      "2017-09-26T14:41:41.116063: step 5801, loss 0.00313114, acc 1\n",
      "2017-09-26T14:41:41.363888: step 5802, loss 0.000417935, acc 1\n",
      "2017-09-26T14:41:41.602335: step 5803, loss 0.00136596, acc 1\n",
      "2017-09-26T14:41:41.854561: step 5804, loss 0.000156957, acc 1\n",
      "2017-09-26T14:41:42.088776: step 5805, loss 0.00295508, acc 1\n",
      "2017-09-26T14:41:42.342887: step 5806, loss 0.00032532, acc 1\n",
      "2017-09-26T14:41:42.588727: step 5807, loss 0.00107042, acc 1\n",
      "2017-09-26T14:41:42.836715: step 5808, loss 0.000712635, acc 1\n",
      "2017-09-26T14:41:43.084011: step 5809, loss 0.000200304, acc 1\n",
      "2017-09-26T14:41:43.324356: step 5810, loss 0.000160561, acc 1\n",
      "2017-09-26T14:41:43.562472: step 5811, loss 0.00109021, acc 1\n",
      "2017-09-26T14:41:43.808515: step 5812, loss 0.00115804, acc 1\n",
      "2017-09-26T14:41:44.054922: step 5813, loss 4.60555e-05, acc 1\n",
      "2017-09-26T14:41:44.311966: step 5814, loss 0.000718576, acc 1\n",
      "2017-09-26T14:41:44.554616: step 5815, loss 0.000592458, acc 1\n",
      "2017-09-26T14:41:44.797938: step 5816, loss 0.000746282, acc 1\n",
      "2017-09-26T14:41:45.039424: step 5817, loss 0.000175497, acc 1\n",
      "2017-09-26T14:41:45.277776: step 5818, loss 0.000170886, acc 1\n",
      "2017-09-26T14:41:45.517173: step 5819, loss 0.00366289, acc 1\n",
      "2017-09-26T14:41:45.752660: step 5820, loss 0.0103713, acc 1\n",
      "2017-09-26T14:41:46.006129: step 5821, loss 0.000728558, acc 1\n",
      "2017-09-26T14:41:46.257936: step 5822, loss 0.0408753, acc 0.984375\n",
      "2017-09-26T14:41:46.489935: step 5823, loss 0.000170598, acc 1\n",
      "2017-09-26T14:41:46.728243: step 5824, loss 0.000166253, acc 1\n",
      "2017-09-26T14:41:46.962310: step 5825, loss 0.000111439, acc 1\n",
      "2017-09-26T14:41:47.225296: step 5826, loss 0.000282466, acc 1\n",
      "2017-09-26T14:41:47.492555: step 5827, loss 0.000409556, acc 1\n",
      "2017-09-26T14:41:47.752948: step 5828, loss 0.00251809, acc 1\n",
      "2017-09-26T14:41:47.993413: step 5829, loss 0.000372482, acc 1\n",
      "2017-09-26T14:41:48.262896: step 5830, loss 0.000776946, acc 1\n",
      "2017-09-26T14:41:48.502937: step 5831, loss 0.00821717, acc 1\n",
      "2017-09-26T14:41:48.751479: step 5832, loss 0.000127007, acc 1\n",
      "2017-09-26T14:41:48.997038: step 5833, loss 0.00124997, acc 1\n",
      "2017-09-26T14:41:49.251053: step 5834, loss 0.00114913, acc 1\n",
      "2017-09-26T14:41:49.493706: step 5835, loss 0.0593302, acc 0.984375\n",
      "2017-09-26T14:41:49.748725: step 5836, loss 0.000102071, acc 1\n",
      "2017-09-26T14:41:49.997201: step 5837, loss 0.000627119, acc 1\n",
      "2017-09-26T14:41:50.207442: step 5838, loss 0.000106677, acc 1\n",
      "2017-09-26T14:41:50.472857: step 5839, loss 0.000266108, acc 1\n",
      "2017-09-26T14:41:50.713186: step 5840, loss 0.000765207, acc 1\n",
      "2017-09-26T14:41:50.963601: step 5841, loss 0.00252995, acc 1\n",
      "2017-09-26T14:41:51.210854: step 5842, loss 0.00192706, acc 1\n",
      "2017-09-26T14:41:51.472854: step 5843, loss 0.000537396, acc 1\n",
      "2017-09-26T14:41:51.710266: step 5844, loss 0.00108291, acc 1\n",
      "2017-09-26T14:41:51.955939: step 5845, loss 0.000214476, acc 1\n",
      "2017-09-26T14:41:52.205680: step 5846, loss 0.0204674, acc 0.984375\n",
      "2017-09-26T14:41:52.473207: step 5847, loss 0.075082, acc 0.984375\n",
      "2017-09-26T14:41:52.744326: step 5848, loss 0.000633618, acc 1\n",
      "2017-09-26T14:41:52.988103: step 5849, loss 0.000890529, acc 1\n",
      "2017-09-26T14:41:53.226608: step 5850, loss 0.00131197, acc 1\n",
      "2017-09-26T14:41:53.476504: step 5851, loss 0.000446656, acc 1\n",
      "2017-09-26T14:41:53.725521: step 5852, loss 0.000156011, acc 1\n",
      "2017-09-26T14:41:53.991963: step 5853, loss 0.000164487, acc 1\n",
      "2017-09-26T14:41:54.246012: step 5854, loss 0.000239326, acc 1\n",
      "2017-09-26T14:41:54.512570: step 5855, loss 0.0105458, acc 1\n",
      "2017-09-26T14:41:54.800552: step 5856, loss 0.000252935, acc 1\n",
      "2017-09-26T14:41:55.072211: step 5857, loss 0.000508968, acc 1\n",
      "2017-09-26T14:41:55.310176: step 5858, loss 0.000156532, acc 1\n",
      "2017-09-26T14:41:55.553441: step 5859, loss 0.00035869, acc 1\n",
      "2017-09-26T14:41:55.827322: step 5860, loss 0.0056419, acc 1\n",
      "2017-09-26T14:41:56.068200: step 5861, loss 0.000367613, acc 1\n",
      "2017-09-26T14:41:56.330120: step 5862, loss 0.00205279, acc 1\n",
      "2017-09-26T14:41:56.628461: step 5863, loss 0.000326053, acc 1\n",
      "2017-09-26T14:41:56.902137: step 5864, loss 0.0160549, acc 1\n",
      "2017-09-26T14:41:57.181980: step 5865, loss 0.000448398, acc 1\n",
      "2017-09-26T14:41:57.448623: step 5866, loss 0.000680954, acc 1\n",
      "2017-09-26T14:41:57.741453: step 5867, loss 0.00253114, acc 1\n",
      "2017-09-26T14:41:58.047038: step 5868, loss 0.000116067, acc 1\n",
      "2017-09-26T14:41:58.333442: step 5869, loss 0.0015322, acc 1\n",
      "2017-09-26T14:41:58.593760: step 5870, loss 0.000829051, acc 1\n",
      "2017-09-26T14:41:58.845213: step 5871, loss 0.00115821, acc 1\n",
      "2017-09-26T14:41:59.094539: step 5872, loss 0.0134952, acc 0.984375\n",
      "2017-09-26T14:41:59.378459: step 5873, loss 0.013684, acc 1\n",
      "2017-09-26T14:41:59.688098: step 5874, loss 0.0112604, acc 1\n",
      "2017-09-26T14:41:59.938257: step 5875, loss 0.00202345, acc 1\n",
      "2017-09-26T14:42:00.188286: step 5876, loss 0.011242, acc 1\n",
      "2017-09-26T14:42:00.443940: step 5877, loss 0.0926627, acc 0.984375\n",
      "2017-09-26T14:42:00.690222: step 5878, loss 0.000181421, acc 1\n",
      "2017-09-26T14:42:00.939895: step 5879, loss 0.000769671, acc 1\n",
      "2017-09-26T14:42:01.156075: step 5880, loss 0.000182357, acc 1\n",
      "2017-09-26T14:42:01.432932: step 5881, loss 0.00013239, acc 1\n",
      "2017-09-26T14:42:01.681985: step 5882, loss 0.000918433, acc 1\n",
      "2017-09-26T14:42:01.949475: step 5883, loss 0.000433573, acc 1\n",
      "2017-09-26T14:42:02.199530: step 5884, loss 0.000406136, acc 1\n",
      "2017-09-26T14:42:02.448142: step 5885, loss 0.000255298, acc 1\n",
      "2017-09-26T14:42:02.691628: step 5886, loss 0.00306386, acc 1\n",
      "2017-09-26T14:42:02.944524: step 5887, loss 0.00434193, acc 1\n",
      "2017-09-26T14:42:03.189499: step 5888, loss 0.00363249, acc 1\n",
      "2017-09-26T14:42:03.438474: step 5889, loss 0.00402465, acc 1\n",
      "2017-09-26T14:42:03.689296: step 5890, loss 0.00176888, acc 1\n",
      "2017-09-26T14:42:03.957813: step 5891, loss 0.0184949, acc 0.984375\n",
      "2017-09-26T14:42:04.220643: step 5892, loss 0.0365316, acc 0.984375\n",
      "2017-09-26T14:42:04.497728: step 5893, loss 0.000269801, acc 1\n",
      "2017-09-26T14:42:04.761894: step 5894, loss 0.000486208, acc 1\n",
      "2017-09-26T14:42:05.010200: step 5895, loss 0.000223393, acc 1\n",
      "2017-09-26T14:42:05.260060: step 5896, loss 0.00393406, acc 1\n",
      "2017-09-26T14:42:05.500345: step 5897, loss 0.00176398, acc 1\n",
      "2017-09-26T14:42:05.747055: step 5898, loss 0.000521786, acc 1\n",
      "2017-09-26T14:42:05.991617: step 5899, loss 0.000832952, acc 1\n",
      "2017-09-26T14:42:06.236454: step 5900, loss 0.000174929, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:42:06.476069: step 5900, loss 0.590243, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-5900\n",
      "\n",
      "2017-09-26T14:42:06.995367: step 5901, loss 0.000240465, acc 1\n",
      "2017-09-26T14:42:07.238487: step 5902, loss 0.00679125, acc 1\n",
      "2017-09-26T14:42:07.477807: step 5903, loss 0.0188945, acc 0.984375\n",
      "2017-09-26T14:42:07.724031: step 5904, loss 0.000128571, acc 1\n",
      "2017-09-26T14:42:07.997316: step 5905, loss 0.0199324, acc 0.984375\n",
      "2017-09-26T14:42:08.239338: step 5906, loss 0.0149618, acc 0.984375\n",
      "2017-09-26T14:42:08.485510: step 5907, loss 0.000803334, acc 1\n",
      "2017-09-26T14:42:08.731666: step 5908, loss 0.000638115, acc 1\n",
      "2017-09-26T14:42:08.987030: step 5909, loss 0.000597461, acc 1\n",
      "2017-09-26T14:42:09.238281: step 5910, loss 0.000150169, acc 1\n",
      "2017-09-26T14:42:09.495331: step 5911, loss 0.000698354, acc 1\n",
      "2017-09-26T14:42:09.745161: step 5912, loss 0.000573144, acc 1\n",
      "2017-09-26T14:42:09.994125: step 5913, loss 0.00441788, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:42:10.251504: step 5914, loss 0.0101342, acc 1\n",
      "2017-09-26T14:42:10.500335: step 5915, loss 0.0012665, acc 1\n",
      "2017-09-26T14:42:10.730825: step 5916, loss 0.00128401, acc 1\n",
      "2017-09-26T14:42:10.960945: step 5917, loss 0.000723337, acc 1\n",
      "2017-09-26T14:42:11.196085: step 5918, loss 0.000843964, acc 1\n",
      "2017-09-26T14:42:11.431015: step 5919, loss 0.000146379, acc 1\n",
      "2017-09-26T14:42:11.665292: step 5920, loss 0.00183261, acc 1\n",
      "2017-09-26T14:42:11.902450: step 5921, loss 0.000680438, acc 1\n",
      "2017-09-26T14:42:12.111182: step 5922, loss 0.000617896, acc 1\n",
      "2017-09-26T14:42:12.348989: step 5923, loss 0.00093113, acc 1\n",
      "2017-09-26T14:42:12.590704: step 5924, loss 0.00260031, acc 1\n",
      "2017-09-26T14:42:12.825220: step 5925, loss 0.000108675, acc 1\n",
      "2017-09-26T14:42:13.108126: step 5926, loss 0.0023688, acc 1\n",
      "2017-09-26T14:42:13.362970: step 5927, loss 0.00149799, acc 1\n",
      "2017-09-26T14:42:13.599539: step 5928, loss 0.000407623, acc 1\n",
      "2017-09-26T14:42:13.856934: step 5929, loss 0.000186644, acc 1\n",
      "2017-09-26T14:42:14.089669: step 5930, loss 0.0106175, acc 1\n",
      "2017-09-26T14:42:14.325994: step 5931, loss 0.0711458, acc 0.984375\n",
      "2017-09-26T14:42:14.583997: step 5932, loss 0.00135224, acc 1\n",
      "2017-09-26T14:42:14.834211: step 5933, loss 0.000282651, acc 1\n",
      "2017-09-26T14:42:15.087375: step 5934, loss 0.000951279, acc 1\n",
      "2017-09-26T14:42:15.348965: step 5935, loss 0.000895547, acc 1\n",
      "2017-09-26T14:42:15.595377: step 5936, loss 0.000275931, acc 1\n",
      "2017-09-26T14:42:15.849280: step 5937, loss 0.000204066, acc 1\n",
      "2017-09-26T14:42:16.081016: step 5938, loss 0.00791235, acc 1\n",
      "2017-09-26T14:42:16.311043: step 5939, loss 0.00171957, acc 1\n",
      "2017-09-26T14:42:16.544480: step 5940, loss 0.000556322, acc 1\n",
      "2017-09-26T14:42:16.788930: step 5941, loss 0.000159331, acc 1\n",
      "2017-09-26T14:42:17.033049: step 5942, loss 0.000125417, acc 1\n",
      "2017-09-26T14:42:17.288149: step 5943, loss 0.000836328, acc 1\n",
      "2017-09-26T14:42:17.521806: step 5944, loss 0.00143921, acc 1\n",
      "2017-09-26T14:42:17.756178: step 5945, loss 0.000492394, acc 1\n",
      "2017-09-26T14:42:18.002116: step 5946, loss 0.000122495, acc 1\n",
      "2017-09-26T14:42:18.234490: step 5947, loss 0.00303361, acc 1\n",
      "2017-09-26T14:42:18.472423: step 5948, loss 0.000425653, acc 1\n",
      "2017-09-26T14:42:18.706764: step 5949, loss 0.000837563, acc 1\n",
      "2017-09-26T14:42:18.952577: step 5950, loss 0.000278418, acc 1\n",
      "2017-09-26T14:42:19.191091: step 5951, loss 0.00375897, acc 1\n",
      "2017-09-26T14:42:19.425736: step 5952, loss 0.000359965, acc 1\n",
      "2017-09-26T14:42:19.658335: step 5953, loss 0.000472198, acc 1\n",
      "2017-09-26T14:42:19.898472: step 5954, loss 0.00022832, acc 1\n",
      "2017-09-26T14:42:20.150305: step 5955, loss 0.00159155, acc 1\n",
      "2017-09-26T14:42:20.403370: step 5956, loss 0.000420284, acc 1\n",
      "2017-09-26T14:42:20.664115: step 5957, loss 0.00385949, acc 1\n",
      "2017-09-26T14:42:20.947919: step 5958, loss 0.000136826, acc 1\n",
      "2017-09-26T14:42:21.215752: step 5959, loss 0.00022115, acc 1\n",
      "2017-09-26T14:42:21.507777: step 5960, loss 0.000688781, acc 1\n",
      "2017-09-26T14:42:21.751244: step 5961, loss 0.00558874, acc 1\n",
      "2017-09-26T14:42:22.021928: step 5962, loss 0.000632907, acc 1\n",
      "2017-09-26T14:42:22.279050: step 5963, loss 0.000327509, acc 1\n",
      "2017-09-26T14:42:22.490849: step 5964, loss 0.000313171, acc 1\n",
      "2017-09-26T14:42:22.736329: step 5965, loss 0.000893141, acc 1\n",
      "2017-09-26T14:42:22.973239: step 5966, loss 8.29795e-05, acc 1\n",
      "2017-09-26T14:42:23.231526: step 5967, loss 0.00070903, acc 1\n",
      "2017-09-26T14:42:23.466289: step 5968, loss 0.00178054, acc 1\n",
      "2017-09-26T14:42:23.702495: step 5969, loss 0.00019501, acc 1\n",
      "2017-09-26T14:42:23.948434: step 5970, loss 0.00203549, acc 1\n",
      "2017-09-26T14:42:24.187178: step 5971, loss 0.00159343, acc 1\n",
      "2017-09-26T14:42:24.426405: step 5972, loss 0.0025822, acc 1\n",
      "2017-09-26T14:42:24.669934: step 5973, loss 0.000177232, acc 1\n",
      "2017-09-26T14:42:24.917470: step 5974, loss 0.000621675, acc 1\n",
      "2017-09-26T14:42:25.150265: step 5975, loss 0.000342588, acc 1\n",
      "2017-09-26T14:42:25.387514: step 5976, loss 0.000282128, acc 1\n",
      "2017-09-26T14:42:25.624072: step 5977, loss 0.000779649, acc 1\n",
      "2017-09-26T14:42:25.857509: step 5978, loss 0.000414322, acc 1\n",
      "2017-09-26T14:42:26.094364: step 5979, loss 0.0111559, acc 1\n",
      "2017-09-26T14:42:26.330759: step 5980, loss 0.000560007, acc 1\n",
      "2017-09-26T14:42:26.566966: step 5981, loss 0.000509253, acc 1\n",
      "2017-09-26T14:42:26.804471: step 5982, loss 9.60542e-05, acc 1\n",
      "2017-09-26T14:42:27.047498: step 5983, loss 0.000644963, acc 1\n",
      "2017-09-26T14:42:27.332271: step 5984, loss 0.000209896, acc 1\n",
      "2017-09-26T14:42:27.600398: step 5985, loss 4.07584e-05, acc 1\n",
      "2017-09-26T14:42:27.838613: step 5986, loss 0.00439195, acc 1\n",
      "2017-09-26T14:42:28.076689: step 5987, loss 0.000122963, acc 1\n",
      "2017-09-26T14:42:28.318856: step 5988, loss 0.000130549, acc 1\n",
      "2017-09-26T14:42:28.556205: step 5989, loss 0.000685143, acc 1\n",
      "2017-09-26T14:42:28.823590: step 5990, loss 0.00379393, acc 1\n",
      "2017-09-26T14:42:29.062866: step 5991, loss 0.00042746, acc 1\n",
      "2017-09-26T14:42:29.328104: step 5992, loss 0.000229164, acc 1\n",
      "2017-09-26T14:42:29.582318: step 5993, loss 0.0074041, acc 1\n",
      "2017-09-26T14:42:29.833376: step 5994, loss 0.00111098, acc 1\n",
      "2017-09-26T14:42:30.073933: step 5995, loss 0.00385581, acc 1\n",
      "2017-09-26T14:42:30.312829: step 5996, loss 0.00619355, acc 1\n",
      "2017-09-26T14:42:30.558751: step 5997, loss 0.000412042, acc 1\n",
      "2017-09-26T14:42:30.798832: step 5998, loss 0.00199631, acc 1\n",
      "2017-09-26T14:42:31.044720: step 5999, loss 0.000130081, acc 1\n",
      "2017-09-26T14:42:31.289480: step 6000, loss 0.000455408, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:42:31.528577: step 6000, loss 0.676675, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6000\n",
      "\n",
      "2017-09-26T14:42:32.077525: step 6001, loss 0.00322321, acc 1\n",
      "2017-09-26T14:42:32.334994: step 6002, loss 0.000153292, acc 1\n",
      "2017-09-26T14:42:32.633130: step 6003, loss 0.000200579, acc 1\n",
      "2017-09-26T14:42:32.889926: step 6004, loss 0.000268996, acc 1\n",
      "2017-09-26T14:42:33.168998: step 6005, loss 0.000515494, acc 1\n",
      "2017-09-26T14:42:33.377512: step 6006, loss 0.000892998, acc 1\n",
      "2017-09-26T14:42:33.617902: step 6007, loss 0.00146739, acc 1\n",
      "2017-09-26T14:42:33.877927: step 6008, loss 0.00229866, acc 1\n",
      "2017-09-26T14:42:34.136801: step 6009, loss 0.00197682, acc 1\n",
      "2017-09-26T14:42:34.386130: step 6010, loss 0.000602744, acc 1\n",
      "2017-09-26T14:42:34.629043: step 6011, loss 0.00367002, acc 1\n",
      "2017-09-26T14:42:34.874090: step 6012, loss 0.00617211, acc 1\n",
      "2017-09-26T14:42:35.112164: step 6013, loss 6.71226e-05, acc 1\n",
      "2017-09-26T14:42:35.352284: step 6014, loss 0.00197559, acc 1\n",
      "2017-09-26T14:42:35.591504: step 6015, loss 0.000394679, acc 1\n",
      "2017-09-26T14:42:35.843979: step 6016, loss 0.000220184, acc 1\n",
      "2017-09-26T14:42:36.082568: step 6017, loss 0.0443192, acc 0.984375\n",
      "2017-09-26T14:42:36.323477: step 6018, loss 0.0125012, acc 0.984375\n",
      "2017-09-26T14:42:36.569128: step 6019, loss 0.000496348, acc 1\n",
      "2017-09-26T14:42:36.808745: step 6020, loss 0.000116133, acc 1\n",
      "2017-09-26T14:42:37.056640: step 6021, loss 8.50165e-05, acc 1\n",
      "2017-09-26T14:42:37.324355: step 6022, loss 0.000300846, acc 1\n",
      "2017-09-26T14:42:37.570042: step 6023, loss 0.00303688, acc 1\n",
      "2017-09-26T14:42:37.826921: step 6024, loss 0.000816721, acc 1\n",
      "2017-09-26T14:42:38.092181: step 6025, loss 0.00530932, acc 1\n",
      "2017-09-26T14:42:38.339029: step 6026, loss 0.000171741, acc 1\n",
      "2017-09-26T14:42:38.594470: step 6027, loss 7.68666e-05, acc 1\n",
      "2017-09-26T14:42:38.846598: step 6028, loss 0.000152981, acc 1\n",
      "2017-09-26T14:42:39.082424: step 6029, loss 0.000406507, acc 1\n",
      "2017-09-26T14:42:39.326686: step 6030, loss 0.00110452, acc 1\n",
      "2017-09-26T14:42:39.583044: step 6031, loss 8.69072e-05, acc 1\n",
      "2017-09-26T14:42:39.853454: step 6032, loss 6.05573e-05, acc 1\n",
      "2017-09-26T14:42:40.091482: step 6033, loss 0.00100638, acc 1\n",
      "2017-09-26T14:42:40.355908: step 6034, loss 0.000459335, acc 1\n",
      "2017-09-26T14:42:40.587757: step 6035, loss 0.000351597, acc 1\n",
      "2017-09-26T14:42:40.837431: step 6036, loss 0.000199869, acc 1\n",
      "2017-09-26T14:42:41.091972: step 6037, loss 0.000232981, acc 1\n",
      "2017-09-26T14:42:41.343538: step 6038, loss 0.000110075, acc 1\n",
      "2017-09-26T14:42:41.581948: step 6039, loss 0.000526454, acc 1\n",
      "2017-09-26T14:42:41.828940: step 6040, loss 0.000459548, acc 1\n",
      "2017-09-26T14:42:42.094702: step 6041, loss 0.000248298, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:42:42.344460: step 6042, loss 0.023661, acc 0.984375\n",
      "2017-09-26T14:42:42.587157: step 6043, loss 0.000240421, acc 1\n",
      "2017-09-26T14:42:42.818764: step 6044, loss 0.000148176, acc 1\n",
      "2017-09-26T14:42:43.075011: step 6045, loss 0.00186267, acc 1\n",
      "2017-09-26T14:42:43.318135: step 6046, loss 0.000962252, acc 1\n",
      "2017-09-26T14:42:43.548353: step 6047, loss 0.00100868, acc 1\n",
      "2017-09-26T14:42:43.766395: step 6048, loss 7.47367e-05, acc 1\n",
      "2017-09-26T14:42:44.013975: step 6049, loss 0.00102078, acc 1\n",
      "2017-09-26T14:42:44.266265: step 6050, loss 0.00161967, acc 1\n",
      "2017-09-26T14:42:44.504274: step 6051, loss 5.36703e-05, acc 1\n",
      "2017-09-26T14:42:44.743268: step 6052, loss 0.00331961, acc 1\n",
      "2017-09-26T14:42:44.985239: step 6053, loss 0.000382201, acc 1\n",
      "2017-09-26T14:42:45.220831: step 6054, loss 0.00033095, acc 1\n",
      "2017-09-26T14:42:45.458349: step 6055, loss 0.00112634, acc 1\n",
      "2017-09-26T14:42:45.701087: step 6056, loss 0.000136718, acc 1\n",
      "2017-09-26T14:42:45.949802: step 6057, loss 6.39538e-05, acc 1\n",
      "2017-09-26T14:42:46.178925: step 6058, loss 0.00176345, acc 1\n",
      "2017-09-26T14:42:46.432265: step 6059, loss 0.000569612, acc 1\n",
      "2017-09-26T14:42:46.724530: step 6060, loss 0.000131111, acc 1\n",
      "2017-09-26T14:42:46.962003: step 6061, loss 6.89532e-05, acc 1\n",
      "2017-09-26T14:42:47.194472: step 6062, loss 0.00828262, acc 1\n",
      "2017-09-26T14:42:47.434994: step 6063, loss 2.50594e-05, acc 1\n",
      "2017-09-26T14:42:47.674372: step 6064, loss 0.000366653, acc 1\n",
      "2017-09-26T14:42:47.918394: step 6065, loss 0.00109874, acc 1\n",
      "2017-09-26T14:42:48.162640: step 6066, loss 0.000349759, acc 1\n",
      "2017-09-26T14:42:48.400941: step 6067, loss 0.000244, acc 1\n",
      "2017-09-26T14:42:48.644517: step 6068, loss 0.00136379, acc 1\n",
      "2017-09-26T14:42:48.888730: step 6069, loss 0.000162616, acc 1\n",
      "2017-09-26T14:42:49.135544: step 6070, loss 0.113287, acc 0.984375\n",
      "2017-09-26T14:42:49.412226: step 6071, loss 6.83297e-05, acc 1\n",
      "2017-09-26T14:42:49.653097: step 6072, loss 0.000282599, acc 1\n",
      "2017-09-26T14:42:49.939227: step 6073, loss 0.000134579, acc 1\n",
      "2017-09-26T14:42:50.213658: step 6074, loss 0.000565908, acc 1\n",
      "2017-09-26T14:42:50.470386: step 6075, loss 0.000244392, acc 1\n",
      "2017-09-26T14:42:50.715272: step 6076, loss 0.00261494, acc 1\n",
      "2017-09-26T14:42:50.981187: step 6077, loss 0.000154757, acc 1\n",
      "2017-09-26T14:42:51.229073: step 6078, loss 0.00175795, acc 1\n",
      "2017-09-26T14:42:51.472985: step 6079, loss 3.55724e-05, acc 1\n",
      "2017-09-26T14:42:51.711994: step 6080, loss 0.000801312, acc 1\n",
      "2017-09-26T14:42:51.961513: step 6081, loss 0.000177788, acc 1\n",
      "2017-09-26T14:42:52.220195: step 6082, loss 0.000304571, acc 1\n",
      "2017-09-26T14:42:52.515733: step 6083, loss 0.00245654, acc 1\n",
      "2017-09-26T14:42:52.769504: step 6084, loss 0.000185478, acc 1\n",
      "2017-09-26T14:42:53.004810: step 6085, loss 0.000282341, acc 1\n",
      "2017-09-26T14:42:53.255305: step 6086, loss 0.000427139, acc 1\n",
      "2017-09-26T14:42:53.512392: step 6087, loss 0.000443157, acc 1\n",
      "2017-09-26T14:42:53.777820: step 6088, loss 0.000537862, acc 1\n",
      "2017-09-26T14:42:54.033222: step 6089, loss 0.000916425, acc 1\n",
      "2017-09-26T14:42:54.240133: step 6090, loss 0.00423033, acc 1\n",
      "2017-09-26T14:42:54.489057: step 6091, loss 0.00193272, acc 1\n",
      "2017-09-26T14:42:54.739513: step 6092, loss 0.00016999, acc 1\n",
      "2017-09-26T14:42:54.974658: step 6093, loss 0.000581989, acc 1\n",
      "2017-09-26T14:42:55.227449: step 6094, loss 0.000811307, acc 1\n",
      "2017-09-26T14:42:55.472267: step 6095, loss 0.00028007, acc 1\n",
      "2017-09-26T14:42:55.707278: step 6096, loss 0.000279677, acc 1\n",
      "2017-09-26T14:42:55.957761: step 6097, loss 0.000846803, acc 1\n",
      "2017-09-26T14:42:56.197749: step 6098, loss 0.000260053, acc 1\n",
      "2017-09-26T14:42:56.429181: step 6099, loss 0.000424321, acc 1\n",
      "2017-09-26T14:42:56.672862: step 6100, loss 0.000263612, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:42:56.920486: step 6100, loss 0.737456, acc 0.875421\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6100\n",
      "\n",
      "2017-09-26T14:42:57.448021: step 6101, loss 0.000915357, acc 1\n",
      "2017-09-26T14:42:57.719444: step 6102, loss 0.000131691, acc 1\n",
      "2017-09-26T14:42:57.971302: step 6103, loss 0.00032334, acc 1\n",
      "2017-09-26T14:42:58.202793: step 6104, loss 0.0319479, acc 0.984375\n",
      "2017-09-26T14:42:58.458893: step 6105, loss 0.000456767, acc 1\n",
      "2017-09-26T14:42:58.702854: step 6106, loss 0.000546362, acc 1\n",
      "2017-09-26T14:42:58.961729: step 6107, loss 0.000135591, acc 1\n",
      "2017-09-26T14:42:59.194431: step 6108, loss 0.000241296, acc 1\n",
      "2017-09-26T14:42:59.441942: step 6109, loss 0.000259045, acc 1\n",
      "2017-09-26T14:42:59.682669: step 6110, loss 0.000338461, acc 1\n",
      "2017-09-26T14:42:59.916746: step 6111, loss 0.0016571, acc 1\n",
      "2017-09-26T14:43:00.172205: step 6112, loss 0.00338988, acc 1\n",
      "2017-09-26T14:43:00.447994: step 6113, loss 0.000112006, acc 1\n",
      "2017-09-26T14:43:00.683553: step 6114, loss 0.00319682, acc 1\n",
      "2017-09-26T14:43:00.931422: step 6115, loss 0.000268732, acc 1\n",
      "2017-09-26T14:43:01.195612: step 6116, loss 0.000231948, acc 1\n",
      "2017-09-26T14:43:01.510413: step 6117, loss 0.00240432, acc 1\n",
      "2017-09-26T14:43:01.834097: step 6118, loss 0.100503, acc 0.984375\n",
      "2017-09-26T14:43:02.169057: step 6119, loss 0.00077994, acc 1\n",
      "2017-09-26T14:43:02.498640: step 6120, loss 0.000210333, acc 1\n",
      "2017-09-26T14:43:02.831871: step 6121, loss 0.000261441, acc 1\n",
      "2017-09-26T14:43:03.161382: step 6122, loss 0.000560838, acc 1\n",
      "2017-09-26T14:43:03.507475: step 6123, loss 0.000385242, acc 1\n",
      "2017-09-26T14:43:03.824959: step 6124, loss 0.000264158, acc 1\n",
      "2017-09-26T14:43:04.127660: step 6125, loss 2.63118e-05, acc 1\n",
      "2017-09-26T14:43:04.439188: step 6126, loss 0.000216098, acc 1\n",
      "2017-09-26T14:43:04.758088: step 6127, loss 0.000291194, acc 1\n",
      "2017-09-26T14:43:05.048077: step 6128, loss 0.000308116, acc 1\n",
      "2017-09-26T14:43:05.307370: step 6129, loss 0.000590816, acc 1\n",
      "2017-09-26T14:43:05.549078: step 6130, loss 0.000329564, acc 1\n",
      "2017-09-26T14:43:05.792169: step 6131, loss 0.00012755, acc 1\n",
      "2017-09-26T14:43:06.014060: step 6132, loss 0.000253409, acc 1\n",
      "2017-09-26T14:43:06.296654: step 6133, loss 0.000105319, acc 1\n",
      "2017-09-26T14:43:06.618629: step 6134, loss 0.000105669, acc 1\n",
      "2017-09-26T14:43:06.935950: step 6135, loss 0.000286155, acc 1\n",
      "2017-09-26T14:43:07.273123: step 6136, loss 0.00546506, acc 1\n",
      "2017-09-26T14:43:07.550978: step 6137, loss 0.000104952, acc 1\n",
      "2017-09-26T14:43:07.937679: step 6138, loss 0.00278186, acc 1\n",
      "2017-09-26T14:43:08.268724: step 6139, loss 0.000257879, acc 1\n",
      "2017-09-26T14:43:08.531170: step 6140, loss 0.00166226, acc 1\n",
      "2017-09-26T14:43:08.801015: step 6141, loss 0.000923354, acc 1\n",
      "2017-09-26T14:43:09.051288: step 6142, loss 0.00010485, acc 1\n",
      "2017-09-26T14:43:09.326585: step 6143, loss 0.0757484, acc 0.984375\n",
      "2017-09-26T14:43:09.642909: step 6144, loss 0.000186618, acc 1\n",
      "2017-09-26T14:43:09.932475: step 6145, loss 0.000100146, acc 1\n",
      "2017-09-26T14:43:10.199298: step 6146, loss 6.61809e-05, acc 1\n",
      "2017-09-26T14:43:10.466178: step 6147, loss 6.37316e-05, acc 1\n",
      "2017-09-26T14:43:10.718035: step 6148, loss 0.000157112, acc 1\n",
      "2017-09-26T14:43:10.979113: step 6149, loss 0.00967594, acc 1\n",
      "2017-09-26T14:43:11.235396: step 6150, loss 7.99767e-05, acc 1\n",
      "2017-09-26T14:43:11.477228: step 6151, loss 0.0388185, acc 0.984375\n",
      "2017-09-26T14:43:11.746589: step 6152, loss 8.85578e-05, acc 1\n",
      "2017-09-26T14:43:11.978909: step 6153, loss 0.00186182, acc 1\n",
      "2017-09-26T14:43:12.255010: step 6154, loss 0.00570114, acc 1\n",
      "2017-09-26T14:43:12.519926: step 6155, loss 0.00114734, acc 1\n",
      "2017-09-26T14:43:12.799217: step 6156, loss 0.00538144, acc 1\n",
      "2017-09-26T14:43:13.076131: step 6157, loss 0.000336642, acc 1\n",
      "2017-09-26T14:43:13.350963: step 6158, loss 0.000322478, acc 1\n",
      "2017-09-26T14:43:13.647490: step 6159, loss 0.00122434, acc 1\n",
      "2017-09-26T14:43:13.917603: step 6160, loss 0.000754628, acc 1\n",
      "2017-09-26T14:43:14.185954: step 6161, loss 0.0350885, acc 0.984375\n",
      "2017-09-26T14:43:14.456716: step 6162, loss 0.000562857, acc 1\n",
      "2017-09-26T14:43:14.755123: step 6163, loss 0.00180187, acc 1\n",
      "2017-09-26T14:43:15.092758: step 6164, loss 0.0020645, acc 1\n",
      "2017-09-26T14:43:15.404481: step 6165, loss 0.000235164, acc 1\n",
      "2017-09-26T14:43:15.709986: step 6166, loss 0.000288709, acc 1\n",
      "2017-09-26T14:43:16.050818: step 6167, loss 7.95695e-05, acc 1\n",
      "2017-09-26T14:43:16.296244: step 6168, loss 0.000249494, acc 1\n",
      "2017-09-26T14:43:16.630341: step 6169, loss 0.000600414, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:43:16.930733: step 6170, loss 0.00263792, acc 1\n",
      "2017-09-26T14:43:17.219517: step 6171, loss 0.011911, acc 0.984375\n",
      "2017-09-26T14:43:17.537443: step 6172, loss 0.000163879, acc 1\n",
      "2017-09-26T14:43:17.783029: step 6173, loss 0.000302425, acc 1\n",
      "2017-09-26T14:43:17.986033: step 6174, loss 0.000484741, acc 1\n",
      "2017-09-26T14:43:18.235622: step 6175, loss 0.00117214, acc 1\n",
      "2017-09-26T14:43:18.533708: step 6176, loss 0.00708039, acc 1\n",
      "2017-09-26T14:43:18.882046: step 6177, loss 0.0171303, acc 0.984375\n",
      "2017-09-26T14:43:19.240974: step 6178, loss 0.00056852, acc 1\n",
      "2017-09-26T14:43:19.502802: step 6179, loss 0.0154494, acc 0.984375\n",
      "2017-09-26T14:43:19.772693: step 6180, loss 0.0154898, acc 0.984375\n",
      "2017-09-26T14:43:20.130917: step 6181, loss 0.00032066, acc 1\n",
      "2017-09-26T14:43:20.418153: step 6182, loss 0.000279069, acc 1\n",
      "2017-09-26T14:43:20.691139: step 6183, loss 0.0906354, acc 0.984375\n",
      "2017-09-26T14:43:20.957273: step 6184, loss 0.000129302, acc 1\n",
      "2017-09-26T14:43:21.230315: step 6185, loss 0.000649582, acc 1\n",
      "2017-09-26T14:43:21.556069: step 6186, loss 0.00135858, acc 1\n",
      "2017-09-26T14:43:21.900624: step 6187, loss 0.000741579, acc 1\n",
      "2017-09-26T14:43:22.214016: step 6188, loss 0.000411002, acc 1\n",
      "2017-09-26T14:43:22.443235: step 6189, loss 0.000416417, acc 1\n",
      "2017-09-26T14:43:22.691210: step 6190, loss 0.000215751, acc 1\n",
      "2017-09-26T14:43:22.984223: step 6191, loss 0.0211256, acc 0.984375\n",
      "2017-09-26T14:43:23.290955: step 6192, loss 0.000489747, acc 1\n",
      "2017-09-26T14:43:23.567597: step 6193, loss 0.000804692, acc 1\n",
      "2017-09-26T14:43:23.911339: step 6194, loss 0.000968797, acc 1\n",
      "2017-09-26T14:43:24.244421: step 6195, loss 0.000102891, acc 1\n",
      "2017-09-26T14:43:24.510364: step 6196, loss 0.00037526, acc 1\n",
      "2017-09-26T14:43:24.851588: step 6197, loss 0.000352139, acc 1\n",
      "2017-09-26T14:43:25.220621: step 6198, loss 0.000219847, acc 1\n",
      "2017-09-26T14:43:25.558597: step 6199, loss 0.00921554, acc 1\n",
      "2017-09-26T14:43:25.813639: step 6200, loss 0.00132187, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:43:26.121303: step 6200, loss 0.833666, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6200\n",
      "\n",
      "2017-09-26T14:43:26.792972: step 6201, loss 0.00839307, acc 1\n",
      "2017-09-26T14:43:27.051266: step 6202, loss 0.000179043, acc 1\n",
      "2017-09-26T14:43:27.319297: step 6203, loss 0.000836504, acc 1\n",
      "2017-09-26T14:43:27.590506: step 6204, loss 0.00043059, acc 1\n",
      "2017-09-26T14:43:27.824632: step 6205, loss 0.00375579, acc 1\n",
      "2017-09-26T14:43:28.077450: step 6206, loss 0.000424742, acc 1\n",
      "2017-09-26T14:43:28.316380: step 6207, loss 0.00258718, acc 1\n",
      "2017-09-26T14:43:28.557049: step 6208, loss 0.000607331, acc 1\n",
      "2017-09-26T14:43:28.808161: step 6209, loss 0.00011041, acc 1\n",
      "2017-09-26T14:43:29.048236: step 6210, loss 0.00147375, acc 1\n",
      "2017-09-26T14:43:29.305445: step 6211, loss 0.000165548, acc 1\n",
      "2017-09-26T14:43:29.570738: step 6212, loss 0.00026759, acc 1\n",
      "2017-09-26T14:43:29.822379: step 6213, loss 0.00278712, acc 1\n",
      "2017-09-26T14:43:30.102847: step 6214, loss 0.00159605, acc 1\n",
      "2017-09-26T14:43:30.382437: step 6215, loss 0.000210037, acc 1\n",
      "2017-09-26T14:43:30.606986: step 6216, loss 0.00014223, acc 1\n",
      "2017-09-26T14:43:30.908091: step 6217, loss 0.000144333, acc 1\n",
      "2017-09-26T14:43:31.211230: step 6218, loss 0.00117611, acc 1\n",
      "2017-09-26T14:43:31.489754: step 6219, loss 0.000583969, acc 1\n",
      "2017-09-26T14:43:31.786023: step 6220, loss 0.000202031, acc 1\n",
      "2017-09-26T14:43:32.056841: step 6221, loss 0.000492913, acc 1\n",
      "2017-09-26T14:43:32.316125: step 6222, loss 0.000396488, acc 1\n",
      "2017-09-26T14:43:32.584493: step 6223, loss 0.000723797, acc 1\n",
      "2017-09-26T14:43:32.848433: step 6224, loss 0.000316137, acc 1\n",
      "2017-09-26T14:43:33.106032: step 6225, loss 0.000473808, acc 1\n",
      "2017-09-26T14:43:33.372728: step 6226, loss 0.0107365, acc 1\n",
      "2017-09-26T14:43:33.650580: step 6227, loss 9.83676e-05, acc 1\n",
      "2017-09-26T14:43:33.926259: step 6228, loss 0.0475326, acc 0.984375\n",
      "2017-09-26T14:43:34.184782: step 6229, loss 0.000245411, acc 1\n",
      "2017-09-26T14:43:34.466815: step 6230, loss 0.000594973, acc 1\n",
      "2017-09-26T14:43:34.816154: step 6231, loss 0.0676395, acc 0.984375\n",
      "2017-09-26T14:43:35.162172: step 6232, loss 0.000152924, acc 1\n",
      "2017-09-26T14:43:35.415350: step 6233, loss 0.000134781, acc 1\n",
      "2017-09-26T14:43:35.678273: step 6234, loss 0.00900146, acc 1\n",
      "2017-09-26T14:43:35.937102: step 6235, loss 0.0201091, acc 0.984375\n",
      "2017-09-26T14:43:36.205792: step 6236, loss 0.000330549, acc 1\n",
      "2017-09-26T14:43:36.463194: step 6237, loss 0.000157933, acc 1\n",
      "2017-09-26T14:43:36.721984: step 6238, loss 2.06229e-05, acc 1\n",
      "2017-09-26T14:43:36.991754: step 6239, loss 0.000129746, acc 1\n",
      "2017-09-26T14:43:37.243290: step 6240, loss 0.000724928, acc 1\n",
      "2017-09-26T14:43:37.529428: step 6241, loss 0.000915497, acc 1\n",
      "2017-09-26T14:43:37.791447: step 6242, loss 0.00753018, acc 1\n",
      "2017-09-26T14:43:38.044459: step 6243, loss 0.000236722, acc 1\n",
      "2017-09-26T14:43:38.286911: step 6244, loss 0.00254905, acc 1\n",
      "2017-09-26T14:43:38.531597: step 6245, loss 0.00124295, acc 1\n",
      "2017-09-26T14:43:38.778507: step 6246, loss 0.000142607, acc 1\n",
      "2017-09-26T14:43:39.019275: step 6247, loss 0.000188498, acc 1\n",
      "2017-09-26T14:43:39.267583: step 6248, loss 0.0099086, acc 1\n",
      "2017-09-26T14:43:39.507394: step 6249, loss 0.00564514, acc 1\n",
      "2017-09-26T14:43:39.756307: step 6250, loss 0.0255931, acc 0.984375\n",
      "2017-09-26T14:43:39.997698: step 6251, loss 0.00276283, acc 1\n",
      "2017-09-26T14:43:40.243348: step 6252, loss 0.00947189, acc 1\n",
      "2017-09-26T14:43:40.493129: step 6253, loss 0.00443559, acc 1\n",
      "2017-09-26T14:43:40.731320: step 6254, loss 0.000158847, acc 1\n",
      "2017-09-26T14:43:40.980528: step 6255, loss 0.00133871, acc 1\n",
      "2017-09-26T14:43:41.226479: step 6256, loss 0.00080059, acc 1\n",
      "2017-09-26T14:43:41.469279: step 6257, loss 0.000531441, acc 1\n",
      "2017-09-26T14:43:41.683745: step 6258, loss 0.000586225, acc 1\n",
      "2017-09-26T14:43:41.936925: step 6259, loss 0.000916063, acc 1\n",
      "2017-09-26T14:43:42.174386: step 6260, loss 0.000807853, acc 1\n",
      "2017-09-26T14:43:42.425822: step 6261, loss 0.000397386, acc 1\n",
      "2017-09-26T14:43:42.666089: step 6262, loss 0.000740896, acc 1\n",
      "2017-09-26T14:43:42.913813: step 6263, loss 0.00101753, acc 1\n",
      "2017-09-26T14:43:43.153538: step 6264, loss 0.000288604, acc 1\n",
      "2017-09-26T14:43:43.395453: step 6265, loss 0.123887, acc 0.984375\n",
      "2017-09-26T14:43:43.637889: step 6266, loss 0.000868533, acc 1\n",
      "2017-09-26T14:43:43.879079: step 6267, loss 0.000222375, acc 1\n",
      "2017-09-26T14:43:44.143327: step 6268, loss 0.000105347, acc 1\n",
      "2017-09-26T14:43:44.394526: step 6269, loss 0.000240627, acc 1\n",
      "2017-09-26T14:43:44.648695: step 6270, loss 0.00337642, acc 1\n",
      "2017-09-26T14:43:44.891373: step 6271, loss 8.19431e-05, acc 1\n",
      "2017-09-26T14:43:45.136936: step 6272, loss 0.00839061, acc 1\n",
      "2017-09-26T14:43:45.383287: step 6273, loss 0.00365185, acc 1\n",
      "2017-09-26T14:43:45.633379: step 6274, loss 0.000131822, acc 1\n",
      "2017-09-26T14:43:45.878044: step 6275, loss 0.000246181, acc 1\n",
      "2017-09-26T14:43:46.143942: step 6276, loss 0.00192569, acc 1\n",
      "2017-09-26T14:43:46.399227: step 6277, loss 0.000273308, acc 1\n",
      "2017-09-26T14:43:46.655810: step 6278, loss 6.11339e-05, acc 1\n",
      "2017-09-26T14:43:46.896443: step 6279, loss 0.000734767, acc 1\n",
      "2017-09-26T14:43:47.144107: step 6280, loss 0.00369353, acc 1\n",
      "2017-09-26T14:43:47.389604: step 6281, loss 4.11484e-05, acc 1\n",
      "2017-09-26T14:43:47.634014: step 6282, loss 0.00156334, acc 1\n",
      "2017-09-26T14:43:47.883379: step 6283, loss 0.000289247, acc 1\n",
      "2017-09-26T14:43:48.126059: step 6284, loss 0.000652802, acc 1\n",
      "2017-09-26T14:43:48.378278: step 6285, loss 0.00726951, acc 1\n",
      "2017-09-26T14:43:48.626590: step 6286, loss 0.000233961, acc 1\n",
      "2017-09-26T14:43:48.874529: step 6287, loss 0.000780922, acc 1\n",
      "2017-09-26T14:43:49.118571: step 6288, loss 0.000238015, acc 1\n",
      "2017-09-26T14:43:49.383686: step 6289, loss 6.72884e-05, acc 1\n",
      "2017-09-26T14:43:49.645265: step 6290, loss 0.000194898, acc 1\n",
      "2017-09-26T14:43:49.890534: step 6291, loss 0.000356599, acc 1\n",
      "2017-09-26T14:43:50.133150: step 6292, loss 0.00040259, acc 1\n",
      "2017-09-26T14:43:50.385142: step 6293, loss 0.000265543, acc 1\n",
      "2017-09-26T14:43:50.626685: step 6294, loss 0.00111595, acc 1\n",
      "2017-09-26T14:43:50.873694: step 6295, loss 0.000215573, acc 1\n",
      "2017-09-26T14:43:51.119976: step 6296, loss 0.003512, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:43:51.367727: step 6297, loss 1.497e-05, acc 1\n",
      "2017-09-26T14:43:51.611182: step 6298, loss 0.000493667, acc 1\n",
      "2017-09-26T14:43:51.842497: step 6299, loss 0.000146637, acc 1\n",
      "2017-09-26T14:43:52.048358: step 6300, loss 0.00017695, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:43:52.280314: step 6300, loss 0.753715, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6300\n",
      "\n",
      "2017-09-26T14:43:52.778020: step 6301, loss 5.80944e-05, acc 1\n",
      "2017-09-26T14:43:53.019015: step 6302, loss 0.000530857, acc 1\n",
      "2017-09-26T14:43:53.251901: step 6303, loss 0.000437938, acc 1\n",
      "2017-09-26T14:43:53.488682: step 6304, loss 0.000446006, acc 1\n",
      "2017-09-26T14:43:53.724800: step 6305, loss 0.00301821, acc 1\n",
      "2017-09-26T14:43:53.954471: step 6306, loss 0.000301787, acc 1\n",
      "2017-09-26T14:43:54.189755: step 6307, loss 0.000236804, acc 1\n",
      "2017-09-26T14:43:54.475753: step 6308, loss 0.00016403, acc 1\n",
      "2017-09-26T14:43:54.753447: step 6309, loss 0.000781191, acc 1\n",
      "2017-09-26T14:43:55.012317: step 6310, loss 4.38946e-05, acc 1\n",
      "2017-09-26T14:43:55.258824: step 6311, loss 0.015736, acc 0.984375\n",
      "2017-09-26T14:43:55.489003: step 6312, loss 0.000146221, acc 1\n",
      "2017-09-26T14:43:55.723771: step 6313, loss 4.86671e-05, acc 1\n",
      "2017-09-26T14:43:55.957896: step 6314, loss 0.000111451, acc 1\n",
      "2017-09-26T14:43:56.206029: step 6315, loss 0.00359728, acc 1\n",
      "2017-09-26T14:43:56.443883: step 6316, loss 3.45562e-05, acc 1\n",
      "2017-09-26T14:43:56.684563: step 6317, loss 0.00223108, acc 1\n",
      "2017-09-26T14:43:56.917356: step 6318, loss 0.00293681, acc 1\n",
      "2017-09-26T14:43:57.177597: step 6319, loss 0.000435345, acc 1\n",
      "2017-09-26T14:43:57.409971: step 6320, loss 0.000762239, acc 1\n",
      "2017-09-26T14:43:57.640647: step 6321, loss 0.00476582, acc 1\n",
      "2017-09-26T14:43:57.881309: step 6322, loss 0.0333312, acc 0.984375\n",
      "2017-09-26T14:43:58.111149: step 6323, loss 7.01688e-05, acc 1\n",
      "2017-09-26T14:43:58.354536: step 6324, loss 0.000800677, acc 1\n",
      "2017-09-26T14:43:58.596361: step 6325, loss 0.000276606, acc 1\n",
      "2017-09-26T14:43:58.832727: step 6326, loss 0.000160269, acc 1\n",
      "2017-09-26T14:43:59.066344: step 6327, loss 7.23334e-05, acc 1\n",
      "2017-09-26T14:43:59.296334: step 6328, loss 0.000214704, acc 1\n",
      "2017-09-26T14:43:59.536447: step 6329, loss 5.72338e-05, acc 1\n",
      "2017-09-26T14:43:59.768001: step 6330, loss 0.000367429, acc 1\n",
      "2017-09-26T14:44:00.005533: step 6331, loss 0.0685263, acc 0.984375\n",
      "2017-09-26T14:44:00.247690: step 6332, loss 0.000569554, acc 1\n",
      "2017-09-26T14:44:00.487559: step 6333, loss 7.36791e-05, acc 1\n",
      "2017-09-26T14:44:00.720928: step 6334, loss 0.00109233, acc 1\n",
      "2017-09-26T14:44:00.987265: step 6335, loss 0.00718814, acc 1\n",
      "2017-09-26T14:44:01.234121: step 6336, loss 0.0103523, acc 1\n",
      "2017-09-26T14:44:01.556344: step 6337, loss 0.00918241, acc 1\n",
      "2017-09-26T14:44:01.850619: step 6338, loss 0.000190575, acc 1\n",
      "2017-09-26T14:44:02.160793: step 6339, loss 0.000152011, acc 1\n",
      "2017-09-26T14:44:02.445767: step 6340, loss 0.000111487, acc 1\n",
      "2017-09-26T14:44:02.714114: step 6341, loss 0.00347368, acc 1\n",
      "2017-09-26T14:44:02.945593: step 6342, loss 1.03108e-05, acc 1\n",
      "2017-09-26T14:44:03.203979: step 6343, loss 0.000788271, acc 1\n",
      "2017-09-26T14:44:03.436681: step 6344, loss 0.00173237, acc 1\n",
      "2017-09-26T14:44:03.674008: step 6345, loss 0.00481447, acc 1\n",
      "2017-09-26T14:44:03.909947: step 6346, loss 0.000794052, acc 1\n",
      "2017-09-26T14:44:04.149840: step 6347, loss 0.0120259, acc 1\n",
      "2017-09-26T14:44:04.379511: step 6348, loss 6.00519e-05, acc 1\n",
      "2017-09-26T14:44:04.616834: step 6349, loss 0.0454816, acc 0.984375\n",
      "2017-09-26T14:44:04.854754: step 6350, loss 8.65966e-05, acc 1\n",
      "2017-09-26T14:44:05.087556: step 6351, loss 0.000619559, acc 1\n",
      "2017-09-26T14:44:05.323824: step 6352, loss 0.00019209, acc 1\n",
      "2017-09-26T14:44:05.556648: step 6353, loss 0.000127135, acc 1\n",
      "2017-09-26T14:44:05.799259: step 6354, loss 7.42615e-05, acc 1\n",
      "2017-09-26T14:44:06.036894: step 6355, loss 0.00214824, acc 1\n",
      "2017-09-26T14:44:06.282572: step 6356, loss 0.00851919, acc 1\n",
      "2017-09-26T14:44:06.512980: step 6357, loss 0.000405491, acc 1\n",
      "2017-09-26T14:44:06.749690: step 6358, loss 0.000197589, acc 1\n",
      "2017-09-26T14:44:06.987201: step 6359, loss 0.0189522, acc 0.984375\n",
      "2017-09-26T14:44:07.225655: step 6360, loss 0.000118534, acc 1\n",
      "2017-09-26T14:44:07.465346: step 6361, loss 0.000739107, acc 1\n",
      "2017-09-26T14:44:07.702313: step 6362, loss 7.23523e-05, acc 1\n",
      "2017-09-26T14:44:07.944816: step 6363, loss 8.37053e-05, acc 1\n",
      "2017-09-26T14:44:08.200273: step 6364, loss 0.00827991, acc 1\n",
      "2017-09-26T14:44:08.437909: step 6365, loss 0.000105697, acc 1\n",
      "2017-09-26T14:44:08.670634: step 6366, loss 0.00022115, acc 1\n",
      "2017-09-26T14:44:08.909500: step 6367, loss 0.000296408, acc 1\n",
      "2017-09-26T14:44:09.140777: step 6368, loss 9.19024e-05, acc 1\n",
      "2017-09-26T14:44:09.376296: step 6369, loss 0.000868738, acc 1\n",
      "2017-09-26T14:44:09.615066: step 6370, loss 0.000122548, acc 1\n",
      "2017-09-26T14:44:09.855064: step 6371, loss 0.00149822, acc 1\n",
      "2017-09-26T14:44:10.093351: step 6372, loss 7.25779e-05, acc 1\n",
      "2017-09-26T14:44:10.326970: step 6373, loss 0.000206917, acc 1\n",
      "2017-09-26T14:44:10.569096: step 6374, loss 0.0362947, acc 0.984375\n",
      "2017-09-26T14:44:10.815688: step 6375, loss 0.000108289, acc 1\n",
      "2017-09-26T14:44:11.082001: step 6376, loss 0.000227028, acc 1\n",
      "2017-09-26T14:44:11.314468: step 6377, loss 0.000450817, acc 1\n",
      "2017-09-26T14:44:11.574205: step 6378, loss 0.00041998, acc 1\n",
      "2017-09-26T14:44:11.817451: step 6379, loss 0.001227, acc 1\n",
      "2017-09-26T14:44:12.059229: step 6380, loss 0.000635698, acc 1\n",
      "2017-09-26T14:44:12.294253: step 6381, loss 0.000245368, acc 1\n",
      "2017-09-26T14:44:12.540707: step 6382, loss 0.000279464, acc 1\n",
      "2017-09-26T14:44:12.779020: step 6383, loss 0.00104534, acc 1\n",
      "2017-09-26T14:44:12.988416: step 6384, loss 7.1145e-05, acc 1\n",
      "2017-09-26T14:44:13.262413: step 6385, loss 0.000123653, acc 1\n",
      "2017-09-26T14:44:13.524758: step 6386, loss 0.00168685, acc 1\n",
      "2017-09-26T14:44:13.785953: step 6387, loss 0.000317515, acc 1\n",
      "2017-09-26T14:44:14.044513: step 6388, loss 0.000199857, acc 1\n",
      "2017-09-26T14:44:14.284725: step 6389, loss 0.00642151, acc 1\n",
      "2017-09-26T14:44:14.562493: step 6390, loss 0.000301003, acc 1\n",
      "2017-09-26T14:44:14.821175: step 6391, loss 0.00132264, acc 1\n",
      "2017-09-26T14:44:15.058011: step 6392, loss 0.0891959, acc 0.984375\n",
      "2017-09-26T14:44:15.314162: step 6393, loss 0.0089498, acc 1\n",
      "2017-09-26T14:44:15.584121: step 6394, loss 0.0103847, acc 1\n",
      "2017-09-26T14:44:15.843229: step 6395, loss 0.000116729, acc 1\n",
      "2017-09-26T14:44:16.121159: step 6396, loss 0.000220727, acc 1\n",
      "2017-09-26T14:44:16.377342: step 6397, loss 0.000215362, acc 1\n",
      "2017-09-26T14:44:16.614871: step 6398, loss 0.0229791, acc 0.984375\n",
      "2017-09-26T14:44:16.855350: step 6399, loss 0.000749353, acc 1\n",
      "2017-09-26T14:44:17.104898: step 6400, loss 0.000351651, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:44:17.379689: step 6400, loss 0.720746, acc 0.878788\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6400\n",
      "\n",
      "2017-09-26T14:44:17.884298: step 6401, loss 0.000270123, acc 1\n",
      "2017-09-26T14:44:18.117764: step 6402, loss 0.000167706, acc 1\n",
      "2017-09-26T14:44:18.360334: step 6403, loss 0.0013251, acc 1\n",
      "2017-09-26T14:44:18.601644: step 6404, loss 0.000257964, acc 1\n",
      "2017-09-26T14:44:18.837248: step 6405, loss 0.00262214, acc 1\n",
      "2017-09-26T14:44:19.079083: step 6406, loss 0.0109667, acc 1\n",
      "2017-09-26T14:44:19.313789: step 6407, loss 0.000470564, acc 1\n",
      "2017-09-26T14:44:19.556579: step 6408, loss 0.000158508, acc 1\n",
      "2017-09-26T14:44:19.794389: step 6409, loss 0.000325433, acc 1\n",
      "2017-09-26T14:44:20.035604: step 6410, loss 0.00120831, acc 1\n",
      "2017-09-26T14:44:20.275956: step 6411, loss 0.00031794, acc 1\n",
      "2017-09-26T14:44:20.514998: step 6412, loss 0.000583976, acc 1\n",
      "2017-09-26T14:44:20.757170: step 6413, loss 0.000132193, acc 1\n",
      "2017-09-26T14:44:21.006296: step 6414, loss 6.131e-05, acc 1\n",
      "2017-09-26T14:44:21.267551: step 6415, loss 0.00141593, acc 1\n",
      "2017-09-26T14:44:21.526563: step 6416, loss 8.04127e-05, acc 1\n",
      "2017-09-26T14:44:21.791339: step 6417, loss 0.0672928, acc 0.984375\n",
      "2017-09-26T14:44:22.032628: step 6418, loss 0.000393682, acc 1\n",
      "2017-09-26T14:44:22.284162: step 6419, loss 0.0296183, acc 0.984375\n",
      "2017-09-26T14:44:22.520709: step 6420, loss 0.00791709, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:44:22.762079: step 6421, loss 0.000571974, acc 1\n",
      "2017-09-26T14:44:23.000235: step 6422, loss 0.000116392, acc 1\n",
      "2017-09-26T14:44:23.233045: step 6423, loss 0.000935593, acc 1\n",
      "2017-09-26T14:44:23.463555: step 6424, loss 0.00103337, acc 1\n",
      "2017-09-26T14:44:23.694934: step 6425, loss 0.000134826, acc 1\n",
      "2017-09-26T14:44:23.896807: step 6426, loss 0.0089307, acc 1\n",
      "2017-09-26T14:44:24.134690: step 6427, loss 0.0012799, acc 1\n",
      "2017-09-26T14:44:24.368529: step 6428, loss 9.10744e-05, acc 1\n",
      "2017-09-26T14:44:24.603656: step 6429, loss 7.39388e-05, acc 1\n",
      "2017-09-26T14:44:24.839043: step 6430, loss 0.000294465, acc 1\n",
      "2017-09-26T14:44:25.065966: step 6431, loss 0.00595329, acc 1\n",
      "2017-09-26T14:44:25.307877: step 6432, loss 0.000603119, acc 1\n",
      "2017-09-26T14:44:25.538159: step 6433, loss 0.000127301, acc 1\n",
      "2017-09-26T14:44:25.774941: step 6434, loss 0.000400495, acc 1\n",
      "2017-09-26T14:44:26.010779: step 6435, loss 0.000856008, acc 1\n",
      "2017-09-26T14:44:26.250270: step 6436, loss 0.0122326, acc 0.984375\n",
      "2017-09-26T14:44:26.492742: step 6437, loss 0.00325764, acc 1\n",
      "2017-09-26T14:44:26.722811: step 6438, loss 0.000860725, acc 1\n",
      "2017-09-26T14:44:26.957225: step 6439, loss 0.00012499, acc 1\n",
      "2017-09-26T14:44:27.188695: step 6440, loss 0.000287722, acc 1\n",
      "2017-09-26T14:44:27.438821: step 6441, loss 0.000200098, acc 1\n",
      "2017-09-26T14:44:27.674178: step 6442, loss 9.93711e-05, acc 1\n",
      "2017-09-26T14:44:27.908687: step 6443, loss 5.51504e-05, acc 1\n",
      "2017-09-26T14:44:28.140312: step 6444, loss 0.000143848, acc 1\n",
      "2017-09-26T14:44:28.372160: step 6445, loss 0.00238171, acc 1\n",
      "2017-09-26T14:44:28.609534: step 6446, loss 0.00543057, acc 1\n",
      "2017-09-26T14:44:28.839144: step 6447, loss 0.000474463, acc 1\n",
      "2017-09-26T14:44:29.078481: step 6448, loss 0.000634543, acc 1\n",
      "2017-09-26T14:44:29.313523: step 6449, loss 0.000148283, acc 1\n",
      "2017-09-26T14:44:29.569532: step 6450, loss 0.00103885, acc 1\n",
      "2017-09-26T14:44:29.799190: step 6451, loss 0.000569315, acc 1\n",
      "2017-09-26T14:44:30.032331: step 6452, loss 2.96734e-05, acc 1\n",
      "2017-09-26T14:44:30.270964: step 6453, loss 9.89883e-05, acc 1\n",
      "2017-09-26T14:44:30.502577: step 6454, loss 0.000144114, acc 1\n",
      "2017-09-26T14:44:30.756466: step 6455, loss 0.0839044, acc 0.984375\n",
      "2017-09-26T14:44:30.987586: step 6456, loss 0.00089421, acc 1\n",
      "2017-09-26T14:44:31.223950: step 6457, loss 0.000244641, acc 1\n",
      "2017-09-26T14:44:31.457296: step 6458, loss 0.000117381, acc 1\n",
      "2017-09-26T14:44:31.707923: step 6459, loss 0.000539243, acc 1\n",
      "2017-09-26T14:44:31.945413: step 6460, loss 3.20349e-05, acc 1\n",
      "2017-09-26T14:44:32.190570: step 6461, loss 0.00104211, acc 1\n",
      "2017-09-26T14:44:32.424669: step 6462, loss 0.000282553, acc 1\n",
      "2017-09-26T14:44:32.656843: step 6463, loss 0.000695429, acc 1\n",
      "2017-09-26T14:44:32.891447: step 6464, loss 0.000377986, acc 1\n",
      "2017-09-26T14:44:33.125939: step 6465, loss 0.000186691, acc 1\n",
      "2017-09-26T14:44:33.367011: step 6466, loss 6.28341e-05, acc 1\n",
      "2017-09-26T14:44:33.595184: step 6467, loss 0.087109, acc 0.984375\n",
      "2017-09-26T14:44:33.806324: step 6468, loss 0.000172221, acc 1\n",
      "2017-09-26T14:44:34.049103: step 6469, loss 0.00143229, acc 1\n",
      "2017-09-26T14:44:34.281646: step 6470, loss 0.000191656, acc 1\n",
      "2017-09-26T14:44:34.520329: step 6471, loss 0.000155526, acc 1\n",
      "2017-09-26T14:44:34.753752: step 6472, loss 8.74099e-05, acc 1\n",
      "2017-09-26T14:44:34.994633: step 6473, loss 0.000268957, acc 1\n",
      "2017-09-26T14:44:35.228580: step 6474, loss 0.000356941, acc 1\n",
      "2017-09-26T14:44:35.466930: step 6475, loss 0.00665539, acc 1\n",
      "2017-09-26T14:44:35.745526: step 6476, loss 0.000700374, acc 1\n",
      "2017-09-26T14:44:36.031842: step 6477, loss 0.000162536, acc 1\n",
      "2017-09-26T14:44:36.300019: step 6478, loss 0.0572633, acc 0.984375\n",
      "2017-09-26T14:44:36.557645: step 6479, loss 0.000395562, acc 1\n",
      "2017-09-26T14:44:36.823118: step 6480, loss 0.000801559, acc 1\n",
      "2017-09-26T14:44:37.067201: step 6481, loss 0.000139026, acc 1\n",
      "2017-09-26T14:44:37.297199: step 6482, loss 0.000601093, acc 1\n",
      "2017-09-26T14:44:37.535924: step 6483, loss 6.35849e-05, acc 1\n",
      "2017-09-26T14:44:37.767887: step 6484, loss 0.000329794, acc 1\n",
      "2017-09-26T14:44:38.002959: step 6485, loss 0.00472169, acc 1\n",
      "2017-09-26T14:44:38.238893: step 6486, loss 0.000183416, acc 1\n",
      "2017-09-26T14:44:38.473692: step 6487, loss 0.000983912, acc 1\n",
      "2017-09-26T14:44:38.719443: step 6488, loss 0.000839411, acc 1\n",
      "2017-09-26T14:44:38.956440: step 6489, loss 0.0301577, acc 0.984375\n",
      "2017-09-26T14:44:39.191453: step 6490, loss 0.000310439, acc 1\n",
      "2017-09-26T14:44:39.430196: step 6491, loss 0.000288656, acc 1\n",
      "2017-09-26T14:44:39.671349: step 6492, loss 8.72645e-05, acc 1\n",
      "2017-09-26T14:44:39.909585: step 6493, loss 0.0061784, acc 1\n",
      "2017-09-26T14:44:40.149072: step 6494, loss 0.000341158, acc 1\n",
      "2017-09-26T14:44:40.392371: step 6495, loss 0.000187257, acc 1\n",
      "2017-09-26T14:44:40.629493: step 6496, loss 0.011527, acc 0.984375\n",
      "2017-09-26T14:44:40.866662: step 6497, loss 0.00174542, acc 1\n",
      "2017-09-26T14:44:41.099826: step 6498, loss 0.000432398, acc 1\n",
      "2017-09-26T14:44:41.340049: step 6499, loss 0.00020328, acc 1\n",
      "2017-09-26T14:44:41.576638: step 6500, loss 0.000699663, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:44:41.810813: step 6500, loss 0.786821, acc 0.875421\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6500\n",
      "\n",
      "2017-09-26T14:44:42.307199: step 6501, loss 0.00534436, acc 1\n",
      "2017-09-26T14:44:42.547172: step 6502, loss 0.000754647, acc 1\n",
      "2017-09-26T14:44:42.787570: step 6503, loss 0.00176835, acc 1\n",
      "2017-09-26T14:44:43.024332: step 6504, loss 0.000848303, acc 1\n",
      "2017-09-26T14:44:43.266702: step 6505, loss 0.0231369, acc 0.984375\n",
      "2017-09-26T14:44:43.554513: step 6506, loss 0.000262357, acc 1\n",
      "2017-09-26T14:44:43.846229: step 6507, loss 0.000157329, acc 1\n",
      "2017-09-26T14:44:44.113402: step 6508, loss 0.000723776, acc 1\n",
      "2017-09-26T14:44:44.356954: step 6509, loss 0.000399918, acc 1\n",
      "2017-09-26T14:44:44.571559: step 6510, loss 0.000685568, acc 1\n",
      "2017-09-26T14:44:44.815356: step 6511, loss 0.0469026, acc 0.984375\n",
      "2017-09-26T14:44:45.054878: step 6512, loss 0.00288199, acc 1\n",
      "2017-09-26T14:44:45.370125: step 6513, loss 0.000537038, acc 1\n",
      "2017-09-26T14:44:45.684808: step 6514, loss 7.59243e-05, acc 1\n",
      "2017-09-26T14:44:46.023438: step 6515, loss 0.00110552, acc 1\n",
      "2017-09-26T14:44:46.305895: step 6516, loss 0.000793816, acc 1\n",
      "2017-09-26T14:44:46.634504: step 6517, loss 0.00040868, acc 1\n",
      "2017-09-26T14:44:46.946586: step 6518, loss 0.000437009, acc 1\n",
      "2017-09-26T14:44:47.262636: step 6519, loss 0.00145558, acc 1\n",
      "2017-09-26T14:44:47.554194: step 6520, loss 0.000614778, acc 1\n",
      "2017-09-26T14:44:47.806379: step 6521, loss 0.00447498, acc 1\n",
      "2017-09-26T14:44:48.090622: step 6522, loss 0.000284939, acc 1\n",
      "2017-09-26T14:44:48.340561: step 6523, loss 0.0125366, acc 1\n",
      "2017-09-26T14:44:48.585851: step 6524, loss 0.00011041, acc 1\n",
      "2017-09-26T14:44:48.829707: step 6525, loss 0.00289531, acc 1\n",
      "2017-09-26T14:44:49.132537: step 6526, loss 0.0012802, acc 1\n",
      "2017-09-26T14:44:49.392526: step 6527, loss 0.00376033, acc 1\n",
      "2017-09-26T14:44:49.656389: step 6528, loss 0.000562047, acc 1\n",
      "2017-09-26T14:44:49.906850: step 6529, loss 9.56273e-05, acc 1\n",
      "2017-09-26T14:44:50.158912: step 6530, loss 0.000313702, acc 1\n",
      "2017-09-26T14:44:50.402711: step 6531, loss 0.000170342, acc 1\n",
      "2017-09-26T14:44:50.649452: step 6532, loss 0.0393341, acc 0.984375\n",
      "2017-09-26T14:44:50.893110: step 6533, loss 0.000121716, acc 1\n",
      "2017-09-26T14:44:51.151350: step 6534, loss 0.000125666, acc 1\n",
      "2017-09-26T14:44:51.405804: step 6535, loss 0.00319698, acc 1\n",
      "2017-09-26T14:44:51.646227: step 6536, loss 0.000368413, acc 1\n",
      "2017-09-26T14:44:51.902589: step 6537, loss 0.000245669, acc 1\n",
      "2017-09-26T14:44:52.144940: step 6538, loss 5.76994e-05, acc 1\n",
      "2017-09-26T14:44:52.401313: step 6539, loss 0.00013046, acc 1\n",
      "2017-09-26T14:44:52.652678: step 6540, loss 0.000377405, acc 1\n",
      "2017-09-26T14:44:52.893829: step 6541, loss 6.69172e-05, acc 1\n",
      "2017-09-26T14:44:53.134319: step 6542, loss 0.000186253, acc 1\n",
      "2017-09-26T14:44:53.391063: step 6543, loss 0.000172374, acc 1\n",
      "2017-09-26T14:44:53.632900: step 6544, loss 0.000482193, acc 1\n",
      "2017-09-26T14:44:53.885733: step 6545, loss 1.55219e-05, acc 1\n",
      "2017-09-26T14:44:54.124686: step 6546, loss 0.00057607, acc 1\n",
      "2017-09-26T14:44:54.373176: step 6547, loss 2.55427e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:44:54.626420: step 6548, loss 0.0813235, acc 0.984375\n",
      "2017-09-26T14:44:54.880586: step 6549, loss 0.000710011, acc 1\n",
      "2017-09-26T14:44:55.123808: step 6550, loss 0.0190505, acc 0.984375\n",
      "2017-09-26T14:44:55.358311: step 6551, loss 0.000123011, acc 1\n",
      "2017-09-26T14:44:55.568207: step 6552, loss 5.61067e-05, acc 1\n",
      "2017-09-26T14:44:55.804057: step 6553, loss 0.00318896, acc 1\n",
      "2017-09-26T14:44:56.043299: step 6554, loss 6.54092e-05, acc 1\n",
      "2017-09-26T14:44:56.274197: step 6555, loss 0.000785648, acc 1\n",
      "2017-09-26T14:44:56.530338: step 6556, loss 4.64592e-05, acc 1\n",
      "2017-09-26T14:44:56.764722: step 6557, loss 0.000221867, acc 1\n",
      "2017-09-26T14:44:57.017478: step 6558, loss 0.00167972, acc 1\n",
      "2017-09-26T14:44:57.257220: step 6559, loss 0.000403825, acc 1\n",
      "2017-09-26T14:44:57.495971: step 6560, loss 0.000342011, acc 1\n",
      "2017-09-26T14:44:57.740469: step 6561, loss 0.000323654, acc 1\n",
      "2017-09-26T14:44:57.984132: step 6562, loss 0.000400879, acc 1\n",
      "2017-09-26T14:44:58.223605: step 6563, loss 0.000323049, acc 1\n",
      "2017-09-26T14:44:58.454166: step 6564, loss 0.00113901, acc 1\n",
      "2017-09-26T14:44:58.684925: step 6565, loss 5.13599e-05, acc 1\n",
      "2017-09-26T14:44:58.927622: step 6566, loss 0.000113316, acc 1\n",
      "2017-09-26T14:44:59.157210: step 6567, loss 0.000757736, acc 1\n",
      "2017-09-26T14:44:59.402251: step 6568, loss 0.00110781, acc 1\n",
      "2017-09-26T14:44:59.642798: step 6569, loss 0.00127451, acc 1\n",
      "2017-09-26T14:44:59.881683: step 6570, loss 5.98344e-05, acc 1\n",
      "2017-09-26T14:45:00.139715: step 6571, loss 0.00276428, acc 1\n",
      "2017-09-26T14:45:00.409929: step 6572, loss 0.000106643, acc 1\n",
      "2017-09-26T14:45:00.679839: step 6573, loss 5.13241e-05, acc 1\n",
      "2017-09-26T14:45:00.924527: step 6574, loss 0.0115221, acc 1\n",
      "2017-09-26T14:45:01.163578: step 6575, loss 0.000226603, acc 1\n",
      "2017-09-26T14:45:01.397646: step 6576, loss 0.0002289, acc 1\n",
      "2017-09-26T14:45:01.628102: step 6577, loss 0.000640347, acc 1\n",
      "2017-09-26T14:45:01.866538: step 6578, loss 0.00180816, acc 1\n",
      "2017-09-26T14:45:02.114978: step 6579, loss 5.62352e-05, acc 1\n",
      "2017-09-26T14:45:02.350904: step 6580, loss 0.000162073, acc 1\n",
      "2017-09-26T14:45:02.636734: step 6581, loss 0.000217868, acc 1\n",
      "2017-09-26T14:45:02.877607: step 6582, loss 0.000279528, acc 1\n",
      "2017-09-26T14:45:03.111254: step 6583, loss 0.000140896, acc 1\n",
      "2017-09-26T14:45:03.345429: step 6584, loss 0.000253176, acc 1\n",
      "2017-09-26T14:45:03.596416: step 6585, loss 0.000128381, acc 1\n",
      "2017-09-26T14:45:03.834961: step 6586, loss 0.00011336, acc 1\n",
      "2017-09-26T14:45:04.070509: step 6587, loss 0.00403446, acc 1\n",
      "2017-09-26T14:45:04.303954: step 6588, loss 0.0520381, acc 0.984375\n",
      "2017-09-26T14:45:04.536314: step 6589, loss 0.000313909, acc 1\n",
      "2017-09-26T14:45:04.772766: step 6590, loss 0.000154711, acc 1\n",
      "2017-09-26T14:45:05.016525: step 6591, loss 1.46022e-05, acc 1\n",
      "2017-09-26T14:45:05.278157: step 6592, loss 5.70483e-05, acc 1\n",
      "2017-09-26T14:45:05.523209: step 6593, loss 0.000897129, acc 1\n",
      "2017-09-26T14:45:05.753943: step 6594, loss 0.013936, acc 0.980769\n",
      "2017-09-26T14:45:05.990100: step 6595, loss 0.000108216, acc 1\n",
      "2017-09-26T14:45:06.230682: step 6596, loss 0.00921376, acc 1\n",
      "2017-09-26T14:45:06.471413: step 6597, loss 0.0010742, acc 1\n",
      "2017-09-26T14:45:06.705472: step 6598, loss 0.000131346, acc 1\n",
      "2017-09-26T14:45:06.942173: step 6599, loss 0.00412742, acc 1\n",
      "2017-09-26T14:45:07.192671: step 6600, loss 0.000114104, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:45:07.435409: step 6600, loss 0.764436, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6600\n",
      "\n",
      "2017-09-26T14:45:07.956023: step 6601, loss 0.000272857, acc 1\n",
      "2017-09-26T14:45:08.197744: step 6602, loss 0.00127898, acc 1\n",
      "2017-09-26T14:45:08.438104: step 6603, loss 0.000681684, acc 1\n",
      "2017-09-26T14:45:08.709909: step 6604, loss 0.00488569, acc 1\n",
      "2017-09-26T14:45:08.941461: step 6605, loss 0.000566124, acc 1\n",
      "2017-09-26T14:45:09.181452: step 6606, loss 4.24969e-05, acc 1\n",
      "2017-09-26T14:45:09.431597: step 6607, loss 4.10007e-05, acc 1\n",
      "2017-09-26T14:45:09.664781: step 6608, loss 8.62105e-05, acc 1\n",
      "2017-09-26T14:45:09.897642: step 6609, loss 0.000206793, acc 1\n",
      "2017-09-26T14:45:10.143709: step 6610, loss 3.31867e-05, acc 1\n",
      "2017-09-26T14:45:10.397647: step 6611, loss 0.00102561, acc 1\n",
      "2017-09-26T14:45:10.639145: step 6612, loss 0.00170962, acc 1\n",
      "2017-09-26T14:45:10.878321: step 6613, loss 0.00253313, acc 1\n",
      "2017-09-26T14:45:11.141236: step 6614, loss 0.000626632, acc 1\n",
      "2017-09-26T14:45:11.402668: step 6615, loss 5.97639e-05, acc 1\n",
      "2017-09-26T14:45:11.648753: step 6616, loss 0.00793407, acc 1\n",
      "2017-09-26T14:45:11.891644: step 6617, loss 0.00140569, acc 1\n",
      "2017-09-26T14:45:12.139061: step 6618, loss 0.000582017, acc 1\n",
      "2017-09-26T14:45:12.392768: step 6619, loss 0.000292625, acc 1\n",
      "2017-09-26T14:45:12.636083: step 6620, loss 0.000120665, acc 1\n",
      "2017-09-26T14:45:12.895686: step 6621, loss 7.47717e-05, acc 1\n",
      "2017-09-26T14:45:13.133657: step 6622, loss 4.7807e-05, acc 1\n",
      "2017-09-26T14:45:13.365173: step 6623, loss 0.000376464, acc 1\n",
      "2017-09-26T14:45:13.599502: step 6624, loss 0.000117045, acc 1\n",
      "2017-09-26T14:45:13.832853: step 6625, loss 0.000160825, acc 1\n",
      "2017-09-26T14:45:14.068725: step 6626, loss 0.000780048, acc 1\n",
      "2017-09-26T14:45:14.302231: step 6627, loss 0.0570061, acc 0.984375\n",
      "2017-09-26T14:45:14.553504: step 6628, loss 0.000670126, acc 1\n",
      "2017-09-26T14:45:14.788007: step 6629, loss 0.000195604, acc 1\n",
      "2017-09-26T14:45:15.050255: step 6630, loss 0.000167896, acc 1\n",
      "2017-09-26T14:45:15.302051: step 6631, loss 0.00101622, acc 1\n",
      "2017-09-26T14:45:15.614329: step 6632, loss 7.14038e-05, acc 1\n",
      "2017-09-26T14:45:15.901692: step 6633, loss 0.000569943, acc 1\n",
      "2017-09-26T14:45:16.167428: step 6634, loss 0.000690644, acc 1\n",
      "2017-09-26T14:45:16.426504: step 6635, loss 0.00019732, acc 1\n",
      "2017-09-26T14:45:16.657467: step 6636, loss 0.00011016, acc 1\n",
      "2017-09-26T14:45:16.944247: step 6637, loss 2.70956e-05, acc 1\n",
      "2017-09-26T14:45:17.193951: step 6638, loss 0.00014501, acc 1\n",
      "2017-09-26T14:45:17.448864: step 6639, loss 4.50906e-05, acc 1\n",
      "2017-09-26T14:45:17.726358: step 6640, loss 0.0392118, acc 0.984375\n",
      "2017-09-26T14:45:18.029940: step 6641, loss 0.000473375, acc 1\n",
      "2017-09-26T14:45:18.281337: step 6642, loss 4.7766e-05, acc 1\n",
      "2017-09-26T14:45:18.594707: step 6643, loss 0.000188707, acc 1\n",
      "2017-09-26T14:45:18.848235: step 6644, loss 0.000701922, acc 1\n",
      "2017-09-26T14:45:19.121473: step 6645, loss 0.00142976, acc 1\n",
      "2017-09-26T14:45:19.418313: step 6646, loss 0.000236935, acc 1\n",
      "2017-09-26T14:45:19.720553: step 6647, loss 0.000725659, acc 1\n",
      "2017-09-26T14:45:20.035287: step 6648, loss 4.89084e-05, acc 1\n",
      "2017-09-26T14:45:20.317995: step 6649, loss 5.07499e-05, acc 1\n",
      "2017-09-26T14:45:20.612747: step 6650, loss 0.000303077, acc 1\n",
      "2017-09-26T14:45:20.903196: step 6651, loss 0.000173592, acc 1\n",
      "2017-09-26T14:45:21.198034: step 6652, loss 0.00022794, acc 1\n",
      "2017-09-26T14:45:21.492362: step 6653, loss 0.000112572, acc 1\n",
      "2017-09-26T14:45:21.815400: step 6654, loss 9.08835e-05, acc 1\n",
      "2017-09-26T14:45:22.111382: step 6655, loss 0.000153999, acc 1\n",
      "2017-09-26T14:45:22.380778: step 6656, loss 0.0189878, acc 0.984375\n",
      "2017-09-26T14:45:22.644878: step 6657, loss 0.000280255, acc 1\n",
      "2017-09-26T14:45:22.911022: step 6658, loss 0.000691195, acc 1\n",
      "2017-09-26T14:45:23.192141: step 6659, loss 0.00483478, acc 1\n",
      "2017-09-26T14:45:23.479165: step 6660, loss 9.35309e-05, acc 1\n",
      "2017-09-26T14:45:23.779071: step 6661, loss 0.000148025, acc 1\n",
      "2017-09-26T14:45:24.047379: step 6662, loss 0.000354113, acc 1\n",
      "2017-09-26T14:45:24.333156: step 6663, loss 0.000846266, acc 1\n",
      "2017-09-26T14:45:24.610151: step 6664, loss 0.000282957, acc 1\n",
      "2017-09-26T14:45:24.873927: step 6665, loss 0.000137749, acc 1\n",
      "2017-09-26T14:45:25.139336: step 6666, loss 9.80833e-05, acc 1\n",
      "2017-09-26T14:45:25.409488: step 6667, loss 0.000102246, acc 1\n",
      "2017-09-26T14:45:25.697626: step 6668, loss 0.000174517, acc 1\n",
      "2017-09-26T14:45:26.002983: step 6669, loss 0.00574166, acc 1\n",
      "2017-09-26T14:45:26.296934: step 6670, loss 0.000204252, acc 1\n",
      "2017-09-26T14:45:26.561011: step 6671, loss 0.000522665, acc 1\n",
      "2017-09-26T14:45:26.838065: step 6672, loss 0.000110717, acc 1\n",
      "2017-09-26T14:45:27.124520: step 6673, loss 0.000353347, acc 1\n",
      "2017-09-26T14:45:27.404524: step 6674, loss 0.0863953, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:45:27.661733: step 6675, loss 0.000328509, acc 1\n",
      "2017-09-26T14:45:27.946282: step 6676, loss 0.00659921, acc 1\n",
      "2017-09-26T14:45:28.216244: step 6677, loss 0.00435095, acc 1\n",
      "2017-09-26T14:45:28.479774: step 6678, loss 0.00111501, acc 1\n",
      "2017-09-26T14:45:28.752701: step 6679, loss 0.00012749, acc 1\n",
      "2017-09-26T14:45:29.033672: step 6680, loss 7.15148e-05, acc 1\n",
      "2017-09-26T14:45:29.311154: step 6681, loss 0.000404319, acc 1\n",
      "2017-09-26T14:45:29.613294: step 6682, loss 0.00160884, acc 1\n",
      "2017-09-26T14:45:29.910828: step 6683, loss 0.000121398, acc 1\n",
      "2017-09-26T14:45:30.172202: step 6684, loss 4.35433e-05, acc 1\n",
      "2017-09-26T14:45:30.452289: step 6685, loss 7.80538e-05, acc 1\n",
      "2017-09-26T14:45:30.729264: step 6686, loss 0.00133092, acc 1\n",
      "2017-09-26T14:45:30.991869: step 6687, loss 7.79659e-05, acc 1\n",
      "2017-09-26T14:45:31.253490: step 6688, loss 3.38938e-05, acc 1\n",
      "2017-09-26T14:45:31.519336: step 6689, loss 0.000304434, acc 1\n",
      "2017-09-26T14:45:31.755533: step 6690, loss 0.000151494, acc 1\n",
      "2017-09-26T14:45:32.016838: step 6691, loss 0.000504205, acc 1\n",
      "2017-09-26T14:45:32.268197: step 6692, loss 0.00695033, acc 1\n",
      "2017-09-26T14:45:32.515129: step 6693, loss 6.07758e-05, acc 1\n",
      "2017-09-26T14:45:32.769038: step 6694, loss 0.00045179, acc 1\n",
      "2017-09-26T14:45:33.025463: step 6695, loss 1.89086e-05, acc 1\n",
      "2017-09-26T14:45:33.295965: step 6696, loss 0.00224838, acc 1\n",
      "2017-09-26T14:45:33.535519: step 6697, loss 0.00011041, acc 1\n",
      "2017-09-26T14:45:33.775851: step 6698, loss 0.0084867, acc 1\n",
      "2017-09-26T14:45:34.025691: step 6699, loss 9.61301e-05, acc 1\n",
      "2017-09-26T14:45:34.271789: step 6700, loss 0.000115622, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:45:34.555371: step 6700, loss 0.821617, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6700\n",
      "\n",
      "2017-09-26T14:45:35.062490: step 6701, loss 0.00778095, acc 1\n",
      "2017-09-26T14:45:35.308933: step 6702, loss 5.85883e-05, acc 1\n",
      "2017-09-26T14:45:35.565744: step 6703, loss 0.000126468, acc 1\n",
      "2017-09-26T14:45:35.828962: step 6704, loss 0.00056455, acc 1\n",
      "2017-09-26T14:45:36.085931: step 6705, loss 1.27411e-05, acc 1\n",
      "2017-09-26T14:45:36.334200: step 6706, loss 0.000322781, acc 1\n",
      "2017-09-26T14:45:36.595597: step 6707, loss 1.57806e-05, acc 1\n",
      "2017-09-26T14:45:36.833339: step 6708, loss 0.039176, acc 0.984375\n",
      "2017-09-26T14:45:37.076574: step 6709, loss 0.000560408, acc 1\n",
      "2017-09-26T14:45:37.321369: step 6710, loss 0.00102281, acc 1\n",
      "2017-09-26T14:45:37.553293: step 6711, loss 9.8707e-05, acc 1\n",
      "2017-09-26T14:45:37.796116: step 6712, loss 0.00145326, acc 1\n",
      "2017-09-26T14:45:38.037214: step 6713, loss 0.000217101, acc 1\n",
      "2017-09-26T14:45:38.281463: step 6714, loss 5.53741e-05, acc 1\n",
      "2017-09-26T14:45:38.519184: step 6715, loss 0.000142368, acc 1\n",
      "2017-09-26T14:45:38.754355: step 6716, loss 0.000176995, acc 1\n",
      "2017-09-26T14:45:39.009541: step 6717, loss 0.000266789, acc 1\n",
      "2017-09-26T14:45:39.292748: step 6718, loss 0.000106902, acc 1\n",
      "2017-09-26T14:45:39.606123: step 6719, loss 0.000376333, acc 1\n",
      "2017-09-26T14:45:39.865804: step 6720, loss 0.000599693, acc 1\n",
      "2017-09-26T14:45:40.213465: step 6721, loss 7.56547e-05, acc 1\n",
      "2017-09-26T14:45:40.509831: step 6722, loss 0.000627042, acc 1\n",
      "2017-09-26T14:45:40.789486: step 6723, loss 9.98653e-05, acc 1\n",
      "2017-09-26T14:45:41.074501: step 6724, loss 0.00778045, acc 1\n",
      "2017-09-26T14:45:41.345176: step 6725, loss 0.000122898, acc 1\n",
      "2017-09-26T14:45:41.637469: step 6726, loss 0.000113606, acc 1\n",
      "2017-09-26T14:45:41.915644: step 6727, loss 0.000152186, acc 1\n",
      "2017-09-26T14:45:42.157826: step 6728, loss 0.0177411, acc 0.984375\n",
      "2017-09-26T14:45:42.393764: step 6729, loss 0.010135, acc 1\n",
      "2017-09-26T14:45:42.632882: step 6730, loss 0.00119035, acc 1\n",
      "2017-09-26T14:45:42.871677: step 6731, loss 0.000306877, acc 1\n",
      "2017-09-26T14:45:43.105486: step 6732, loss 4.9607e-05, acc 1\n",
      "2017-09-26T14:45:43.343260: step 6733, loss 0.00022943, acc 1\n",
      "2017-09-26T14:45:43.578513: step 6734, loss 0.000282915, acc 1\n",
      "2017-09-26T14:45:43.811127: step 6735, loss 2.58795e-05, acc 1\n",
      "2017-09-26T14:45:44.059159: step 6736, loss 0.000218608, acc 1\n",
      "2017-09-26T14:45:44.294770: step 6737, loss 0.000223335, acc 1\n",
      "2017-09-26T14:45:44.549613: step 6738, loss 0.0375395, acc 0.984375\n",
      "2017-09-26T14:45:44.795130: step 6739, loss 0.00300735, acc 1\n",
      "2017-09-26T14:45:45.033798: step 6740, loss 4.95089e-05, acc 1\n",
      "2017-09-26T14:45:45.276898: step 6741, loss 0.000108143, acc 1\n",
      "2017-09-26T14:45:45.509060: step 6742, loss 0.000417341, acc 1\n",
      "2017-09-26T14:45:45.749505: step 6743, loss 0.00328806, acc 1\n",
      "2017-09-26T14:45:46.006155: step 6744, loss 0.00022204, acc 1\n",
      "2017-09-26T14:45:46.266832: step 6745, loss 1.88332e-05, acc 1\n",
      "2017-09-26T14:45:46.502069: step 6746, loss 0.00484997, acc 1\n",
      "2017-09-26T14:45:46.799897: step 6747, loss 1.19323e-05, acc 1\n",
      "2017-09-26T14:45:47.070992: step 6748, loss 0.000335651, acc 1\n",
      "2017-09-26T14:45:47.309418: step 6749, loss 1.31028e-05, acc 1\n",
      "2017-09-26T14:45:47.559747: step 6750, loss 0.0435445, acc 0.984375\n",
      "2017-09-26T14:45:47.857856: step 6751, loss 0.000180962, acc 1\n",
      "2017-09-26T14:45:48.165873: step 6752, loss 0.000108926, acc 1\n",
      "2017-09-26T14:45:48.504683: step 6753, loss 0.000222083, acc 1\n",
      "2017-09-26T14:45:48.794085: step 6754, loss 0.000148915, acc 1\n",
      "2017-09-26T14:45:49.073278: step 6755, loss 0.00346965, acc 1\n",
      "2017-09-26T14:45:49.375037: step 6756, loss 3.92517e-05, acc 1\n",
      "2017-09-26T14:45:49.788588: step 6757, loss 0.000250363, acc 1\n",
      "2017-09-26T14:45:50.146837: step 6758, loss 4.65229e-05, acc 1\n",
      "2017-09-26T14:45:50.488655: step 6759, loss 0.000338819, acc 1\n",
      "2017-09-26T14:45:50.800945: step 6760, loss 8.31278e-05, acc 1\n",
      "2017-09-26T14:45:51.071860: step 6761, loss 7.72779e-05, acc 1\n",
      "2017-09-26T14:45:51.315161: step 6762, loss 5.43711e-05, acc 1\n",
      "2017-09-26T14:45:51.577713: step 6763, loss 0.000271786, acc 1\n",
      "2017-09-26T14:45:51.843840: step 6764, loss 0.000970619, acc 1\n",
      "2017-09-26T14:45:52.077139: step 6765, loss 5.07031e-05, acc 1\n",
      "2017-09-26T14:45:52.311945: step 6766, loss 0.000115474, acc 1\n",
      "2017-09-26T14:45:52.569532: step 6767, loss 0.00138858, acc 1\n",
      "2017-09-26T14:45:52.891169: step 6768, loss 0.00324949, acc 1\n",
      "2017-09-26T14:45:53.194821: step 6769, loss 0.000364039, acc 1\n",
      "2017-09-26T14:45:53.467477: step 6770, loss 0.00108457, acc 1\n",
      "2017-09-26T14:45:53.789314: step 6771, loss 0.0687618, acc 0.984375\n",
      "2017-09-26T14:45:54.044540: step 6772, loss 0.00151398, acc 1\n",
      "2017-09-26T14:45:54.301034: step 6773, loss 0.00137385, acc 1\n",
      "2017-09-26T14:45:54.572812: step 6774, loss 0.000218948, acc 1\n",
      "2017-09-26T14:45:54.838764: step 6775, loss 0.00123689, acc 1\n",
      "2017-09-26T14:45:55.128066: step 6776, loss 0.00533073, acc 1\n",
      "2017-09-26T14:45:55.384721: step 6777, loss 0.00109084, acc 1\n",
      "2017-09-26T14:45:55.640371: step 6778, loss 0.0211714, acc 0.984375\n",
      "2017-09-26T14:45:55.889581: step 6779, loss 1.9019e-05, acc 1\n",
      "2017-09-26T14:45:56.136209: step 6780, loss 8.10071e-05, acc 1\n",
      "2017-09-26T14:45:56.399519: step 6781, loss 0.000169538, acc 1\n",
      "2017-09-26T14:45:56.639821: step 6782, loss 0.000110315, acc 1\n",
      "2017-09-26T14:45:56.889681: step 6783, loss 9.27066e-05, acc 1\n",
      "2017-09-26T14:45:57.155066: step 6784, loss 0.00388948, acc 1\n",
      "2017-09-26T14:45:57.388954: step 6785, loss 0.000119599, acc 1\n",
      "2017-09-26T14:45:57.633462: step 6786, loss 0.00170564, acc 1\n",
      "2017-09-26T14:45:57.870063: step 6787, loss 0.000894048, acc 1\n",
      "2017-09-26T14:45:58.128955: step 6788, loss 0.000193527, acc 1\n",
      "2017-09-26T14:45:58.390138: step 6789, loss 0.00237973, acc 1\n",
      "2017-09-26T14:45:58.664809: step 6790, loss 7.39058e-05, acc 1\n",
      "2017-09-26T14:45:58.915816: step 6791, loss 0.000625729, acc 1\n",
      "2017-09-26T14:45:59.161821: step 6792, loss 0.000373051, acc 1\n",
      "2017-09-26T14:45:59.412654: step 6793, loss 0.000268535, acc 1\n",
      "2017-09-26T14:45:59.657534: step 6794, loss 0.00052056, acc 1\n",
      "2017-09-26T14:45:59.897724: step 6795, loss 7.87527e-05, acc 1\n",
      "2017-09-26T14:46:00.162237: step 6796, loss 6.40391e-05, acc 1\n",
      "2017-09-26T14:46:00.414759: step 6797, loss 0.000287356, acc 1\n",
      "2017-09-26T14:46:00.663571: step 6798, loss 5.24617e-05, acc 1\n",
      "2017-09-26T14:46:00.902208: step 6799, loss 0.00196449, acc 1\n",
      "2017-09-26T14:46:01.145059: step 6800, loss 0.000524493, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:46:01.384081: step 6800, loss 0.835372, acc 0.882155\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6800\n",
      "\n",
      "2017-09-26T14:46:01.891237: step 6801, loss 9.61444e-05, acc 1\n",
      "2017-09-26T14:46:02.126707: step 6802, loss 0.000246012, acc 1\n",
      "2017-09-26T14:46:02.358747: step 6803, loss 0.00206069, acc 1\n",
      "2017-09-26T14:46:02.561113: step 6804, loss 3.80929e-05, acc 1\n",
      "2017-09-26T14:46:02.805115: step 6805, loss 0.00331263, acc 1\n",
      "2017-09-26T14:46:03.073719: step 6806, loss 0.00153664, acc 1\n",
      "2017-09-26T14:46:03.318525: step 6807, loss 0.000863261, acc 1\n",
      "2017-09-26T14:46:03.566460: step 6808, loss 0.000392788, acc 1\n",
      "2017-09-26T14:46:03.803366: step 6809, loss 0.000121003, acc 1\n",
      "2017-09-26T14:46:04.044409: step 6810, loss 0.000793397, acc 1\n",
      "2017-09-26T14:46:04.293199: step 6811, loss 0.000152958, acc 1\n",
      "2017-09-26T14:46:04.526335: step 6812, loss 0.000302147, acc 1\n",
      "2017-09-26T14:46:04.769879: step 6813, loss 0.0128793, acc 0.984375\n",
      "2017-09-26T14:46:05.009404: step 6814, loss 0.000705032, acc 1\n",
      "2017-09-26T14:46:05.245528: step 6815, loss 0.00106564, acc 1\n",
      "2017-09-26T14:46:05.485674: step 6816, loss 0.00016476, acc 1\n",
      "2017-09-26T14:46:05.737221: step 6817, loss 6.85197e-05, acc 1\n",
      "2017-09-26T14:46:05.972489: step 6818, loss 3.26503e-05, acc 1\n",
      "2017-09-26T14:46:06.210075: step 6819, loss 0.0618061, acc 0.984375\n",
      "2017-09-26T14:46:06.447311: step 6820, loss 0.000216115, acc 1\n",
      "2017-09-26T14:46:06.689358: step 6821, loss 0.000971143, acc 1\n",
      "2017-09-26T14:46:06.921523: step 6822, loss 0.035083, acc 0.96875\n",
      "2017-09-26T14:46:07.169347: step 6823, loss 8.08482e-05, acc 1\n",
      "2017-09-26T14:46:07.409354: step 6824, loss 2.47961e-05, acc 1\n",
      "2017-09-26T14:46:07.651584: step 6825, loss 0.00220825, acc 1\n",
      "2017-09-26T14:46:07.910821: step 6826, loss 0.0096432, acc 1\n",
      "2017-09-26T14:46:08.145999: step 6827, loss 8.76678e-05, acc 1\n",
      "2017-09-26T14:46:08.380414: step 6828, loss 9.56314e-05, acc 1\n",
      "2017-09-26T14:46:08.622068: step 6829, loss 1.3103e-05, acc 1\n",
      "2017-09-26T14:46:08.864034: step 6830, loss 0.00096093, acc 1\n",
      "2017-09-26T14:46:09.105460: step 6831, loss 0.00289396, acc 1\n",
      "2017-09-26T14:46:09.340222: step 6832, loss 3.01163e-05, acc 1\n",
      "2017-09-26T14:46:09.572449: step 6833, loss 0.000143286, acc 1\n",
      "2017-09-26T14:46:09.810429: step 6834, loss 0.00165818, acc 1\n",
      "2017-09-26T14:46:10.058509: step 6835, loss 0.000611478, acc 1\n",
      "2017-09-26T14:46:10.296390: step 6836, loss 3.56374e-05, acc 1\n",
      "2017-09-26T14:46:10.536512: step 6837, loss 0.000153243, acc 1\n",
      "2017-09-26T14:46:10.776704: step 6838, loss 8.22424e-05, acc 1\n",
      "2017-09-26T14:46:11.028001: step 6839, loss 0.000100128, acc 1\n",
      "2017-09-26T14:46:11.269996: step 6840, loss 0.000320609, acc 1\n",
      "2017-09-26T14:46:11.511650: step 6841, loss 0.0306829, acc 0.984375\n",
      "2017-09-26T14:46:11.753639: step 6842, loss 0.000449724, acc 1\n",
      "2017-09-26T14:46:11.998729: step 6843, loss 0.000101987, acc 1\n",
      "2017-09-26T14:46:12.259979: step 6844, loss 1.99962e-05, acc 1\n",
      "2017-09-26T14:46:12.491411: step 6845, loss 2.80736e-05, acc 1\n",
      "2017-09-26T14:46:12.701536: step 6846, loss 0.000246582, acc 1\n",
      "2017-09-26T14:46:12.940042: step 6847, loss 0.000285879, acc 1\n",
      "2017-09-26T14:46:13.179640: step 6848, loss 0.000294207, acc 1\n",
      "2017-09-26T14:46:13.416797: step 6849, loss 4.2428e-05, acc 1\n",
      "2017-09-26T14:46:13.648358: step 6850, loss 0.00107006, acc 1\n",
      "2017-09-26T14:46:13.886249: step 6851, loss 0.000258951, acc 1\n",
      "2017-09-26T14:46:14.117435: step 6852, loss 0.00017907, acc 1\n",
      "2017-09-26T14:46:14.351044: step 6853, loss 0.00012767, acc 1\n",
      "2017-09-26T14:46:14.593881: step 6854, loss 4.51014e-05, acc 1\n",
      "2017-09-26T14:46:14.842876: step 6855, loss 6.22007e-05, acc 1\n",
      "2017-09-26T14:46:15.094508: step 6856, loss 5.09006e-05, acc 1\n",
      "2017-09-26T14:46:15.330608: step 6857, loss 0.00043548, acc 1\n",
      "2017-09-26T14:46:15.577892: step 6858, loss 1.76403e-05, acc 1\n",
      "2017-09-26T14:46:15.814497: step 6859, loss 7.33664e-05, acc 1\n",
      "2017-09-26T14:46:16.064569: step 6860, loss 0.0304013, acc 0.984375\n",
      "2017-09-26T14:46:16.321325: step 6861, loss 0.00119836, acc 1\n",
      "2017-09-26T14:46:16.603613: step 6862, loss 0.000570816, acc 1\n",
      "2017-09-26T14:46:16.871897: step 6863, loss 6.00659e-05, acc 1\n",
      "2017-09-26T14:46:17.133356: step 6864, loss 5.66554e-05, acc 1\n",
      "2017-09-26T14:46:17.381987: step 6865, loss 0.0191482, acc 0.984375\n",
      "2017-09-26T14:46:17.624797: step 6866, loss 0.000101535, acc 1\n",
      "2017-09-26T14:46:17.871279: step 6867, loss 9.93467e-05, acc 1\n",
      "2017-09-26T14:46:18.112979: step 6868, loss 0.0587811, acc 0.984375\n",
      "2017-09-26T14:46:18.392494: step 6869, loss 4.68203e-05, acc 1\n",
      "2017-09-26T14:46:18.642792: step 6870, loss 0.00443876, acc 1\n",
      "2017-09-26T14:46:18.913621: step 6871, loss 0.000192158, acc 1\n",
      "2017-09-26T14:46:19.193324: step 6872, loss 0.00413729, acc 1\n",
      "2017-09-26T14:46:19.434673: step 6873, loss 0.000109943, acc 1\n",
      "2017-09-26T14:46:19.705992: step 6874, loss 0.0238755, acc 0.984375\n",
      "2017-09-26T14:46:19.952271: step 6875, loss 2.99574e-05, acc 1\n",
      "2017-09-26T14:46:20.190554: step 6876, loss 0.00413525, acc 1\n",
      "2017-09-26T14:46:20.425966: step 6877, loss 0.00737062, acc 1\n",
      "2017-09-26T14:46:20.664601: step 6878, loss 0.000509223, acc 1\n",
      "2017-09-26T14:46:20.901936: step 6879, loss 5.69755e-05, acc 1\n",
      "2017-09-26T14:46:21.143918: step 6880, loss 0.000149949, acc 1\n",
      "2017-09-26T14:46:21.378394: step 6881, loss 6.30618e-05, acc 1\n",
      "2017-09-26T14:46:21.614319: step 6882, loss 0.000927938, acc 1\n",
      "2017-09-26T14:46:21.858446: step 6883, loss 0.00101145, acc 1\n",
      "2017-09-26T14:46:22.095070: step 6884, loss 4.80703e-05, acc 1\n",
      "2017-09-26T14:46:22.331817: step 6885, loss 5.30095e-05, acc 1\n",
      "2017-09-26T14:46:22.573541: step 6886, loss 6.45248e-05, acc 1\n",
      "2017-09-26T14:46:22.815753: step 6887, loss 0.00151548, acc 1\n",
      "2017-09-26T14:46:23.026910: step 6888, loss 0.00650069, acc 1\n",
      "2017-09-26T14:46:23.292881: step 6889, loss 1.14622e-05, acc 1\n",
      "2017-09-26T14:46:23.533904: step 6890, loss 0.000769725, acc 1\n",
      "2017-09-26T14:46:23.773335: step 6891, loss 5.90554e-05, acc 1\n",
      "2017-09-26T14:46:24.022668: step 6892, loss 1.69896e-05, acc 1\n",
      "2017-09-26T14:46:24.255800: step 6893, loss 1.89699e-05, acc 1\n",
      "2017-09-26T14:46:24.509340: step 6894, loss 0.000312604, acc 1\n",
      "2017-09-26T14:46:24.747471: step 6895, loss 2.40003e-05, acc 1\n",
      "2017-09-26T14:46:24.991001: step 6896, loss 0.000142033, acc 1\n",
      "2017-09-26T14:46:25.239185: step 6897, loss 0.000143449, acc 1\n",
      "2017-09-26T14:46:25.481154: step 6898, loss 6.65219e-05, acc 1\n",
      "2017-09-26T14:46:25.735878: step 6899, loss 0.00123195, acc 1\n",
      "2017-09-26T14:46:25.981083: step 6900, loss 9.43514e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:46:26.212826: step 6900, loss 0.767591, acc 0.878788\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-6900\n",
      "\n",
      "2017-09-26T14:46:26.724009: step 6901, loss 0.000241853, acc 1\n",
      "2017-09-26T14:46:26.964278: step 6902, loss 0.0230419, acc 0.984375\n",
      "2017-09-26T14:46:27.203469: step 6903, loss 0.000313695, acc 1\n",
      "2017-09-26T14:46:27.439451: step 6904, loss 7.18758e-05, acc 1\n",
      "2017-09-26T14:46:27.682861: step 6905, loss 0.000103277, acc 1\n",
      "2017-09-26T14:46:27.919361: step 6906, loss 0.000176164, acc 1\n",
      "2017-09-26T14:46:28.168427: step 6907, loss 0.000185343, acc 1\n",
      "2017-09-26T14:46:28.421450: step 6908, loss 0.00137063, acc 1\n",
      "2017-09-26T14:46:28.659095: step 6909, loss 1.44874e-05, acc 1\n",
      "2017-09-26T14:46:28.896863: step 6910, loss 0.00013262, acc 1\n",
      "2017-09-26T14:46:29.207169: step 6911, loss 3.48786e-05, acc 1\n",
      "2017-09-26T14:46:29.503622: step 6912, loss 0.000276235, acc 1\n",
      "2017-09-26T14:46:29.760572: step 6913, loss 4.20091e-05, acc 1\n",
      "2017-09-26T14:46:30.031218: step 6914, loss 1.19216e-05, acc 1\n",
      "2017-09-26T14:46:30.329121: step 6915, loss 0.000249486, acc 1\n",
      "2017-09-26T14:46:30.589718: step 6916, loss 0.00140076, acc 1\n",
      "2017-09-26T14:46:30.857353: step 6917, loss 6.30365e-05, acc 1\n",
      "2017-09-26T14:46:31.155608: step 6918, loss 0.000604863, acc 1\n",
      "2017-09-26T14:46:31.458826: step 6919, loss 6.17253e-06, acc 1\n",
      "2017-09-26T14:46:31.729166: step 6920, loss 0.000118595, acc 1\n",
      "2017-09-26T14:46:31.970396: step 6921, loss 4.51617e-05, acc 1\n",
      "2017-09-26T14:46:32.215965: step 6922, loss 0.00011845, acc 1\n",
      "2017-09-26T14:46:32.474679: step 6923, loss 0.000783795, acc 1\n",
      "2017-09-26T14:46:32.721463: step 6924, loss 7.43917e-05, acc 1\n",
      "2017-09-26T14:46:32.964404: step 6925, loss 0.000334409, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:46:33.210400: step 6926, loss 8.7907e-05, acc 1\n",
      "2017-09-26T14:46:33.474169: step 6927, loss 0.00130065, acc 1\n",
      "2017-09-26T14:46:33.736941: step 6928, loss 0.000208291, acc 1\n",
      "2017-09-26T14:46:33.978671: step 6929, loss 0.07779, acc 0.984375\n",
      "2017-09-26T14:46:34.188118: step 6930, loss 0.000654674, acc 1\n",
      "2017-09-26T14:46:34.437830: step 6931, loss 0.000598693, acc 1\n",
      "2017-09-26T14:46:34.676713: step 6932, loss 7.9671e-05, acc 1\n",
      "2017-09-26T14:46:34.974816: step 6933, loss 0.0214007, acc 0.984375\n",
      "2017-09-26T14:46:35.242961: step 6934, loss 0.000101735, acc 1\n",
      "2017-09-26T14:46:35.522070: step 6935, loss 1.7089e-05, acc 1\n",
      "2017-09-26T14:46:35.771347: step 6936, loss 5.31365e-05, acc 1\n",
      "2017-09-26T14:46:36.003349: step 6937, loss 7.27997e-05, acc 1\n",
      "2017-09-26T14:46:36.239373: step 6938, loss 0.000494606, acc 1\n",
      "2017-09-26T14:46:36.470024: step 6939, loss 0.000183776, acc 1\n",
      "2017-09-26T14:46:36.707229: step 6940, loss 0.000169859, acc 1\n",
      "2017-09-26T14:46:36.945586: step 6941, loss 8.87704e-05, acc 1\n",
      "2017-09-26T14:46:37.186387: step 6942, loss 0.000141795, acc 1\n",
      "2017-09-26T14:46:37.419780: step 6943, loss 0.000617167, acc 1\n",
      "2017-09-26T14:46:37.672752: step 6944, loss 0.000451195, acc 1\n",
      "2017-09-26T14:46:37.914993: step 6945, loss 1.22288e-05, acc 1\n",
      "2017-09-26T14:46:38.150767: step 6946, loss 7.59879e-05, acc 1\n",
      "2017-09-26T14:46:38.400161: step 6947, loss 2.71791e-05, acc 1\n",
      "2017-09-26T14:46:38.642766: step 6948, loss 0.00166713, acc 1\n",
      "2017-09-26T14:46:38.880926: step 6949, loss 8.11038e-05, acc 1\n",
      "2017-09-26T14:46:39.119732: step 6950, loss 2.38109e-05, acc 1\n",
      "2017-09-26T14:46:39.395404: step 6951, loss 0.00172677, acc 1\n",
      "2017-09-26T14:46:39.631245: step 6952, loss 5.13736e-05, acc 1\n",
      "2017-09-26T14:46:39.876963: step 6953, loss 0.0388737, acc 0.984375\n",
      "2017-09-26T14:46:40.119874: step 6954, loss 0.000237114, acc 1\n",
      "2017-09-26T14:46:40.362540: step 6955, loss 0.000126425, acc 1\n",
      "2017-09-26T14:46:40.625718: step 6956, loss 0.000223855, acc 1\n",
      "2017-09-26T14:46:40.871164: step 6957, loss 8.08739e-05, acc 1\n",
      "2017-09-26T14:46:41.117299: step 6958, loss 0.000184973, acc 1\n",
      "2017-09-26T14:46:41.390781: step 6959, loss 0.000635755, acc 1\n",
      "2017-09-26T14:46:41.632267: step 6960, loss 6.59753e-05, acc 1\n",
      "2017-09-26T14:46:41.875745: step 6961, loss 0.000189209, acc 1\n",
      "2017-09-26T14:46:42.113821: step 6962, loss 0.000197107, acc 1\n",
      "2017-09-26T14:46:42.354530: step 6963, loss 5.18443e-05, acc 1\n",
      "2017-09-26T14:46:42.612484: step 6964, loss 0.000153815, acc 1\n",
      "2017-09-26T14:46:42.876443: step 6965, loss 0.00044823, acc 1\n",
      "2017-09-26T14:46:43.122810: step 6966, loss 0.000209727, acc 1\n",
      "2017-09-26T14:46:43.395045: step 6967, loss 0.00127914, acc 1\n",
      "2017-09-26T14:46:43.661188: step 6968, loss 9.65017e-05, acc 1\n",
      "2017-09-26T14:46:43.932770: step 6969, loss 1.582e-05, acc 1\n",
      "2017-09-26T14:46:44.243666: step 6970, loss 3.75178e-05, acc 1\n",
      "2017-09-26T14:46:44.572053: step 6971, loss 0.00136893, acc 1\n",
      "2017-09-26T14:46:44.890517: step 6972, loss 0.000139723, acc 1\n",
      "2017-09-26T14:46:45.183852: step 6973, loss 0.000409318, acc 1\n",
      "2017-09-26T14:46:45.464181: step 6974, loss 0.000244894, acc 1\n",
      "2017-09-26T14:46:45.737741: step 6975, loss 0.000611493, acc 1\n",
      "2017-09-26T14:46:46.023564: step 6976, loss 9.02226e-05, acc 1\n",
      "2017-09-26T14:46:46.312815: step 6977, loss 0.000468902, acc 1\n",
      "2017-09-26T14:46:46.588399: step 6978, loss 0.00530231, acc 1\n",
      "2017-09-26T14:46:46.848496: step 6979, loss 0.000760912, acc 1\n",
      "2017-09-26T14:46:47.125884: step 6980, loss 8.86326e-05, acc 1\n",
      "2017-09-26T14:46:47.450296: step 6981, loss 3.45619e-05, acc 1\n",
      "2017-09-26T14:46:47.808972: step 6982, loss 3.41619e-05, acc 1\n",
      "2017-09-26T14:46:48.149961: step 6983, loss 0.00319819, acc 1\n",
      "2017-09-26T14:46:48.469193: step 6984, loss 0.000140051, acc 1\n",
      "2017-09-26T14:46:48.726922: step 6985, loss 4.50518e-05, acc 1\n",
      "2017-09-26T14:46:49.041669: step 6986, loss 4.31124e-05, acc 1\n",
      "2017-09-26T14:46:49.332372: step 6987, loss 2.90067e-05, acc 1\n",
      "2017-09-26T14:46:49.628031: step 6988, loss 0.000153784, acc 1\n",
      "2017-09-26T14:46:49.938202: step 6989, loss 3.59566e-05, acc 1\n",
      "2017-09-26T14:46:50.252316: step 6990, loss 8.64701e-05, acc 1\n",
      "2017-09-26T14:46:50.576513: step 6991, loss 0.000242399, acc 1\n",
      "2017-09-26T14:46:50.825087: step 6992, loss 0.142956, acc 0.984375\n",
      "2017-09-26T14:46:51.055210: step 6993, loss 0.000143158, acc 1\n",
      "2017-09-26T14:46:51.291182: step 6994, loss 0.000220958, acc 1\n",
      "2017-09-26T14:46:51.521974: step 6995, loss 0.00381606, acc 1\n",
      "2017-09-26T14:46:51.755957: step 6996, loss 5.71614e-05, acc 1\n",
      "2017-09-26T14:46:51.983770: step 6997, loss 0.000228266, acc 1\n",
      "2017-09-26T14:46:52.222487: step 6998, loss 0.000115896, acc 1\n",
      "2017-09-26T14:46:52.451723: step 6999, loss 8.12431e-05, acc 1\n",
      "2017-09-26T14:46:52.682308: step 7000, loss 0.000183887, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:46:52.917743: step 7000, loss 0.722364, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7000\n",
      "\n",
      "2017-09-26T14:46:53.472022: step 7001, loss 0.000197673, acc 1\n",
      "2017-09-26T14:46:53.742794: step 7002, loss 5.66408e-05, acc 1\n",
      "2017-09-26T14:46:54.038212: step 7003, loss 0.000100275, acc 1\n",
      "2017-09-26T14:46:54.335159: step 7004, loss 0.000167985, acc 1\n",
      "2017-09-26T14:46:54.620243: step 7005, loss 1.87939e-05, acc 1\n",
      "2017-09-26T14:46:54.922616: step 7006, loss 0.000196651, acc 1\n",
      "2017-09-26T14:46:55.219986: step 7007, loss 0.000177492, acc 1\n",
      "2017-09-26T14:46:55.510430: step 7008, loss 0.000125391, acc 1\n",
      "2017-09-26T14:46:55.794653: step 7009, loss 3.92028e-05, acc 1\n",
      "2017-09-26T14:46:56.072512: step 7010, loss 0.000100249, acc 1\n",
      "2017-09-26T14:46:56.372736: step 7011, loss 7.76934e-05, acc 1\n",
      "2017-09-26T14:46:56.649997: step 7012, loss 5.32937e-05, acc 1\n",
      "2017-09-26T14:46:56.928491: step 7013, loss 1.76678e-05, acc 1\n",
      "2017-09-26T14:46:57.163904: step 7014, loss 3.75855e-05, acc 1\n",
      "2017-09-26T14:46:57.460532: step 7015, loss 0.00207821, acc 1\n",
      "2017-09-26T14:46:57.782692: step 7016, loss 0.000130482, acc 1\n",
      "2017-09-26T14:46:58.082440: step 7017, loss 0.0017873, acc 1\n",
      "2017-09-26T14:46:58.378287: step 7018, loss 4.33639e-05, acc 1\n",
      "2017-09-26T14:46:58.652226: step 7019, loss 1.98727e-05, acc 1\n",
      "2017-09-26T14:46:58.936532: step 7020, loss 1.33835e-05, acc 1\n",
      "2017-09-26T14:46:59.265953: step 7021, loss 0.0010439, acc 1\n",
      "2017-09-26T14:46:59.573814: step 7022, loss 0.000964095, acc 1\n",
      "2017-09-26T14:46:59.879055: step 7023, loss 6.00088e-05, acc 1\n",
      "2017-09-26T14:47:00.161382: step 7024, loss 0.000107106, acc 1\n",
      "2017-09-26T14:47:00.565287: step 7025, loss 3.40559e-05, acc 1\n",
      "2017-09-26T14:47:00.966081: step 7026, loss 0.000106288, acc 1\n",
      "2017-09-26T14:47:01.374403: step 7027, loss 0.000166857, acc 1\n",
      "2017-09-26T14:47:01.723042: step 7028, loss 0.00023775, acc 1\n",
      "2017-09-26T14:47:02.036064: step 7029, loss 0.000113096, acc 1\n",
      "2017-09-26T14:47:02.370901: step 7030, loss 0.000225622, acc 1\n",
      "2017-09-26T14:47:02.697329: step 7031, loss 9.96952e-06, acc 1\n",
      "2017-09-26T14:47:02.972362: step 7032, loss 0.000568811, acc 1\n",
      "2017-09-26T14:47:03.288450: step 7033, loss 9.61137e-05, acc 1\n",
      "2017-09-26T14:47:03.644119: step 7034, loss 7.37753e-05, acc 1\n",
      "2017-09-26T14:47:04.015418: step 7035, loss 0.0003215, acc 1\n",
      "2017-09-26T14:47:04.398560: step 7036, loss 2.2655e-05, acc 1\n",
      "2017-09-26T14:47:04.778099: step 7037, loss 0.0473517, acc 0.984375\n",
      "2017-09-26T14:47:05.171028: step 7038, loss 0.000221064, acc 1\n",
      "2017-09-26T14:47:05.599425: step 7039, loss 0.000270124, acc 1\n",
      "2017-09-26T14:47:05.958950: step 7040, loss 0.000133872, acc 1\n",
      "2017-09-26T14:47:06.261939: step 7041, loss 0.000414862, acc 1\n",
      "2017-09-26T14:47:06.527292: step 7042, loss 0.00023732, acc 1\n",
      "2017-09-26T14:47:06.808442: step 7043, loss 0.000228388, acc 1\n",
      "2017-09-26T14:47:07.098396: step 7044, loss 7.55409e-05, acc 1\n",
      "2017-09-26T14:47:07.373063: step 7045, loss 0.00805271, acc 1\n",
      "2017-09-26T14:47:07.672056: step 7046, loss 0.000108089, acc 1\n",
      "2017-09-26T14:47:07.964658: step 7047, loss 0.000434199, acc 1\n",
      "2017-09-26T14:47:08.301872: step 7048, loss 9.483e-05, acc 1\n",
      "2017-09-26T14:47:08.642417: step 7049, loss 0.00406646, acc 1\n",
      "2017-09-26T14:47:08.932077: step 7050, loss 0.000200544, acc 1\n",
      "2017-09-26T14:47:09.291310: step 7051, loss 3.32598e-05, acc 1\n",
      "2017-09-26T14:47:09.640306: step 7052, loss 0.000226444, acc 1\n",
      "2017-09-26T14:47:09.960109: step 7053, loss 0.000401509, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:47:10.254066: step 7054, loss 0.000226688, acc 1\n",
      "2017-09-26T14:47:10.524256: step 7055, loss 0.000189644, acc 1\n",
      "2017-09-26T14:47:10.747817: step 7056, loss 0.000514807, acc 1\n",
      "2017-09-26T14:47:11.030013: step 7057, loss 0.00106533, acc 1\n",
      "2017-09-26T14:47:11.312406: step 7058, loss 0.00229298, acc 1\n",
      "2017-09-26T14:47:11.580127: step 7059, loss 5.65291e-05, acc 1\n",
      "2017-09-26T14:47:11.833975: step 7060, loss 0.00018918, acc 1\n",
      "2017-09-26T14:47:12.069882: step 7061, loss 0.000158653, acc 1\n",
      "2017-09-26T14:47:12.311966: step 7062, loss 0.000133407, acc 1\n",
      "2017-09-26T14:47:12.560778: step 7063, loss 0.00746555, acc 1\n",
      "2017-09-26T14:47:12.823043: step 7064, loss 3.49486e-05, acc 1\n",
      "2017-09-26T14:47:13.081269: step 7065, loss 4.32279e-05, acc 1\n",
      "2017-09-26T14:47:13.311689: step 7066, loss 0.000948859, acc 1\n",
      "2017-09-26T14:47:13.537133: step 7067, loss 3.32682e-05, acc 1\n",
      "2017-09-26T14:47:13.768740: step 7068, loss 0.000162547, acc 1\n",
      "2017-09-26T14:47:13.998979: step 7069, loss 0.00136792, acc 1\n",
      "2017-09-26T14:47:14.260133: step 7070, loss 0.00010228, acc 1\n",
      "2017-09-26T14:47:14.493051: step 7071, loss 0.000157727, acc 1\n",
      "2017-09-26T14:47:14.725885: step 7072, loss 0.000579394, acc 1\n",
      "2017-09-26T14:47:14.953460: step 7073, loss 0.000359689, acc 1\n",
      "2017-09-26T14:47:15.217677: step 7074, loss 0.000247278, acc 1\n",
      "2017-09-26T14:47:15.446019: step 7075, loss 3.5634e-05, acc 1\n",
      "2017-09-26T14:47:15.674735: step 7076, loss 5.16145e-05, acc 1\n",
      "2017-09-26T14:47:15.903502: step 7077, loss 0.0303247, acc 0.984375\n",
      "2017-09-26T14:47:16.135662: step 7078, loss 4.39493e-05, acc 1\n",
      "2017-09-26T14:47:16.367739: step 7079, loss 0.000214066, acc 1\n",
      "2017-09-26T14:47:16.599331: step 7080, loss 0.000397247, acc 1\n",
      "2017-09-26T14:47:16.836618: step 7081, loss 3.36769e-05, acc 1\n",
      "2017-09-26T14:47:17.097625: step 7082, loss 0.00011429, acc 1\n",
      "2017-09-26T14:47:17.327756: step 7083, loss 0.0208357, acc 0.984375\n",
      "2017-09-26T14:47:17.573326: step 7084, loss 0.000200599, acc 1\n",
      "2017-09-26T14:47:17.856413: step 7085, loss 0.00497835, acc 1\n",
      "2017-09-26T14:47:18.100195: step 7086, loss 0.00075444, acc 1\n",
      "2017-09-26T14:47:18.335500: step 7087, loss 1.09628e-05, acc 1\n",
      "2017-09-26T14:47:18.587631: step 7088, loss 4.02065e-05, acc 1\n",
      "2017-09-26T14:47:18.818097: step 7089, loss 5.69233e-05, acc 1\n",
      "2017-09-26T14:47:19.052440: step 7090, loss 3.75624e-05, acc 1\n",
      "2017-09-26T14:47:19.285468: step 7091, loss 0.000207022, acc 1\n",
      "2017-09-26T14:47:19.517656: step 7092, loss 6.04063e-05, acc 1\n",
      "2017-09-26T14:47:19.751333: step 7093, loss 0.00341696, acc 1\n",
      "2017-09-26T14:47:19.979978: step 7094, loss 7.44455e-05, acc 1\n",
      "2017-09-26T14:47:20.208147: step 7095, loss 0.00240666, acc 1\n",
      "2017-09-26T14:47:20.448402: step 7096, loss 0.000214651, acc 1\n",
      "2017-09-26T14:47:20.689672: step 7097, loss 0.000139507, acc 1\n",
      "2017-09-26T14:47:20.889329: step 7098, loss 0.000591688, acc 1\n",
      "2017-09-26T14:47:21.122237: step 7099, loss 9.62864e-05, acc 1\n",
      "2017-09-26T14:47:21.360163: step 7100, loss 0.000514928, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:47:21.585155: step 7100, loss 0.757805, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7100\n",
      "\n",
      "2017-09-26T14:47:22.087652: step 7101, loss 0.000224967, acc 1\n",
      "2017-09-26T14:47:22.316109: step 7102, loss 7.85509e-05, acc 1\n",
      "2017-09-26T14:47:22.545756: step 7103, loss 0.00394473, acc 1\n",
      "2017-09-26T14:47:22.776679: step 7104, loss 0.000223043, acc 1\n",
      "2017-09-26T14:47:23.005001: step 7105, loss 0.000118826, acc 1\n",
      "2017-09-26T14:47:23.237365: step 7106, loss 0.000138021, acc 1\n",
      "2017-09-26T14:47:23.486163: step 7107, loss 5.18103e-05, acc 1\n",
      "2017-09-26T14:47:23.734799: step 7108, loss 1.70204e-05, acc 1\n",
      "2017-09-26T14:47:23.993179: step 7109, loss 0.0132418, acc 0.984375\n",
      "2017-09-26T14:47:24.250247: step 7110, loss 3.60872e-05, acc 1\n",
      "2017-09-26T14:47:24.487031: step 7111, loss 0.000987752, acc 1\n",
      "2017-09-26T14:47:24.718911: step 7112, loss 0.000428005, acc 1\n",
      "2017-09-26T14:47:24.957725: step 7113, loss 0.000314454, acc 1\n",
      "2017-09-26T14:47:25.185838: step 7114, loss 0.000117611, acc 1\n",
      "2017-09-26T14:47:25.416816: step 7115, loss 0.00014729, acc 1\n",
      "2017-09-26T14:47:25.647420: step 7116, loss 6.15304e-05, acc 1\n",
      "2017-09-26T14:47:25.876689: step 7117, loss 0.00211766, acc 1\n",
      "2017-09-26T14:47:26.108619: step 7118, loss 4.48944e-05, acc 1\n",
      "2017-09-26T14:47:26.338885: step 7119, loss 5.94069e-05, acc 1\n",
      "2017-09-26T14:47:26.578233: step 7120, loss 0.000414461, acc 1\n",
      "2017-09-26T14:47:26.811724: step 7121, loss 0.00164818, acc 1\n",
      "2017-09-26T14:47:27.043579: step 7122, loss 2.86468e-05, acc 1\n",
      "2017-09-26T14:47:27.281412: step 7123, loss 0.000133648, acc 1\n",
      "2017-09-26T14:47:27.509412: step 7124, loss 1.977e-05, acc 1\n",
      "2017-09-26T14:47:27.741933: step 7125, loss 1.24343e-05, acc 1\n",
      "2017-09-26T14:47:27.971168: step 7126, loss 4.20048e-05, acc 1\n",
      "2017-09-26T14:47:28.200068: step 7127, loss 0.000113255, acc 1\n",
      "2017-09-26T14:47:28.427035: step 7128, loss 0.00020342, acc 1\n",
      "2017-09-26T14:47:28.655257: step 7129, loss 0.000331786, acc 1\n",
      "2017-09-26T14:47:28.882811: step 7130, loss 1.10859e-05, acc 1\n",
      "2017-09-26T14:47:29.115162: step 7131, loss 8.37636e-05, acc 1\n",
      "2017-09-26T14:47:29.355590: step 7132, loss 5.24256e-05, acc 1\n",
      "2017-09-26T14:47:29.587537: step 7133, loss 0.000438986, acc 1\n",
      "2017-09-26T14:47:29.821943: step 7134, loss 3.6598e-05, acc 1\n",
      "2017-09-26T14:47:30.051194: step 7135, loss 0.00336368, acc 1\n",
      "2017-09-26T14:47:30.287522: step 7136, loss 0.000249698, acc 1\n",
      "2017-09-26T14:47:30.518288: step 7137, loss 0.00464588, acc 1\n",
      "2017-09-26T14:47:30.748680: step 7138, loss 0.00258214, acc 1\n",
      "2017-09-26T14:47:30.975357: step 7139, loss 1.8389e-05, acc 1\n",
      "2017-09-26T14:47:31.174544: step 7140, loss 9.09394e-06, acc 1\n",
      "2017-09-26T14:47:31.408819: step 7141, loss 0.000186087, acc 1\n",
      "2017-09-26T14:47:31.636598: step 7142, loss 0.000159783, acc 1\n",
      "2017-09-26T14:47:31.865201: step 7143, loss 2.32573e-05, acc 1\n",
      "2017-09-26T14:47:32.096889: step 7144, loss 0.000318411, acc 1\n",
      "2017-09-26T14:47:32.327223: step 7145, loss 0.000948995, acc 1\n",
      "2017-09-26T14:47:32.562478: step 7146, loss 0.00255632, acc 1\n",
      "2017-09-26T14:47:32.801966: step 7147, loss 0.000160435, acc 1\n",
      "2017-09-26T14:47:33.041718: step 7148, loss 0.000246797, acc 1\n",
      "2017-09-26T14:47:33.282457: step 7149, loss 0.00126461, acc 1\n",
      "2017-09-26T14:47:33.519010: step 7150, loss 3.22534e-05, acc 1\n",
      "2017-09-26T14:47:33.752322: step 7151, loss 1.72298e-05, acc 1\n",
      "2017-09-26T14:47:33.992189: step 7152, loss 7.69938e-05, acc 1\n",
      "2017-09-26T14:47:34.262413: step 7153, loss 0.000216261, acc 1\n",
      "2017-09-26T14:47:34.527057: step 7154, loss 0.00137859, acc 1\n",
      "2017-09-26T14:47:34.760838: step 7155, loss 0.000174409, acc 1\n",
      "2017-09-26T14:47:34.997652: step 7156, loss 3.51452e-05, acc 1\n",
      "2017-09-26T14:47:35.228388: step 7157, loss 0.00045328, acc 1\n",
      "2017-09-26T14:47:35.463923: step 7158, loss 6.37604e-05, acc 1\n",
      "2017-09-26T14:47:35.695904: step 7159, loss 2.93833e-05, acc 1\n",
      "2017-09-26T14:47:35.924575: step 7160, loss 2.91356e-05, acc 1\n",
      "2017-09-26T14:47:36.153722: step 7161, loss 0.00065868, acc 1\n",
      "2017-09-26T14:47:36.389736: step 7162, loss 1.88665e-05, acc 1\n",
      "2017-09-26T14:47:36.618954: step 7163, loss 1.45595e-05, acc 1\n",
      "2017-09-26T14:47:36.873696: step 7164, loss 2.81271e-05, acc 1\n",
      "2017-09-26T14:47:37.105438: step 7165, loss 3.13181e-05, acc 1\n",
      "2017-09-26T14:47:37.343364: step 7166, loss 6.73886e-05, acc 1\n",
      "2017-09-26T14:47:37.588738: step 7167, loss 0.0104831, acc 1\n",
      "2017-09-26T14:47:37.819297: step 7168, loss 3.96426e-05, acc 1\n",
      "2017-09-26T14:47:38.046150: step 7169, loss 1.22703e-05, acc 1\n",
      "2017-09-26T14:47:38.285086: step 7170, loss 0.000101095, acc 1\n",
      "2017-09-26T14:47:38.514172: step 7171, loss 0.000164337, acc 1\n",
      "2017-09-26T14:47:38.747056: step 7172, loss 0.0801112, acc 0.984375\n",
      "2017-09-26T14:47:38.976551: step 7173, loss 6.11854e-05, acc 1\n",
      "2017-09-26T14:47:39.207197: step 7174, loss 6.80353e-05, acc 1\n",
      "2017-09-26T14:47:39.435008: step 7175, loss 7.79473e-05, acc 1\n",
      "2017-09-26T14:47:39.663882: step 7176, loss 0.000147505, acc 1\n",
      "2017-09-26T14:47:39.893470: step 7177, loss 0.0211958, acc 0.984375\n",
      "2017-09-26T14:47:40.126705: step 7178, loss 0.000131972, acc 1\n",
      "2017-09-26T14:47:40.373392: step 7179, loss 0.000134802, acc 1\n",
      "2017-09-26T14:47:40.601716: step 7180, loss 4.87583e-05, acc 1\n",
      "2017-09-26T14:47:40.831042: step 7181, loss 0.000240584, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:47:41.033541: step 7182, loss 0.000115319, acc 1\n",
      "2017-09-26T14:47:41.264333: step 7183, loss 0.0381361, acc 0.984375\n",
      "2017-09-26T14:47:41.493053: step 7184, loss 0.000153802, acc 1\n",
      "2017-09-26T14:47:41.724546: step 7185, loss 0.000308094, acc 1\n",
      "2017-09-26T14:47:41.957646: step 7186, loss 0.00021593, acc 1\n",
      "2017-09-26T14:47:42.206063: step 7187, loss 0.000440443, acc 1\n",
      "2017-09-26T14:47:42.446999: step 7188, loss 9.37886e-05, acc 1\n",
      "2017-09-26T14:47:42.675723: step 7189, loss 4.18257e-05, acc 1\n",
      "2017-09-26T14:47:42.901859: step 7190, loss 0.000139309, acc 1\n",
      "2017-09-26T14:47:43.133440: step 7191, loss 0.000481267, acc 1\n",
      "2017-09-26T14:47:43.360935: step 7192, loss 0.000626225, acc 1\n",
      "2017-09-26T14:47:43.595150: step 7193, loss 0.000128038, acc 1\n",
      "2017-09-26T14:47:43.827217: step 7194, loss 0.000708019, acc 1\n",
      "2017-09-26T14:47:44.076059: step 7195, loss 2.55816e-05, acc 1\n",
      "2017-09-26T14:47:44.355993: step 7196, loss 0.000264091, acc 1\n",
      "2017-09-26T14:47:44.642096: step 7197, loss 0.000423194, acc 1\n",
      "2017-09-26T14:47:44.977400: step 7198, loss 3.85519e-05, acc 1\n",
      "2017-09-26T14:47:45.302717: step 7199, loss 0.000611832, acc 1\n",
      "2017-09-26T14:47:45.665758: step 7200, loss 0.000247829, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:47:45.988447: step 7200, loss 0.801869, acc 0.875421\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7200\n",
      "\n",
      "2017-09-26T14:47:46.554077: step 7201, loss 0.0833473, acc 0.984375\n",
      "2017-09-26T14:47:46.845908: step 7202, loss 0.000300974, acc 1\n",
      "2017-09-26T14:47:47.132639: step 7203, loss 0.000135304, acc 1\n",
      "2017-09-26T14:47:47.416401: step 7204, loss 0.000135487, acc 1\n",
      "2017-09-26T14:47:47.715444: step 7205, loss 0.0029253, acc 1\n",
      "2017-09-26T14:47:48.000288: step 7206, loss 0.0139064, acc 0.984375\n",
      "2017-09-26T14:47:48.277420: step 7207, loss 0.000303021, acc 1\n",
      "2017-09-26T14:47:48.570673: step 7208, loss 0.000473166, acc 1\n",
      "2017-09-26T14:47:48.911905: step 7209, loss 0.000187971, acc 1\n",
      "2017-09-26T14:47:49.260502: step 7210, loss 1.09962e-05, acc 1\n",
      "2017-09-26T14:47:49.608245: step 7211, loss 0.000125281, acc 1\n",
      "2017-09-26T14:47:49.904581: step 7212, loss 5.40266e-05, acc 1\n",
      "2017-09-26T14:47:50.211264: step 7213, loss 8.51859e-05, acc 1\n",
      "2017-09-26T14:47:50.512738: step 7214, loss 0.000235513, acc 1\n",
      "2017-09-26T14:47:50.838081: step 7215, loss 0.000231173, acc 1\n",
      "2017-09-26T14:47:51.222158: step 7216, loss 0.000138698, acc 1\n",
      "2017-09-26T14:47:51.522839: step 7217, loss 0.000688811, acc 1\n",
      "2017-09-26T14:47:51.802371: step 7218, loss 0.000208583, acc 1\n",
      "2017-09-26T14:47:52.116087: step 7219, loss 5.54269e-05, acc 1\n",
      "2017-09-26T14:47:52.429699: step 7220, loss 8.27533e-05, acc 1\n",
      "2017-09-26T14:47:52.746071: step 7221, loss 0.000242572, acc 1\n",
      "2017-09-26T14:47:53.025005: step 7222, loss 0.000100975, acc 1\n",
      "2017-09-26T14:47:53.262154: step 7223, loss 0.000956528, acc 1\n",
      "2017-09-26T14:47:53.460869: step 7224, loss 3.9803e-05, acc 1\n",
      "2017-09-26T14:47:53.700512: step 7225, loss 6.15565e-05, acc 1\n",
      "2017-09-26T14:47:53.934749: step 7226, loss 0.000181616, acc 1\n",
      "2017-09-26T14:47:54.168910: step 7227, loss 0.000150679, acc 1\n",
      "2017-09-26T14:47:54.398800: step 7228, loss 0.000188367, acc 1\n",
      "2017-09-26T14:47:54.631434: step 7229, loss 0.000198837, acc 1\n",
      "2017-09-26T14:47:54.865409: step 7230, loss 0.000192703, acc 1\n",
      "2017-09-26T14:47:55.124069: step 7231, loss 0.000197574, acc 1\n",
      "2017-09-26T14:47:55.402207: step 7232, loss 7.29134e-05, acc 1\n",
      "2017-09-26T14:47:55.663222: step 7233, loss 0.000609321, acc 1\n",
      "2017-09-26T14:47:55.919388: step 7234, loss 0.000379009, acc 1\n",
      "2017-09-26T14:47:56.166081: step 7235, loss 0.00245443, acc 1\n",
      "2017-09-26T14:47:56.403219: step 7236, loss 0.000742282, acc 1\n",
      "2017-09-26T14:47:56.634182: step 7237, loss 0.00015835, acc 1\n",
      "2017-09-26T14:47:56.863819: step 7238, loss 0.0129857, acc 0.984375\n",
      "2017-09-26T14:47:57.093082: step 7239, loss 1.92509e-05, acc 1\n",
      "2017-09-26T14:47:57.328481: step 7240, loss 0.00133641, acc 1\n",
      "2017-09-26T14:47:57.554095: step 7241, loss 0.000847063, acc 1\n",
      "2017-09-26T14:47:57.787543: step 7242, loss 7.47946e-05, acc 1\n",
      "2017-09-26T14:47:58.016747: step 7243, loss 0.00253963, acc 1\n",
      "2017-09-26T14:47:58.264087: step 7244, loss 0.000109929, acc 1\n",
      "2017-09-26T14:47:58.489944: step 7245, loss 0.000104092, acc 1\n",
      "2017-09-26T14:47:58.717710: step 7246, loss 0.000115273, acc 1\n",
      "2017-09-26T14:47:58.959277: step 7247, loss 0.000113242, acc 1\n",
      "2017-09-26T14:47:59.185007: step 7248, loss 0.000158987, acc 1\n",
      "2017-09-26T14:47:59.413430: step 7249, loss 5.6624e-05, acc 1\n",
      "2017-09-26T14:47:59.641622: step 7250, loss 9.54605e-05, acc 1\n",
      "2017-09-26T14:47:59.871491: step 7251, loss 6.98242e-05, acc 1\n",
      "2017-09-26T14:48:00.101943: step 7252, loss 9.66203e-05, acc 1\n",
      "2017-09-26T14:48:00.376303: step 7253, loss 0.000337397, acc 1\n",
      "2017-09-26T14:48:00.641724: step 7254, loss 2.31891e-05, acc 1\n",
      "2017-09-26T14:48:00.872024: step 7255, loss 0.000139208, acc 1\n",
      "2017-09-26T14:48:01.108341: step 7256, loss 0.0002642, acc 1\n",
      "2017-09-26T14:48:01.356579: step 7257, loss 0.000317379, acc 1\n",
      "2017-09-26T14:48:01.588941: step 7258, loss 2.60544e-05, acc 1\n",
      "2017-09-26T14:48:01.817595: step 7259, loss 0.0328796, acc 0.984375\n",
      "2017-09-26T14:48:02.047298: step 7260, loss 0.000187998, acc 1\n",
      "2017-09-26T14:48:02.273944: step 7261, loss 0.0010554, acc 1\n",
      "2017-09-26T14:48:02.501440: step 7262, loss 0.000104988, acc 1\n",
      "2017-09-26T14:48:02.734708: step 7263, loss 6.90645e-05, acc 1\n",
      "2017-09-26T14:48:02.974046: step 7264, loss 0.0580081, acc 0.984375\n",
      "2017-09-26T14:48:03.202312: step 7265, loss 8.50481e-05, acc 1\n",
      "2017-09-26T14:48:03.400408: step 7266, loss 0.000168184, acc 1\n",
      "2017-09-26T14:48:03.629722: step 7267, loss 0.00408059, acc 1\n",
      "2017-09-26T14:48:03.857013: step 7268, loss 0.000127202, acc 1\n",
      "2017-09-26T14:48:04.088190: step 7269, loss 2.97935e-05, acc 1\n",
      "2017-09-26T14:48:04.320548: step 7270, loss 3.26682e-05, acc 1\n",
      "2017-09-26T14:48:04.547371: step 7271, loss 4.80563e-05, acc 1\n",
      "2017-09-26T14:48:04.775511: step 7272, loss 0.0236799, acc 0.984375\n",
      "2017-09-26T14:48:05.011479: step 7273, loss 0.000164127, acc 1\n",
      "2017-09-26T14:48:05.245927: step 7274, loss 7.91461e-05, acc 1\n",
      "2017-09-26T14:48:05.476033: step 7275, loss 8.73806e-05, acc 1\n",
      "2017-09-26T14:48:05.707486: step 7276, loss 0.0109656, acc 0.984375\n",
      "2017-09-26T14:48:05.946776: step 7277, loss 9.42069e-05, acc 1\n",
      "2017-09-26T14:48:06.186669: step 7278, loss 5.18244e-05, acc 1\n",
      "2017-09-26T14:48:06.416924: step 7279, loss 0.000214741, acc 1\n",
      "2017-09-26T14:48:06.651140: step 7280, loss 3.05664e-05, acc 1\n",
      "2017-09-26T14:48:06.882048: step 7281, loss 0.000747349, acc 1\n",
      "2017-09-26T14:48:07.115608: step 7282, loss 0.000101522, acc 1\n",
      "2017-09-26T14:48:07.356720: step 7283, loss 0.00208116, acc 1\n",
      "2017-09-26T14:48:07.583404: step 7284, loss 5.92486e-05, acc 1\n",
      "2017-09-26T14:48:07.810853: step 7285, loss 0.000251606, acc 1\n",
      "2017-09-26T14:48:08.039627: step 7286, loss 0.000541078, acc 1\n",
      "2017-09-26T14:48:08.269606: step 7287, loss 0.000276121, acc 1\n",
      "2017-09-26T14:48:08.497758: step 7288, loss 0.000329057, acc 1\n",
      "2017-09-26T14:48:08.725791: step 7289, loss 0.00022336, acc 1\n",
      "2017-09-26T14:48:08.953160: step 7290, loss 7.64986e-05, acc 1\n",
      "2017-09-26T14:48:09.184341: step 7291, loss 0.00168441, acc 1\n",
      "2017-09-26T14:48:09.417170: step 7292, loss 0.000430241, acc 1\n",
      "2017-09-26T14:48:09.650645: step 7293, loss 0.0235839, acc 0.984375\n",
      "2017-09-26T14:48:09.906027: step 7294, loss 0.0001623, acc 1\n",
      "2017-09-26T14:48:10.157761: step 7295, loss 0.0130549, acc 0.984375\n",
      "2017-09-26T14:48:10.423213: step 7296, loss 0.00233472, acc 1\n",
      "2017-09-26T14:48:10.659446: step 7297, loss 3.50907e-05, acc 1\n",
      "2017-09-26T14:48:10.892678: step 7298, loss 0.0011487, acc 1\n",
      "2017-09-26T14:48:11.137968: step 7299, loss 0.000122339, acc 1\n",
      "2017-09-26T14:48:11.380403: step 7300, loss 8.54938e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:48:11.622448: step 7300, loss 0.818468, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7300\n",
      "\n",
      "2017-09-26T14:48:12.115632: step 7301, loss 0.00254344, acc 1\n",
      "2017-09-26T14:48:12.348360: step 7302, loss 0.000405079, acc 1\n",
      "2017-09-26T14:48:12.585125: step 7303, loss 5.7959e-05, acc 1\n",
      "2017-09-26T14:48:12.818118: step 7304, loss 0.000215664, acc 1\n",
      "2017-09-26T14:48:13.054868: step 7305, loss 0.000588177, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:48:13.307613: step 7306, loss 4.4527e-05, acc 1\n",
      "2017-09-26T14:48:13.535745: step 7307, loss 0.000126938, acc 1\n",
      "2017-09-26T14:48:13.744090: step 7308, loss 0.000179677, acc 1\n",
      "2017-09-26T14:48:13.982740: step 7309, loss 0.00106206, acc 1\n",
      "2017-09-26T14:48:14.219504: step 7310, loss 6.25258e-05, acc 1\n",
      "2017-09-26T14:48:14.456091: step 7311, loss 1.15954e-05, acc 1\n",
      "2017-09-26T14:48:14.692130: step 7312, loss 7.2004e-05, acc 1\n",
      "2017-09-26T14:48:14.929415: step 7313, loss 0.00233798, acc 1\n",
      "2017-09-26T14:48:15.174454: step 7314, loss 0.000158, acc 1\n",
      "2017-09-26T14:48:15.420889: step 7315, loss 5.1348e-05, acc 1\n",
      "2017-09-26T14:48:15.653639: step 7316, loss 0.00022401, acc 1\n",
      "2017-09-26T14:48:15.887001: step 7317, loss 1.87467e-05, acc 1\n",
      "2017-09-26T14:48:16.117226: step 7318, loss 0.000512976, acc 1\n",
      "2017-09-26T14:48:16.346259: step 7319, loss 0.00233433, acc 1\n",
      "2017-09-26T14:48:16.578801: step 7320, loss 0.00031785, acc 1\n",
      "2017-09-26T14:48:16.810756: step 7321, loss 5.48832e-05, acc 1\n",
      "2017-09-26T14:48:17.055126: step 7322, loss 0.00407743, acc 1\n",
      "2017-09-26T14:48:17.304850: step 7323, loss 0.000228981, acc 1\n",
      "2017-09-26T14:48:17.588198: step 7324, loss 0.000131178, acc 1\n",
      "2017-09-26T14:48:17.828198: step 7325, loss 0.000874632, acc 1\n",
      "2017-09-26T14:48:18.060362: step 7326, loss 0.000156115, acc 1\n",
      "2017-09-26T14:48:18.294730: step 7327, loss 0.000159026, acc 1\n",
      "2017-09-26T14:48:18.527593: step 7328, loss 0.000183745, acc 1\n",
      "2017-09-26T14:48:18.758439: step 7329, loss 0.000378958, acc 1\n",
      "2017-09-26T14:48:18.987336: step 7330, loss 0.000499785, acc 1\n",
      "2017-09-26T14:48:19.216952: step 7331, loss 0.000190044, acc 1\n",
      "2017-09-26T14:48:19.447536: step 7332, loss 0.0261308, acc 0.984375\n",
      "2017-09-26T14:48:19.684414: step 7333, loss 0.000402216, acc 1\n",
      "2017-09-26T14:48:19.926795: step 7334, loss 0.000219085, acc 1\n",
      "2017-09-26T14:48:20.161699: step 7335, loss 0.000416894, acc 1\n",
      "2017-09-26T14:48:20.394990: step 7336, loss 5.47101e-05, acc 1\n",
      "2017-09-26T14:48:20.628605: step 7337, loss 7.0117e-05, acc 1\n",
      "2017-09-26T14:48:20.856485: step 7338, loss 2.05961e-05, acc 1\n",
      "2017-09-26T14:48:21.089739: step 7339, loss 6.60073e-05, acc 1\n",
      "2017-09-26T14:48:21.334052: step 7340, loss 2.95561e-05, acc 1\n",
      "2017-09-26T14:48:21.561925: step 7341, loss 0.000108212, acc 1\n",
      "2017-09-26T14:48:21.791930: step 7342, loss 0.000131505, acc 1\n",
      "2017-09-26T14:48:22.022130: step 7343, loss 0.000186519, acc 1\n",
      "2017-09-26T14:48:22.269142: step 7344, loss 3.55244e-05, acc 1\n",
      "2017-09-26T14:48:22.501265: step 7345, loss 0.000110733, acc 1\n",
      "2017-09-26T14:48:22.730292: step 7346, loss 2.45144e-05, acc 1\n",
      "2017-09-26T14:48:22.957034: step 7347, loss 1.42297e-05, acc 1\n",
      "2017-09-26T14:48:23.187163: step 7348, loss 0.000190395, acc 1\n",
      "2017-09-26T14:48:23.434026: step 7349, loss 0.000230127, acc 1\n",
      "2017-09-26T14:48:23.639818: step 7350, loss 7.104e-05, acc 1\n",
      "2017-09-26T14:48:23.869658: step 7351, loss 5.25311e-05, acc 1\n",
      "2017-09-26T14:48:24.101933: step 7352, loss 8.10377e-05, acc 1\n",
      "2017-09-26T14:48:24.331906: step 7353, loss 5.40432e-05, acc 1\n",
      "2017-09-26T14:48:24.560359: step 7354, loss 0.0404947, acc 0.984375\n",
      "2017-09-26T14:48:24.789829: step 7355, loss 0.000255583, acc 1\n",
      "2017-09-26T14:48:25.017791: step 7356, loss 0.000699517, acc 1\n",
      "2017-09-26T14:48:25.257434: step 7357, loss 6.37862e-05, acc 1\n",
      "2017-09-26T14:48:25.491295: step 7358, loss 0.00132008, acc 1\n",
      "2017-09-26T14:48:25.753624: step 7359, loss 0.000108934, acc 1\n",
      "2017-09-26T14:48:25.986505: step 7360, loss 0.0016877, acc 1\n",
      "2017-09-26T14:48:26.226539: step 7361, loss 0.000530881, acc 1\n",
      "2017-09-26T14:48:26.464856: step 7362, loss 2.31377e-05, acc 1\n",
      "2017-09-26T14:48:26.726718: step 7363, loss 0.000164895, acc 1\n",
      "2017-09-26T14:48:26.959855: step 7364, loss 2.11687e-05, acc 1\n",
      "2017-09-26T14:48:27.190818: step 7365, loss 0.00163877, acc 1\n",
      "2017-09-26T14:48:27.418744: step 7366, loss 2.85254e-05, acc 1\n",
      "2017-09-26T14:48:27.647296: step 7367, loss 0.00108251, acc 1\n",
      "2017-09-26T14:48:27.881203: step 7368, loss 8.5983e-05, acc 1\n",
      "2017-09-26T14:48:28.113123: step 7369, loss 1.74521e-05, acc 1\n",
      "2017-09-26T14:48:28.379955: step 7370, loss 6.30189e-05, acc 1\n",
      "2017-09-26T14:48:28.616033: step 7371, loss 0.000565932, acc 1\n",
      "2017-09-26T14:48:28.849984: step 7372, loss 3.46405e-05, acc 1\n",
      "2017-09-26T14:48:29.086399: step 7373, loss 0.000115154, acc 1\n",
      "2017-09-26T14:48:29.327272: step 7374, loss 7.15554e-05, acc 1\n",
      "2017-09-26T14:48:29.565937: step 7375, loss 2.3604e-05, acc 1\n",
      "2017-09-26T14:48:29.809514: step 7376, loss 0.000126241, acc 1\n",
      "2017-09-26T14:48:30.051779: step 7377, loss 2.18544e-05, acc 1\n",
      "2017-09-26T14:48:30.293310: step 7378, loss 7.5699e-05, acc 1\n",
      "2017-09-26T14:48:30.526630: step 7379, loss 0.0536672, acc 0.984375\n",
      "2017-09-26T14:48:30.760939: step 7380, loss 2.15654e-05, acc 1\n",
      "2017-09-26T14:48:31.007009: step 7381, loss 0.000289573, acc 1\n",
      "2017-09-26T14:48:31.259533: step 7382, loss 0.000109999, acc 1\n",
      "2017-09-26T14:48:31.520607: step 7383, loss 4.22469e-05, acc 1\n",
      "2017-09-26T14:48:31.793513: step 7384, loss 0.000121322, acc 1\n",
      "2017-09-26T14:48:32.028421: step 7385, loss 9.48797e-05, acc 1\n",
      "2017-09-26T14:48:32.289838: step 7386, loss 0.000257024, acc 1\n",
      "2017-09-26T14:48:32.529534: step 7387, loss 8.71592e-06, acc 1\n",
      "2017-09-26T14:48:32.762273: step 7388, loss 5.9419e-05, acc 1\n",
      "2017-09-26T14:48:32.989956: step 7389, loss 0.00108814, acc 1\n",
      "2017-09-26T14:48:33.219176: step 7390, loss 2.50093e-05, acc 1\n",
      "2017-09-26T14:48:33.454304: step 7391, loss 2.88072e-05, acc 1\n",
      "2017-09-26T14:48:33.680276: step 7392, loss 0.000430537, acc 1\n",
      "2017-09-26T14:48:33.925896: step 7393, loss 0.000105492, acc 1\n",
      "2017-09-26T14:48:34.157760: step 7394, loss 3.75888e-05, acc 1\n",
      "2017-09-26T14:48:34.390920: step 7395, loss 2.93293e-05, acc 1\n",
      "2017-09-26T14:48:34.623386: step 7396, loss 3.97313e-05, acc 1\n",
      "2017-09-26T14:48:34.854383: step 7397, loss 2.46779e-05, acc 1\n",
      "2017-09-26T14:48:35.097993: step 7398, loss 0.000386488, acc 1\n",
      "2017-09-26T14:48:35.329328: step 7399, loss 0.00685095, acc 1\n",
      "2017-09-26T14:48:35.564116: step 7400, loss 3.12539e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:48:35.791847: step 7400, loss 0.775126, acc 0.882155\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7400\n",
      "\n",
      "2017-09-26T14:48:36.279372: step 7401, loss 1.13058e-05, acc 1\n",
      "2017-09-26T14:48:36.511019: step 7402, loss 3.06023e-05, acc 1\n",
      "2017-09-26T14:48:36.756446: step 7403, loss 0.000199233, acc 1\n",
      "2017-09-26T14:48:36.982934: step 7404, loss 0.000161021, acc 1\n",
      "2017-09-26T14:48:37.213621: step 7405, loss 1.59777e-05, acc 1\n",
      "2017-09-26T14:48:37.447346: step 7406, loss 8.70461e-05, acc 1\n",
      "2017-09-26T14:48:37.679796: step 7407, loss 7.53555e-05, acc 1\n",
      "2017-09-26T14:48:37.913495: step 7408, loss 3.7248e-05, acc 1\n",
      "2017-09-26T14:48:38.149689: step 7409, loss 2.63498e-05, acc 1\n",
      "2017-09-26T14:48:38.389079: step 7410, loss 6.754e-05, acc 1\n",
      "2017-09-26T14:48:38.619767: step 7411, loss 4.38784e-05, acc 1\n",
      "2017-09-26T14:48:38.849543: step 7412, loss 4.68773e-05, acc 1\n",
      "2017-09-26T14:48:39.079873: step 7413, loss 3.0566e-05, acc 1\n",
      "2017-09-26T14:48:39.321223: step 7414, loss 0.000469872, acc 1\n",
      "2017-09-26T14:48:39.557109: step 7415, loss 0.000165208, acc 1\n",
      "2017-09-26T14:48:39.790672: step 7416, loss 0.000142433, acc 1\n",
      "2017-09-26T14:48:40.023028: step 7417, loss 5.6013e-05, acc 1\n",
      "2017-09-26T14:48:40.260756: step 7418, loss 0.000611439, acc 1\n",
      "2017-09-26T14:48:40.494102: step 7419, loss 0.000107283, acc 1\n",
      "2017-09-26T14:48:40.727873: step 7420, loss 0.00632094, acc 1\n",
      "2017-09-26T14:48:40.963069: step 7421, loss 0.000117047, acc 1\n",
      "2017-09-26T14:48:41.194403: step 7422, loss 5.42665e-05, acc 1\n",
      "2017-09-26T14:48:41.423299: step 7423, loss 3.30635e-05, acc 1\n",
      "2017-09-26T14:48:41.656476: step 7424, loss 4.50956e-05, acc 1\n",
      "2017-09-26T14:48:41.884699: step 7425, loss 8.88305e-05, acc 1\n",
      "2017-09-26T14:48:42.116786: step 7426, loss 2.17854e-05, acc 1\n",
      "2017-09-26T14:48:42.361002: step 7427, loss 1.40393e-05, acc 1\n",
      "2017-09-26T14:48:42.588218: step 7428, loss 0.000447049, acc 1\n",
      "2017-09-26T14:48:42.821975: step 7429, loss 0.000159293, acc 1\n",
      "2017-09-26T14:48:43.053142: step 7430, loss 5.05698e-06, acc 1\n",
      "2017-09-26T14:48:43.295166: step 7431, loss 1.67284e-05, acc 1\n",
      "2017-09-26T14:48:43.533240: step 7432, loss 0.000112827, acc 1\n",
      "2017-09-26T14:48:43.764767: step 7433, loss 1.44778e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:48:43.974392: step 7434, loss 7.24364e-05, acc 1\n",
      "2017-09-26T14:48:44.201493: step 7435, loss 1.28548e-05, acc 1\n",
      "2017-09-26T14:48:44.429143: step 7436, loss 8.08699e-05, acc 1\n",
      "2017-09-26T14:48:44.657637: step 7437, loss 3.92181e-05, acc 1\n",
      "2017-09-26T14:48:44.889653: step 7438, loss 3.3016e-05, acc 1\n",
      "2017-09-26T14:48:45.121225: step 7439, loss 0.000607478, acc 1\n",
      "2017-09-26T14:48:45.371951: step 7440, loss 3.61461e-05, acc 1\n",
      "2017-09-26T14:48:45.601435: step 7441, loss 2.16626e-05, acc 1\n",
      "2017-09-26T14:48:45.833087: step 7442, loss 5.16067e-05, acc 1\n",
      "2017-09-26T14:48:46.107277: step 7443, loss 9.02469e-05, acc 1\n",
      "2017-09-26T14:48:46.375123: step 7444, loss 0.000308505, acc 1\n",
      "2017-09-26T14:48:46.607686: step 7445, loss 0.00178105, acc 1\n",
      "2017-09-26T14:48:46.854801: step 7446, loss 0.0121923, acc 0.984375\n",
      "2017-09-26T14:48:47.102917: step 7447, loss 3.49467e-05, acc 1\n",
      "2017-09-26T14:48:47.335059: step 7448, loss 3.2634e-05, acc 1\n",
      "2017-09-26T14:48:47.565347: step 7449, loss 0.000102178, acc 1\n",
      "2017-09-26T14:48:47.797465: step 7450, loss 6.71252e-06, acc 1\n",
      "2017-09-26T14:48:48.036114: step 7451, loss 0.000226672, acc 1\n",
      "2017-09-26T14:48:48.307965: step 7452, loss 5.62165e-05, acc 1\n",
      "2017-09-26T14:48:48.567562: step 7453, loss 8.25846e-05, acc 1\n",
      "2017-09-26T14:48:48.825492: step 7454, loss 0.000170591, acc 1\n",
      "2017-09-26T14:48:49.072640: step 7455, loss 6.15979e-05, acc 1\n",
      "2017-09-26T14:48:49.307263: step 7456, loss 0.000362595, acc 1\n",
      "2017-09-26T14:48:49.536953: step 7457, loss 2.63003e-06, acc 1\n",
      "2017-09-26T14:48:49.773096: step 7458, loss 0.00107345, acc 1\n",
      "2017-09-26T14:48:50.006444: step 7459, loss 2.2801e-05, acc 1\n",
      "2017-09-26T14:48:50.239410: step 7460, loss 7.37934e-05, acc 1\n",
      "2017-09-26T14:48:50.464615: step 7461, loss 8.37699e-05, acc 1\n",
      "2017-09-26T14:48:50.694429: step 7462, loss 4.80175e-05, acc 1\n",
      "2017-09-26T14:48:50.921028: step 7463, loss 0.000244785, acc 1\n",
      "2017-09-26T14:48:51.147217: step 7464, loss 4.88766e-05, acc 1\n",
      "2017-09-26T14:48:51.377633: step 7465, loss 3.24042e-05, acc 1\n",
      "2017-09-26T14:48:51.609135: step 7466, loss 4.30682e-05, acc 1\n",
      "2017-09-26T14:48:51.844180: step 7467, loss 5.00672e-05, acc 1\n",
      "2017-09-26T14:48:52.078048: step 7468, loss 0.000380166, acc 1\n",
      "2017-09-26T14:48:52.306929: step 7469, loss 0.000103798, acc 1\n",
      "2017-09-26T14:48:52.534734: step 7470, loss 3.92868e-05, acc 1\n",
      "2017-09-26T14:48:52.763969: step 7471, loss 0.000121942, acc 1\n",
      "2017-09-26T14:48:52.994759: step 7472, loss 0.0151277, acc 0.984375\n",
      "2017-09-26T14:48:53.229862: step 7473, loss 0.0357549, acc 0.984375\n",
      "2017-09-26T14:48:53.460675: step 7474, loss 0.00051507, acc 1\n",
      "2017-09-26T14:48:53.690596: step 7475, loss 0.000108025, acc 1\n",
      "2017-09-26T14:48:53.893443: step 7476, loss 0.000161901, acc 1\n",
      "2017-09-26T14:48:54.126964: step 7477, loss 3.60875e-05, acc 1\n",
      "2017-09-26T14:48:54.353842: step 7478, loss 1.57418e-05, acc 1\n",
      "2017-09-26T14:48:54.586040: step 7479, loss 1.38311e-05, acc 1\n",
      "2017-09-26T14:48:54.812906: step 7480, loss 0.00028183, acc 1\n",
      "2017-09-26T14:48:55.039934: step 7481, loss 0.000302683, acc 1\n",
      "2017-09-26T14:48:55.276613: step 7482, loss 0.000797572, acc 1\n",
      "2017-09-26T14:48:55.505096: step 7483, loss 4.21608e-05, acc 1\n",
      "2017-09-26T14:48:55.732167: step 7484, loss 2.2818e-05, acc 1\n",
      "2017-09-26T14:48:55.963009: step 7485, loss 7.13726e-05, acc 1\n",
      "2017-09-26T14:48:56.190997: step 7486, loss 2.86297e-05, acc 1\n",
      "2017-09-26T14:48:56.419515: step 7487, loss 0.000822863, acc 1\n",
      "2017-09-26T14:48:56.650164: step 7488, loss 0.0004112, acc 1\n",
      "2017-09-26T14:48:56.879491: step 7489, loss 1.81742e-05, acc 1\n",
      "2017-09-26T14:48:57.110013: step 7490, loss 0.0023579, acc 1\n",
      "2017-09-26T14:48:57.343996: step 7491, loss 0.000160397, acc 1\n",
      "2017-09-26T14:48:57.573319: step 7492, loss 0.000396142, acc 1\n",
      "2017-09-26T14:48:57.801717: step 7493, loss 0.000169056, acc 1\n",
      "2017-09-26T14:48:58.028775: step 7494, loss 0.0212318, acc 0.984375\n",
      "2017-09-26T14:48:58.263974: step 7495, loss 3.38139e-05, acc 1\n",
      "2017-09-26T14:48:58.489806: step 7496, loss 0.000280602, acc 1\n",
      "2017-09-26T14:48:58.720034: step 7497, loss 0.00017177, acc 1\n",
      "2017-09-26T14:48:58.952746: step 7498, loss 0.000460272, acc 1\n",
      "2017-09-26T14:48:59.182009: step 7499, loss 0.0153064, acc 0.984375\n",
      "2017-09-26T14:48:59.412369: step 7500, loss 0.00149402, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:48:59.640653: step 7500, loss 0.940584, acc 0.875421\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7500\n",
      "\n",
      "2017-09-26T14:49:00.118785: step 7501, loss 0.000388249, acc 1\n",
      "2017-09-26T14:49:00.350945: step 7502, loss 1.39362e-05, acc 1\n",
      "2017-09-26T14:49:00.580423: step 7503, loss 3.53307e-05, acc 1\n",
      "2017-09-26T14:49:00.814617: step 7504, loss 0.000275259, acc 1\n",
      "2017-09-26T14:49:01.042764: step 7505, loss 2.72844e-05, acc 1\n",
      "2017-09-26T14:49:01.279513: step 7506, loss 0.000204865, acc 1\n",
      "2017-09-26T14:49:01.507287: step 7507, loss 0.000197916, acc 1\n",
      "2017-09-26T14:49:01.735500: step 7508, loss 5.86288e-05, acc 1\n",
      "2017-09-26T14:49:01.963457: step 7509, loss 0.00100478, acc 1\n",
      "2017-09-26T14:49:02.196647: step 7510, loss 6.27595e-05, acc 1\n",
      "2017-09-26T14:49:02.423799: step 7511, loss 0.000120404, acc 1\n",
      "2017-09-26T14:49:02.653311: step 7512, loss 1.51137e-05, acc 1\n",
      "2017-09-26T14:49:02.886563: step 7513, loss 0.000467281, acc 1\n",
      "2017-09-26T14:49:03.133640: step 7514, loss 0.000280773, acc 1\n",
      "2017-09-26T14:49:03.398923: step 7515, loss 7.41495e-06, acc 1\n",
      "2017-09-26T14:49:03.647291: step 7516, loss 0.0306954, acc 0.984375\n",
      "2017-09-26T14:49:03.876340: step 7517, loss 0.0470165, acc 0.984375\n",
      "2017-09-26T14:49:04.076966: step 7518, loss 5.52936e-06, acc 1\n",
      "2017-09-26T14:49:04.310295: step 7519, loss 0.0197628, acc 0.984375\n",
      "2017-09-26T14:49:04.572167: step 7520, loss 7.70829e-05, acc 1\n",
      "2017-09-26T14:49:04.837853: step 7521, loss 6.68926e-05, acc 1\n",
      "2017-09-26T14:49:05.094652: step 7522, loss 9.66819e-05, acc 1\n",
      "2017-09-26T14:49:05.360928: step 7523, loss 1.83913e-05, acc 1\n",
      "2017-09-26T14:49:05.615664: step 7524, loss 2.40149e-05, acc 1\n",
      "2017-09-26T14:49:05.852984: step 7525, loss 0.000316443, acc 1\n",
      "2017-09-26T14:49:06.109692: step 7526, loss 7.92656e-05, acc 1\n",
      "2017-09-26T14:49:06.376265: step 7527, loss 3.61553e-05, acc 1\n",
      "2017-09-26T14:49:06.632325: step 7528, loss 0.000164965, acc 1\n",
      "2017-09-26T14:49:06.865365: step 7529, loss 0.000369239, acc 1\n",
      "2017-09-26T14:49:07.093306: step 7530, loss 0.000954991, acc 1\n",
      "2017-09-26T14:49:07.319306: step 7531, loss 2.76705e-05, acc 1\n",
      "2017-09-26T14:49:07.553337: step 7532, loss 2.15848e-05, acc 1\n",
      "2017-09-26T14:49:07.784350: step 7533, loss 8.02227e-05, acc 1\n",
      "2017-09-26T14:49:08.028761: step 7534, loss 4.48218e-05, acc 1\n",
      "2017-09-26T14:49:08.295310: step 7535, loss 0.000113167, acc 1\n",
      "2017-09-26T14:49:08.541996: step 7536, loss 0.000139776, acc 1\n",
      "2017-09-26T14:49:08.839973: step 7537, loss 0.000132702, acc 1\n",
      "2017-09-26T14:49:09.130234: step 7538, loss 0.000302678, acc 1\n",
      "2017-09-26T14:49:09.365252: step 7539, loss 9.35924e-05, acc 1\n",
      "2017-09-26T14:49:09.593734: step 7540, loss 1.15937e-05, acc 1\n",
      "2017-09-26T14:49:09.822042: step 7541, loss 8.56219e-06, acc 1\n",
      "2017-09-26T14:49:10.061366: step 7542, loss 0.000104046, acc 1\n",
      "2017-09-26T14:49:10.307567: step 7543, loss 3.61708e-05, acc 1\n",
      "2017-09-26T14:49:10.537681: step 7544, loss 5.61194e-05, acc 1\n",
      "2017-09-26T14:49:10.845022: step 7545, loss 0.000206243, acc 1\n",
      "2017-09-26T14:49:11.100885: step 7546, loss 0.000558159, acc 1\n",
      "2017-09-26T14:49:11.346103: step 7547, loss 0.000857483, acc 1\n",
      "2017-09-26T14:49:11.583068: step 7548, loss 3.48867e-05, acc 1\n",
      "2017-09-26T14:49:11.823028: step 7549, loss 7.75941e-05, acc 1\n",
      "2017-09-26T14:49:12.053758: step 7550, loss 0.000173538, acc 1\n",
      "2017-09-26T14:49:12.297859: step 7551, loss 9.70447e-05, acc 1\n",
      "2017-09-26T14:49:12.538911: step 7552, loss 0.00168444, acc 1\n",
      "2017-09-26T14:49:12.778375: step 7553, loss 0.000165584, acc 1\n",
      "2017-09-26T14:49:13.031129: step 7554, loss 0.018951, acc 0.984375\n",
      "2017-09-26T14:49:13.284532: step 7555, loss 0.000147263, acc 1\n",
      "2017-09-26T14:49:13.516321: step 7556, loss 3.81562e-05, acc 1\n",
      "2017-09-26T14:49:13.756022: step 7557, loss 2.44306e-05, acc 1\n",
      "2017-09-26T14:49:13.988075: step 7558, loss 0.000142178, acc 1\n",
      "2017-09-26T14:49:14.222259: step 7559, loss 6.0878e-05, acc 1\n",
      "2017-09-26T14:49:14.421255: step 7560, loss 4.65251e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:49:14.656330: step 7561, loss 0.000295435, acc 1\n",
      "2017-09-26T14:49:14.888388: step 7562, loss 1.57309e-05, acc 1\n",
      "2017-09-26T14:49:15.119668: step 7563, loss 3.87112e-05, acc 1\n",
      "2017-09-26T14:49:15.362930: step 7564, loss 0.000211124, acc 1\n",
      "2017-09-26T14:49:15.595418: step 7565, loss 0.000119384, acc 1\n",
      "2017-09-26T14:49:15.833337: step 7566, loss 0.000142272, acc 1\n",
      "2017-09-26T14:49:16.067938: step 7567, loss 6.77421e-06, acc 1\n",
      "2017-09-26T14:49:16.294715: step 7568, loss 0.000150873, acc 1\n",
      "2017-09-26T14:49:16.524902: step 7569, loss 0.000973314, acc 1\n",
      "2017-09-26T14:49:16.757127: step 7570, loss 6.46952e-05, acc 1\n",
      "2017-09-26T14:49:17.000339: step 7571, loss 5.77832e-05, acc 1\n",
      "2017-09-26T14:49:17.234658: step 7572, loss 0.000107438, acc 1\n",
      "2017-09-26T14:49:17.468740: step 7573, loss 0.00136625, acc 1\n",
      "2017-09-26T14:49:17.698547: step 7574, loss 3.65602e-05, acc 1\n",
      "2017-09-26T14:49:17.927295: step 7575, loss 0.000811962, acc 1\n",
      "2017-09-26T14:49:18.156845: step 7576, loss 5.09063e-05, acc 1\n",
      "2017-09-26T14:49:18.388379: step 7577, loss 6.08443e-05, acc 1\n",
      "2017-09-26T14:49:18.618864: step 7578, loss 0.000302283, acc 1\n",
      "2017-09-26T14:49:18.862095: step 7579, loss 0.000220086, acc 1\n",
      "2017-09-26T14:49:19.090867: step 7580, loss 2.35261e-05, acc 1\n",
      "2017-09-26T14:49:19.327756: step 7581, loss 0.000107537, acc 1\n",
      "2017-09-26T14:49:19.558778: step 7582, loss 0.00163242, acc 1\n",
      "2017-09-26T14:49:19.788058: step 7583, loss 0.00253217, acc 1\n",
      "2017-09-26T14:49:20.017695: step 7584, loss 0.00131295, acc 1\n",
      "2017-09-26T14:49:20.254651: step 7585, loss 7.27748e-05, acc 1\n",
      "2017-09-26T14:49:20.484835: step 7586, loss 0.000141943, acc 1\n",
      "2017-09-26T14:49:20.728422: step 7587, loss 3.79033e-05, acc 1\n",
      "2017-09-26T14:49:20.958529: step 7588, loss 0.00124507, acc 1\n",
      "2017-09-26T14:49:21.189809: step 7589, loss 1.59279e-05, acc 1\n",
      "2017-09-26T14:49:21.428018: step 7590, loss 4.26862e-05, acc 1\n",
      "2017-09-26T14:49:21.657624: step 7591, loss 0.00162018, acc 1\n",
      "2017-09-26T14:49:21.894270: step 7592, loss 0.0132007, acc 0.984375\n",
      "2017-09-26T14:49:22.130658: step 7593, loss 9.28911e-05, acc 1\n",
      "2017-09-26T14:49:22.380013: step 7594, loss 0.000142874, acc 1\n",
      "2017-09-26T14:49:22.658222: step 7595, loss 2.56672e-05, acc 1\n",
      "2017-09-26T14:49:22.897965: step 7596, loss 0.000202386, acc 1\n",
      "2017-09-26T14:49:23.168895: step 7597, loss 0.000524799, acc 1\n",
      "2017-09-26T14:49:23.413222: step 7598, loss 0.0529353, acc 0.984375\n",
      "2017-09-26T14:49:23.662657: step 7599, loss 9.11909e-06, acc 1\n",
      "2017-09-26T14:49:23.894790: step 7600, loss 8.33061e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:49:24.132929: step 7600, loss 0.776294, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7600\n",
      "\n",
      "2017-09-26T14:49:24.627371: step 7601, loss 4.73174e-05, acc 1\n",
      "2017-09-26T14:49:24.828132: step 7602, loss 2.88468e-05, acc 1\n",
      "2017-09-26T14:49:25.070965: step 7603, loss 0.000138225, acc 1\n",
      "2017-09-26T14:49:25.320225: step 7604, loss 6.49935e-05, acc 1\n",
      "2017-09-26T14:49:25.581947: step 7605, loss 2.51381e-05, acc 1\n",
      "2017-09-26T14:49:25.822961: step 7606, loss 9.99266e-05, acc 1\n",
      "2017-09-26T14:49:26.055582: step 7607, loss 0.00265401, acc 1\n",
      "2017-09-26T14:49:26.289724: step 7608, loss 0.00037946, acc 1\n",
      "2017-09-26T14:49:26.518913: step 7609, loss 2.73484e-05, acc 1\n",
      "2017-09-26T14:49:26.746351: step 7610, loss 0.000105815, acc 1\n",
      "2017-09-26T14:49:26.972928: step 7611, loss 8.88689e-05, acc 1\n",
      "2017-09-26T14:49:27.215702: step 7612, loss 7.48478e-05, acc 1\n",
      "2017-09-26T14:49:27.459515: step 7613, loss 5.52936e-05, acc 1\n",
      "2017-09-26T14:49:27.685943: step 7614, loss 0.000340407, acc 1\n",
      "2017-09-26T14:49:27.915427: step 7615, loss 4.61388e-05, acc 1\n",
      "2017-09-26T14:49:28.150025: step 7616, loss 0.000180307, acc 1\n",
      "2017-09-26T14:49:28.386764: step 7617, loss 0.00160189, acc 1\n",
      "2017-09-26T14:49:28.616941: step 7618, loss 3.45405e-05, acc 1\n",
      "2017-09-26T14:49:28.846202: step 7619, loss 2.97035e-05, acc 1\n",
      "2017-09-26T14:49:29.077123: step 7620, loss 5.01282e-05, acc 1\n",
      "2017-09-26T14:49:29.319663: step 7621, loss 0.00015783, acc 1\n",
      "2017-09-26T14:49:29.549634: step 7622, loss 3.28579e-05, acc 1\n",
      "2017-09-26T14:49:29.793870: step 7623, loss 0.000135264, acc 1\n",
      "2017-09-26T14:49:30.022744: step 7624, loss 0.0148539, acc 0.984375\n",
      "2017-09-26T14:49:30.253146: step 7625, loss 9.20655e-05, acc 1\n",
      "2017-09-26T14:49:30.480920: step 7626, loss 7.19601e-05, acc 1\n",
      "2017-09-26T14:49:30.709461: step 7627, loss 2.73606e-05, acc 1\n",
      "2017-09-26T14:49:30.942432: step 7628, loss 6.22457e-05, acc 1\n",
      "2017-09-26T14:49:31.169815: step 7629, loss 7.90382e-05, acc 1\n",
      "2017-09-26T14:49:31.399592: step 7630, loss 4.52635e-05, acc 1\n",
      "2017-09-26T14:49:31.631501: step 7631, loss 8.88169e-05, acc 1\n",
      "2017-09-26T14:49:31.860198: step 7632, loss 0.00572022, acc 1\n",
      "2017-09-26T14:49:32.093083: step 7633, loss 4.59537e-05, acc 1\n",
      "2017-09-26T14:49:32.331431: step 7634, loss 4.76548e-05, acc 1\n",
      "2017-09-26T14:49:32.561239: step 7635, loss 0.00426089, acc 1\n",
      "2017-09-26T14:49:32.836777: step 7636, loss 0.000525143, acc 1\n",
      "2017-09-26T14:49:33.071959: step 7637, loss 0.000480206, acc 1\n",
      "2017-09-26T14:49:33.337725: step 7638, loss 2.3265e-05, acc 1\n",
      "2017-09-26T14:49:33.607101: step 7639, loss 0.000145033, acc 1\n",
      "2017-09-26T14:49:33.861311: step 7640, loss 3.2526e-05, acc 1\n",
      "2017-09-26T14:49:34.101123: step 7641, loss 0.000159243, acc 1\n",
      "2017-09-26T14:49:34.338924: step 7642, loss 0.000203819, acc 1\n",
      "2017-09-26T14:49:34.585388: step 7643, loss 1.31454e-05, acc 1\n",
      "2017-09-26T14:49:34.812928: step 7644, loss 3.7618e-05, acc 1\n",
      "2017-09-26T14:49:35.066709: step 7645, loss 3.98092e-05, acc 1\n",
      "2017-09-26T14:49:35.307190: step 7646, loss 0.00249077, acc 1\n",
      "2017-09-26T14:49:35.537680: step 7647, loss 0.000251043, acc 1\n",
      "2017-09-26T14:49:35.781787: step 7648, loss 0.000392565, acc 1\n",
      "2017-09-26T14:49:36.020956: step 7649, loss 0.000158579, acc 1\n",
      "2017-09-26T14:49:36.252155: step 7650, loss 0.000118148, acc 1\n",
      "2017-09-26T14:49:36.485000: step 7651, loss 8.84299e-05, acc 1\n",
      "2017-09-26T14:49:36.727894: step 7652, loss 0.000117523, acc 1\n",
      "2017-09-26T14:49:36.969296: step 7653, loss 2.89264e-05, acc 1\n",
      "2017-09-26T14:49:37.204962: step 7654, loss 1.88633e-05, acc 1\n",
      "2017-09-26T14:49:37.432810: step 7655, loss 5.42314e-05, acc 1\n",
      "2017-09-26T14:49:37.660111: step 7656, loss 1.42572e-05, acc 1\n",
      "2017-09-26T14:49:37.892522: step 7657, loss 1.12948e-05, acc 1\n",
      "2017-09-26T14:49:38.123497: step 7658, loss 0.00828721, acc 1\n",
      "2017-09-26T14:49:38.353647: step 7659, loss 0.000302324, acc 1\n",
      "2017-09-26T14:49:38.585454: step 7660, loss 8.91057e-06, acc 1\n",
      "2017-09-26T14:49:38.818473: step 7661, loss 0.000447421, acc 1\n",
      "2017-09-26T14:49:39.055646: step 7662, loss 0.00029019, acc 1\n",
      "2017-09-26T14:49:39.301359: step 7663, loss 1.21755e-05, acc 1\n",
      "2017-09-26T14:49:39.542665: step 7664, loss 5.2132e-06, acc 1\n",
      "2017-09-26T14:49:39.772362: step 7665, loss 2.97666e-05, acc 1\n",
      "2017-09-26T14:49:40.002893: step 7666, loss 8.82665e-06, acc 1\n",
      "2017-09-26T14:49:40.233482: step 7667, loss 8.08731e-05, acc 1\n",
      "2017-09-26T14:49:40.463251: step 7668, loss 0.000185728, acc 1\n",
      "2017-09-26T14:49:40.696201: step 7669, loss 6.67757e-05, acc 1\n",
      "2017-09-26T14:49:40.927638: step 7670, loss 0.000200748, acc 1\n",
      "2017-09-26T14:49:41.160248: step 7671, loss 5.84519e-05, acc 1\n",
      "2017-09-26T14:49:41.392901: step 7672, loss 0.000630781, acc 1\n",
      "2017-09-26T14:49:41.624529: step 7673, loss 1.89482e-05, acc 1\n",
      "2017-09-26T14:49:41.857204: step 7674, loss 8.61212e-05, acc 1\n",
      "2017-09-26T14:49:42.090208: step 7675, loss 0.00187317, acc 1\n",
      "2017-09-26T14:49:42.322302: step 7676, loss 0.00484158, acc 1\n",
      "2017-09-26T14:49:42.556608: step 7677, loss 0.0355275, acc 0.984375\n",
      "2017-09-26T14:49:42.792038: step 7678, loss 0.000141856, acc 1\n",
      "2017-09-26T14:49:43.023300: step 7679, loss 0.000184968, acc 1\n",
      "2017-09-26T14:49:43.263920: step 7680, loss 3.85817e-05, acc 1\n",
      "2017-09-26T14:49:43.493798: step 7681, loss 5.77566e-05, acc 1\n",
      "2017-09-26T14:49:43.725883: step 7682, loss 0.000526276, acc 1\n",
      "2017-09-26T14:49:43.958416: step 7683, loss 0.000913046, acc 1\n",
      "2017-09-26T14:49:44.191227: step 7684, loss 0.000192981, acc 1\n",
      "2017-09-26T14:49:44.424251: step 7685, loss 0.000120413, acc 1\n",
      "2017-09-26T14:49:44.623730: step 7686, loss 2.64381e-05, acc 1\n",
      "2017-09-26T14:49:44.866742: step 7687, loss 2.26393e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:49:45.112130: step 7688, loss 7.79475e-06, acc 1\n",
      "2017-09-26T14:49:45.370135: step 7689, loss 0.000115779, acc 1\n",
      "2017-09-26T14:49:45.611054: step 7690, loss 0.00107635, acc 1\n",
      "2017-09-26T14:49:45.848423: step 7691, loss 2.79358e-05, acc 1\n",
      "2017-09-26T14:49:46.096944: step 7692, loss 1.59152e-05, acc 1\n",
      "2017-09-26T14:49:46.328852: step 7693, loss 1.08851e-05, acc 1\n",
      "2017-09-26T14:49:46.555485: step 7694, loss 0.000671543, acc 1\n",
      "2017-09-26T14:49:46.784753: step 7695, loss 0.000209732, acc 1\n",
      "2017-09-26T14:49:47.010224: step 7696, loss 1.62766e-05, acc 1\n",
      "2017-09-26T14:49:47.241263: step 7697, loss 0.000505726, acc 1\n",
      "2017-09-26T14:49:47.469962: step 7698, loss 0.000700333, acc 1\n",
      "2017-09-26T14:49:47.716748: step 7699, loss 0.000124752, acc 1\n",
      "2017-09-26T14:49:47.962871: step 7700, loss 5.42026e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:49:48.205222: step 7700, loss 0.841487, acc 0.895623\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7700\n",
      "\n",
      "2017-09-26T14:49:48.705696: step 7701, loss 0.000130355, acc 1\n",
      "2017-09-26T14:49:48.939761: step 7702, loss 2.86268e-05, acc 1\n",
      "2017-09-26T14:49:49.172475: step 7703, loss 3.37233e-05, acc 1\n",
      "2017-09-26T14:49:49.404894: step 7704, loss 1.97748e-05, acc 1\n",
      "2017-09-26T14:49:49.640359: step 7705, loss 0.000198224, acc 1\n",
      "2017-09-26T14:49:49.877693: step 7706, loss 0.0182934, acc 0.984375\n",
      "2017-09-26T14:49:50.118699: step 7707, loss 2.95325e-05, acc 1\n",
      "2017-09-26T14:49:50.353619: step 7708, loss 1.34609e-05, acc 1\n",
      "2017-09-26T14:49:50.591393: step 7709, loss 3.46123e-05, acc 1\n",
      "2017-09-26T14:49:50.819475: step 7710, loss 1.53564e-05, acc 1\n",
      "2017-09-26T14:49:51.053033: step 7711, loss 8.32609e-05, acc 1\n",
      "2017-09-26T14:49:51.280758: step 7712, loss 3.2016e-05, acc 1\n",
      "2017-09-26T14:49:51.511457: step 7713, loss 0.000290703, acc 1\n",
      "2017-09-26T14:49:51.738899: step 7714, loss 0.000164399, acc 1\n",
      "2017-09-26T14:49:51.965668: step 7715, loss 0.0214884, acc 0.984375\n",
      "2017-09-26T14:49:52.196562: step 7716, loss 9.76355e-06, acc 1\n",
      "2017-09-26T14:49:52.425418: step 7717, loss 0.000189045, acc 1\n",
      "2017-09-26T14:49:52.660502: step 7718, loss 6.04342e-05, acc 1\n",
      "2017-09-26T14:49:52.888746: step 7719, loss 0.000161351, acc 1\n",
      "2017-09-26T14:49:53.116016: step 7720, loss 5.57733e-05, acc 1\n",
      "2017-09-26T14:49:53.347855: step 7721, loss 3.17648e-05, acc 1\n",
      "2017-09-26T14:49:53.588370: step 7722, loss 0.000111491, acc 1\n",
      "2017-09-26T14:49:53.858407: step 7723, loss 0.000111242, acc 1\n",
      "2017-09-26T14:49:54.099874: step 7724, loss 0.000102302, acc 1\n",
      "2017-09-26T14:49:54.337869: step 7725, loss 7.96776e-05, acc 1\n",
      "2017-09-26T14:49:54.567818: step 7726, loss 7.72414e-05, acc 1\n",
      "2017-09-26T14:49:54.804911: step 7727, loss 7.40077e-05, acc 1\n",
      "2017-09-26T14:49:55.006039: step 7728, loss 0.000318181, acc 1\n",
      "2017-09-26T14:49:55.255548: step 7729, loss 0.000115358, acc 1\n",
      "2017-09-26T14:49:55.542510: step 7730, loss 2.92278e-05, acc 1\n",
      "2017-09-26T14:49:55.796131: step 7731, loss 9.91433e-06, acc 1\n",
      "2017-09-26T14:49:56.048501: step 7732, loss 1.12125e-05, acc 1\n",
      "2017-09-26T14:49:56.286352: step 7733, loss 0.000119527, acc 1\n",
      "2017-09-26T14:49:56.526627: step 7734, loss 0.0670108, acc 0.984375\n",
      "2017-09-26T14:49:56.757227: step 7735, loss 0.000197358, acc 1\n",
      "2017-09-26T14:49:56.995429: step 7736, loss 0.00014719, acc 1\n",
      "2017-09-26T14:49:57.225184: step 7737, loss 0.00011667, acc 1\n",
      "2017-09-26T14:49:57.460053: step 7738, loss 3.106e-05, acc 1\n",
      "2017-09-26T14:49:57.690267: step 7739, loss 0.00476145, acc 1\n",
      "2017-09-26T14:49:57.923306: step 7740, loss 0.000302122, acc 1\n",
      "2017-09-26T14:49:58.154430: step 7741, loss 0.000622395, acc 1\n",
      "2017-09-26T14:49:58.424527: step 7742, loss 0.000128305, acc 1\n",
      "2017-09-26T14:49:58.700786: step 7743, loss 3.81231e-05, acc 1\n",
      "2017-09-26T14:49:58.954356: step 7744, loss 0.000973257, acc 1\n",
      "2017-09-26T14:49:59.210942: step 7745, loss 4.69459e-05, acc 1\n",
      "2017-09-26T14:49:59.448849: step 7746, loss 0.000890982, acc 1\n",
      "2017-09-26T14:49:59.696053: step 7747, loss 7.28884e-05, acc 1\n",
      "2017-09-26T14:49:59.928973: step 7748, loss 0.000195661, acc 1\n",
      "2017-09-26T14:50:00.166745: step 7749, loss 5.80831e-05, acc 1\n",
      "2017-09-26T14:50:00.411323: step 7750, loss 0.00130657, acc 1\n",
      "2017-09-26T14:50:00.662270: step 7751, loss 3.12839e-05, acc 1\n",
      "2017-09-26T14:50:00.918731: step 7752, loss 0.00031781, acc 1\n",
      "2017-09-26T14:50:01.153791: step 7753, loss 0.000344189, acc 1\n",
      "2017-09-26T14:50:01.384935: step 7754, loss 1.20001e-05, acc 1\n",
      "2017-09-26T14:50:01.614531: step 7755, loss 0.000169648, acc 1\n",
      "2017-09-26T14:50:01.843408: step 7756, loss 0.00148539, acc 1\n",
      "2017-09-26T14:50:02.073835: step 7757, loss 0.00021953, acc 1\n",
      "2017-09-26T14:50:02.326260: step 7758, loss 2.07931e-05, acc 1\n",
      "2017-09-26T14:50:02.556817: step 7759, loss 2.77014e-05, acc 1\n",
      "2017-09-26T14:50:02.793480: step 7760, loss 4.70097e-05, acc 1\n",
      "2017-09-26T14:50:03.021043: step 7761, loss 5.81336e-05, acc 1\n",
      "2017-09-26T14:50:03.258938: step 7762, loss 0.000510954, acc 1\n",
      "2017-09-26T14:50:03.490078: step 7763, loss 0.000890921, acc 1\n",
      "2017-09-26T14:50:03.725216: step 7764, loss 0.000267944, acc 1\n",
      "2017-09-26T14:50:03.970939: step 7765, loss 0.000122447, acc 1\n",
      "2017-09-26T14:50:04.201747: step 7766, loss 0.000267541, acc 1\n",
      "2017-09-26T14:50:04.435636: step 7767, loss 0.000118507, acc 1\n",
      "2017-09-26T14:50:04.666959: step 7768, loss 4.31561e-06, acc 1\n",
      "2017-09-26T14:50:04.897474: step 7769, loss 8.32942e-05, acc 1\n",
      "2017-09-26T14:50:05.114774: step 7770, loss 1.35476e-05, acc 1\n",
      "2017-09-26T14:50:05.364820: step 7771, loss 7.01093e-05, acc 1\n",
      "2017-09-26T14:50:05.590722: step 7772, loss 0.00581813, acc 1\n",
      "2017-09-26T14:50:05.826195: step 7773, loss 1.23669e-05, acc 1\n",
      "2017-09-26T14:50:06.052308: step 7774, loss 1.34044e-05, acc 1\n",
      "2017-09-26T14:50:06.279592: step 7775, loss 6.47116e-05, acc 1\n",
      "2017-09-26T14:50:06.506399: step 7776, loss 0.000235387, acc 1\n",
      "2017-09-26T14:50:06.739201: step 7777, loss 4.73705e-05, acc 1\n",
      "2017-09-26T14:50:06.972383: step 7778, loss 0.0233244, acc 0.984375\n",
      "2017-09-26T14:50:07.199474: step 7779, loss 1.67751e-05, acc 1\n",
      "2017-09-26T14:50:07.433255: step 7780, loss 6.28906e-05, acc 1\n",
      "2017-09-26T14:50:07.668887: step 7781, loss 3.44315e-05, acc 1\n",
      "2017-09-26T14:50:07.900451: step 7782, loss 0.000118109, acc 1\n",
      "2017-09-26T14:50:08.131439: step 7783, loss 3.116e-05, acc 1\n",
      "2017-09-26T14:50:08.359669: step 7784, loss 0.000411455, acc 1\n",
      "2017-09-26T14:50:08.593327: step 7785, loss 0.000371495, acc 1\n",
      "2017-09-26T14:50:08.827152: step 7786, loss 0.000169456, acc 1\n",
      "2017-09-26T14:50:09.065786: step 7787, loss 0.000127218, acc 1\n",
      "2017-09-26T14:50:09.309813: step 7788, loss 0.000122953, acc 1\n",
      "2017-09-26T14:50:09.539371: step 7789, loss 0.000189286, acc 1\n",
      "2017-09-26T14:50:09.776016: step 7790, loss 5.95104e-06, acc 1\n",
      "2017-09-26T14:50:10.006096: step 7791, loss 0.000171741, acc 1\n",
      "2017-09-26T14:50:10.240466: step 7792, loss 0.000365132, acc 1\n",
      "2017-09-26T14:50:10.480533: step 7793, loss 0.000447288, acc 1\n",
      "2017-09-26T14:50:10.714516: step 7794, loss 6.03097e-06, acc 1\n",
      "2017-09-26T14:50:10.947548: step 7795, loss 0.000107625, acc 1\n",
      "2017-09-26T14:50:11.225053: step 7796, loss 0.000345796, acc 1\n",
      "2017-09-26T14:50:11.459006: step 7797, loss 8.35596e-05, acc 1\n",
      "2017-09-26T14:50:11.689654: step 7798, loss 2.11403e-05, acc 1\n",
      "2017-09-26T14:50:11.938588: step 7799, loss 5.48335e-05, acc 1\n",
      "2017-09-26T14:50:12.213494: step 7800, loss 0.000154872, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:50:12.471246: step 7800, loss 0.825736, acc 0.892256\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7800\n",
      "\n",
      "2017-09-26T14:50:13.032205: step 7801, loss 0.00011131, acc 1\n",
      "2017-09-26T14:50:13.268248: step 7802, loss 0.000124507, acc 1\n",
      "2017-09-26T14:50:13.497708: step 7803, loss 0.00750719, acc 1\n",
      "2017-09-26T14:50:13.758833: step 7804, loss 0.000130711, acc 1\n",
      "2017-09-26T14:50:14.005598: step 7805, loss 0.00043152, acc 1\n",
      "2017-09-26T14:50:14.238636: step 7806, loss 0.000157825, acc 1\n",
      "2017-09-26T14:50:14.470595: step 7807, loss 2.33613e-05, acc 1\n",
      "2017-09-26T14:50:14.699445: step 7808, loss 2.98832e-05, acc 1\n",
      "2017-09-26T14:50:14.933846: step 7809, loss 5.18944e-05, acc 1\n",
      "2017-09-26T14:50:15.164212: step 7810, loss 0.000128769, acc 1\n",
      "2017-09-26T14:50:15.392937: step 7811, loss 4.76654e-05, acc 1\n",
      "2017-09-26T14:50:15.590594: step 7812, loss 0.000213914, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:50:15.824521: step 7813, loss 0.000195229, acc 1\n",
      "2017-09-26T14:50:16.055661: step 7814, loss 0.00057184, acc 1\n",
      "2017-09-26T14:50:16.292552: step 7815, loss 5.69407e-05, acc 1\n",
      "2017-09-26T14:50:16.522524: step 7816, loss 0.000137777, acc 1\n",
      "2017-09-26T14:50:16.751223: step 7817, loss 0.000374898, acc 1\n",
      "2017-09-26T14:50:16.981493: step 7818, loss 0.000536135, acc 1\n",
      "2017-09-26T14:50:17.212484: step 7819, loss 0.000228488, acc 1\n",
      "2017-09-26T14:50:17.442470: step 7820, loss 0.000488888, acc 1\n",
      "2017-09-26T14:50:17.669124: step 7821, loss 6.13948e-05, acc 1\n",
      "2017-09-26T14:50:17.895679: step 7822, loss 0.000210111, acc 1\n",
      "2017-09-26T14:50:18.122483: step 7823, loss 6.71113e-05, acc 1\n",
      "2017-09-26T14:50:18.361352: step 7824, loss 1.70367e-05, acc 1\n",
      "2017-09-26T14:50:18.589152: step 7825, loss 0.000472969, acc 1\n",
      "2017-09-26T14:50:18.815088: step 7826, loss 4.18683e-05, acc 1\n",
      "2017-09-26T14:50:19.061489: step 7827, loss 0.000512618, acc 1\n",
      "2017-09-26T14:50:19.293738: step 7828, loss 0.000119848, acc 1\n",
      "2017-09-26T14:50:19.524161: step 7829, loss 0.000198706, acc 1\n",
      "2017-09-26T14:50:19.754771: step 7830, loss 1.39122e-05, acc 1\n",
      "2017-09-26T14:50:19.980624: step 7831, loss 6.84979e-05, acc 1\n",
      "2017-09-26T14:50:20.212105: step 7832, loss 0.000124556, acc 1\n",
      "2017-09-26T14:50:20.444090: step 7833, loss 3.79549e-05, acc 1\n",
      "2017-09-26T14:50:20.675500: step 7834, loss 1.94641e-05, acc 1\n",
      "2017-09-26T14:50:20.902939: step 7835, loss 0.000268335, acc 1\n",
      "2017-09-26T14:50:21.133186: step 7836, loss 0.00199813, acc 1\n",
      "2017-09-26T14:50:21.367939: step 7837, loss 0.000108432, acc 1\n",
      "2017-09-26T14:50:21.597114: step 7838, loss 0.000400849, acc 1\n",
      "2017-09-26T14:50:21.825325: step 7839, loss 5.88518e-05, acc 1\n",
      "2017-09-26T14:50:22.052613: step 7840, loss 9.13112e-05, acc 1\n",
      "2017-09-26T14:50:22.282268: step 7841, loss 0.000321288, acc 1\n",
      "2017-09-26T14:50:22.509509: step 7842, loss 0.000424852, acc 1\n",
      "2017-09-26T14:50:22.744620: step 7843, loss 0.00899491, acc 1\n",
      "2017-09-26T14:50:23.003299: step 7844, loss 3.24072e-05, acc 1\n",
      "2017-09-26T14:50:23.267387: step 7845, loss 0.000276136, acc 1\n",
      "2017-09-26T14:50:23.520013: step 7846, loss 2.23173e-05, acc 1\n",
      "2017-09-26T14:50:23.748588: step 7847, loss 0.000105581, acc 1\n",
      "2017-09-26T14:50:23.975728: step 7848, loss 0.000325411, acc 1\n",
      "2017-09-26T14:50:24.211638: step 7849, loss 0.010969, acc 1\n",
      "2017-09-26T14:50:24.448105: step 7850, loss 0.00151023, acc 1\n",
      "2017-09-26T14:50:24.720067: step 7851, loss 0.000192408, acc 1\n",
      "2017-09-26T14:50:24.976935: step 7852, loss 0.0127536, acc 0.984375\n",
      "2017-09-26T14:50:25.215369: step 7853, loss 0.000187055, acc 1\n",
      "2017-09-26T14:50:25.434828: step 7854, loss 9.30394e-05, acc 1\n",
      "2017-09-26T14:50:25.697734: step 7855, loss 5.58934e-06, acc 1\n",
      "2017-09-26T14:50:25.970816: step 7856, loss 0.000256229, acc 1\n",
      "2017-09-26T14:50:26.204141: step 7857, loss 3.22186e-05, acc 1\n",
      "2017-09-26T14:50:26.440434: step 7858, loss 0.0317749, acc 0.984375\n",
      "2017-09-26T14:50:26.712468: step 7859, loss 2.46398e-05, acc 1\n",
      "2017-09-26T14:50:26.970306: step 7860, loss 0.000169859, acc 1\n",
      "2017-09-26T14:50:27.246587: step 7861, loss 0.00152141, acc 1\n",
      "2017-09-26T14:50:27.510417: step 7862, loss 0.000475826, acc 1\n",
      "2017-09-26T14:50:27.781886: step 7863, loss 0.000118147, acc 1\n",
      "2017-09-26T14:50:28.054136: step 7864, loss 4.20867e-05, acc 1\n",
      "2017-09-26T14:50:28.379701: step 7865, loss 2.66525e-05, acc 1\n",
      "2017-09-26T14:50:28.716386: step 7866, loss 1.27241e-05, acc 1\n",
      "2017-09-26T14:50:28.976813: step 7867, loss 4.85696e-05, acc 1\n",
      "2017-09-26T14:50:29.267045: step 7868, loss 8.02255e-05, acc 1\n",
      "2017-09-26T14:50:29.597321: step 7869, loss 0.00104284, acc 1\n",
      "2017-09-26T14:50:30.011370: step 7870, loss 0.000127044, acc 1\n",
      "2017-09-26T14:50:30.321869: step 7871, loss 0.000665902, acc 1\n",
      "2017-09-26T14:50:30.608884: step 7872, loss 3.45068e-05, acc 1\n",
      "2017-09-26T14:50:30.952865: step 7873, loss 5.05883e-06, acc 1\n",
      "2017-09-26T14:50:31.272366: step 7874, loss 0.000206532, acc 1\n",
      "2017-09-26T14:50:31.570775: step 7875, loss 0.000829884, acc 1\n",
      "2017-09-26T14:50:31.900487: step 7876, loss 0.0412541, acc 0.984375\n",
      "2017-09-26T14:50:32.155943: step 7877, loss 0.000389967, acc 1\n",
      "2017-09-26T14:50:32.420763: step 7878, loss 0.000109526, acc 1\n",
      "2017-09-26T14:50:32.721799: step 7879, loss 0.000167505, acc 1\n",
      "2017-09-26T14:50:33.029106: step 7880, loss 3.03625e-05, acc 1\n",
      "2017-09-26T14:50:33.358851: step 7881, loss 1.40043e-05, acc 1\n",
      "2017-09-26T14:50:33.632917: step 7882, loss 0.000114515, acc 1\n",
      "2017-09-26T14:50:33.901402: step 7883, loss 2.61538e-05, acc 1\n",
      "2017-09-26T14:50:34.190917: step 7884, loss 8.53383e-06, acc 1\n",
      "2017-09-26T14:50:34.453515: step 7885, loss 0.000178046, acc 1\n",
      "2017-09-26T14:50:34.755876: step 7886, loss 0.000226052, acc 1\n",
      "2017-09-26T14:50:35.023409: step 7887, loss 3.87308e-05, acc 1\n",
      "2017-09-26T14:50:35.280961: step 7888, loss 8.53438e-05, acc 1\n",
      "2017-09-26T14:50:35.537485: step 7889, loss 0.000148204, acc 1\n",
      "2017-09-26T14:50:35.820063: step 7890, loss 0.000684658, acc 1\n",
      "2017-09-26T14:50:36.111383: step 7891, loss 0.00011031, acc 1\n",
      "2017-09-26T14:50:36.385464: step 7892, loss 1.57303e-05, acc 1\n",
      "2017-09-26T14:50:36.649913: step 7893, loss 1.01449e-05, acc 1\n",
      "2017-09-26T14:50:36.893187: step 7894, loss 6.84485e-06, acc 1\n",
      "2017-09-26T14:50:37.141145: step 7895, loss 6.43522e-06, acc 1\n",
      "2017-09-26T14:50:37.383142: step 7896, loss 0.00366073, acc 1\n",
      "2017-09-26T14:50:37.633366: step 7897, loss 2.02263e-05, acc 1\n",
      "2017-09-26T14:50:37.873350: step 7898, loss 3.69117e-05, acc 1\n",
      "2017-09-26T14:50:38.139872: step 7899, loss 0.0115747, acc 0.984375\n",
      "2017-09-26T14:50:38.410658: step 7900, loss 6.94764e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:50:38.691190: step 7900, loss 0.85773, acc 0.888889\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-7900\n",
      "\n",
      "2017-09-26T14:50:39.356854: step 7901, loss 0.0280765, acc 0.984375\n",
      "2017-09-26T14:50:39.662999: step 7902, loss 5.46309e-05, acc 1\n",
      "2017-09-26T14:50:39.975335: step 7903, loss 2.48263e-05, acc 1\n",
      "2017-09-26T14:50:40.289908: step 7904, loss 9.90519e-05, acc 1\n",
      "2017-09-26T14:50:40.586351: step 7905, loss 4.98231e-06, acc 1\n",
      "2017-09-26T14:50:40.834135: step 7906, loss 0.000152021, acc 1\n",
      "2017-09-26T14:50:41.078155: step 7907, loss 0.000108541, acc 1\n",
      "2017-09-26T14:50:41.314402: step 7908, loss 0.000110876, acc 1\n",
      "2017-09-26T14:50:41.547605: step 7909, loss 8.44461e-05, acc 1\n",
      "2017-09-26T14:50:41.776304: step 7910, loss 0.000466627, acc 1\n",
      "2017-09-26T14:50:42.009228: step 7911, loss 0.00320601, acc 1\n",
      "2017-09-26T14:50:42.255131: step 7912, loss 0.0123823, acc 0.984375\n",
      "2017-09-26T14:50:42.485732: step 7913, loss 0.000479002, acc 1\n",
      "2017-09-26T14:50:42.738770: step 7914, loss 8.12228e-05, acc 1\n",
      "2017-09-26T14:50:42.979565: step 7915, loss 0.000109914, acc 1\n",
      "2017-09-26T14:50:43.215434: step 7916, loss 2.33633e-05, acc 1\n",
      "2017-09-26T14:50:43.447195: step 7917, loss 1.7341e-06, acc 1\n",
      "2017-09-26T14:50:43.684439: step 7918, loss 0.00209876, acc 1\n",
      "2017-09-26T14:50:43.912721: step 7919, loss 0.000821621, acc 1\n",
      "2017-09-26T14:50:44.145496: step 7920, loss 0.000209796, acc 1\n",
      "2017-09-26T14:50:44.388217: step 7921, loss 8.34578e-06, acc 1\n",
      "2017-09-26T14:50:44.618741: step 7922, loss 6.64832e-05, acc 1\n",
      "2017-09-26T14:50:44.851786: step 7923, loss 1.62934e-05, acc 1\n",
      "2017-09-26T14:50:45.088685: step 7924, loss 0.000489225, acc 1\n",
      "2017-09-26T14:50:45.324171: step 7925, loss 2.51694e-05, acc 1\n",
      "2017-09-26T14:50:45.557996: step 7926, loss 3.77546e-06, acc 1\n",
      "2017-09-26T14:50:45.802829: step 7927, loss 0.00227994, acc 1\n",
      "2017-09-26T14:50:46.032238: step 7928, loss 2.34566e-05, acc 1\n",
      "2017-09-26T14:50:46.272811: step 7929, loss 0.00606822, acc 1\n",
      "2017-09-26T14:50:46.512182: step 7930, loss 5.53495e-05, acc 1\n",
      "2017-09-26T14:50:46.756802: step 7931, loss 0.000107883, acc 1\n",
      "2017-09-26T14:50:47.001807: step 7932, loss 0.000714582, acc 1\n",
      "2017-09-26T14:50:47.244853: step 7933, loss 1.70284e-05, acc 1\n",
      "2017-09-26T14:50:47.481937: step 7934, loss 3.50144e-05, acc 1\n",
      "2017-09-26T14:50:47.728651: step 7935, loss 2.30514e-05, acc 1\n",
      "2017-09-26T14:50:47.959126: step 7936, loss 2.29961e-05, acc 1\n",
      "2017-09-26T14:50:48.192161: step 7937, loss 6.49099e-06, acc 1\n",
      "2017-09-26T14:50:48.394895: step 7938, loss 1.8816e-05, acc 1\n",
      "2017-09-26T14:50:48.630046: step 7939, loss 1.60464e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:50:48.859835: step 7940, loss 2.59774e-05, acc 1\n",
      "2017-09-26T14:50:49.094190: step 7941, loss 0.000966448, acc 1\n",
      "2017-09-26T14:50:49.339731: step 7942, loss 0.000151189, acc 1\n",
      "2017-09-26T14:50:49.570527: step 7943, loss 0.000701426, acc 1\n",
      "2017-09-26T14:50:49.804945: step 7944, loss 8.39062e-06, acc 1\n",
      "2017-09-26T14:50:50.040258: step 7945, loss 4.40032e-05, acc 1\n",
      "2017-09-26T14:50:50.286150: step 7946, loss 1.42603e-05, acc 1\n",
      "2017-09-26T14:50:50.562964: step 7947, loss 9.08657e-05, acc 1\n",
      "2017-09-26T14:50:50.796875: step 7948, loss 6.29133e-05, acc 1\n",
      "2017-09-26T14:50:51.028574: step 7949, loss 0.000161557, acc 1\n",
      "2017-09-26T14:50:51.262010: step 7950, loss 0.000685359, acc 1\n",
      "2017-09-26T14:50:51.491591: step 7951, loss 5.16863e-06, acc 1\n",
      "2017-09-26T14:50:51.730819: step 7952, loss 5.82805e-05, acc 1\n",
      "2017-09-26T14:50:51.965922: step 7953, loss 8.81247e-05, acc 1\n",
      "2017-09-26T14:50:52.207060: step 7954, loss 2.90148e-05, acc 1\n",
      "2017-09-26T14:50:52.436409: step 7955, loss 4.46645e-06, acc 1\n",
      "2017-09-26T14:50:52.668050: step 7956, loss 0.000106515, acc 1\n",
      "2017-09-26T14:50:52.900536: step 7957, loss 0.000109919, acc 1\n",
      "2017-09-26T14:50:53.129931: step 7958, loss 3.86964e-05, acc 1\n",
      "2017-09-26T14:50:53.372883: step 7959, loss 8.27493e-05, acc 1\n",
      "2017-09-26T14:50:53.606757: step 7960, loss 0.00029351, acc 1\n",
      "2017-09-26T14:50:53.855274: step 7961, loss 0.000202385, acc 1\n",
      "2017-09-26T14:50:54.085538: step 7962, loss 2.29513e-05, acc 1\n",
      "2017-09-26T14:50:54.317656: step 7963, loss 4.67327e-05, acc 1\n",
      "2017-09-26T14:50:54.547811: step 7964, loss 1.1449e-05, acc 1\n",
      "2017-09-26T14:50:54.776274: step 7965, loss 8.39797e-05, acc 1\n",
      "2017-09-26T14:50:55.008583: step 7966, loss 0.00989144, acc 1\n",
      "2017-09-26T14:50:55.254294: step 7967, loss 7.45575e-05, acc 1\n",
      "2017-09-26T14:50:55.483052: step 7968, loss 0.0543413, acc 0.984375\n",
      "2017-09-26T14:50:55.727617: step 7969, loss 2.54451e-05, acc 1\n",
      "2017-09-26T14:50:56.005503: step 7970, loss 1.92648e-05, acc 1\n",
      "2017-09-26T14:50:56.306200: step 7971, loss 1.50341e-05, acc 1\n",
      "2017-09-26T14:50:56.539866: step 7972, loss 3.85148e-05, acc 1\n",
      "2017-09-26T14:50:56.775396: step 7973, loss 0.000135385, acc 1\n",
      "2017-09-26T14:50:57.022967: step 7974, loss 2.33108e-05, acc 1\n",
      "2017-09-26T14:50:57.279058: step 7975, loss 7.15921e-05, acc 1\n",
      "2017-09-26T14:50:57.510794: step 7976, loss 4.79185e-05, acc 1\n",
      "2017-09-26T14:50:57.766107: step 7977, loss 0.00266876, acc 1\n",
      "2017-09-26T14:50:58.012901: step 7978, loss 0.000495733, acc 1\n",
      "2017-09-26T14:50:58.250345: step 7979, loss 3.27355e-05, acc 1\n",
      "2017-09-26T14:50:58.451358: step 7980, loss 1.26945e-05, acc 1\n",
      "2017-09-26T14:50:58.686576: step 7981, loss 4.74619e-05, acc 1\n",
      "2017-09-26T14:50:58.921133: step 7982, loss 0.000305199, acc 1\n",
      "2017-09-26T14:50:59.155031: step 7983, loss 9.12989e-05, acc 1\n",
      "2017-09-26T14:50:59.394330: step 7984, loss 0.000121089, acc 1\n",
      "2017-09-26T14:50:59.625330: step 7985, loss 1.61178e-05, acc 1\n",
      "2017-09-26T14:50:59.859377: step 7986, loss 1.14633e-05, acc 1\n",
      "2017-09-26T14:51:00.095428: step 7987, loss 0.000114893, acc 1\n",
      "2017-09-26T14:51:00.335965: step 7988, loss 2.34061e-05, acc 1\n",
      "2017-09-26T14:51:00.569399: step 7989, loss 7.92703e-06, acc 1\n",
      "2017-09-26T14:51:00.804269: step 7990, loss 1.57903e-05, acc 1\n",
      "2017-09-26T14:51:01.042953: step 7991, loss 0.000124048, acc 1\n",
      "2017-09-26T14:51:01.280145: step 7992, loss 8.98527e-05, acc 1\n",
      "2017-09-26T14:51:01.508874: step 7993, loss 0.000140199, acc 1\n",
      "2017-09-26T14:51:01.742346: step 7994, loss 2.45393e-05, acc 1\n",
      "2017-09-26T14:51:01.971051: step 7995, loss 2.84524e-05, acc 1\n",
      "2017-09-26T14:51:02.200096: step 7996, loss 8.26726e-05, acc 1\n",
      "2017-09-26T14:51:02.434981: step 7997, loss 0.00143581, acc 1\n",
      "2017-09-26T14:51:02.665884: step 7998, loss 0.000284104, acc 1\n",
      "2017-09-26T14:51:02.894742: step 7999, loss 5.91118e-05, acc 1\n",
      "2017-09-26T14:51:03.125935: step 8000, loss 0.0532141, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:51:03.360718: step 8000, loss 0.820414, acc 0.885522\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-8000\n",
      "\n",
      "2017-09-26T14:51:03.854947: step 8001, loss 3.8475e-05, acc 1\n",
      "2017-09-26T14:51:04.087946: step 8002, loss 3.01047e-05, acc 1\n",
      "2017-09-26T14:51:04.321855: step 8003, loss 2.63397e-05, acc 1\n",
      "2017-09-26T14:51:04.556497: step 8004, loss 0.000229615, acc 1\n",
      "2017-09-26T14:51:04.783887: step 8005, loss 2.52891e-05, acc 1\n",
      "2017-09-26T14:51:05.015867: step 8006, loss 0.0282771, acc 0.984375\n",
      "2017-09-26T14:51:05.257203: step 8007, loss 5.79052e-05, acc 1\n",
      "2017-09-26T14:51:05.492047: step 8008, loss 0.00033247, acc 1\n",
      "2017-09-26T14:51:05.723677: step 8009, loss 3.44469e-05, acc 1\n",
      "2017-09-26T14:51:05.955874: step 8010, loss 9.68746e-05, acc 1\n",
      "2017-09-26T14:51:06.185987: step 8011, loss 5.24419e-05, acc 1\n",
      "2017-09-26T14:51:06.415308: step 8012, loss 6.04389e-05, acc 1\n",
      "2017-09-26T14:51:06.647138: step 8013, loss 7.50791e-06, acc 1\n",
      "2017-09-26T14:51:06.881780: step 8014, loss 2.27125e-05, acc 1\n",
      "2017-09-26T14:51:07.109627: step 8015, loss 0.000195138, acc 1\n",
      "2017-09-26T14:51:07.347707: step 8016, loss 8.21907e-06, acc 1\n",
      "2017-09-26T14:51:07.579732: step 8017, loss 3.66346e-05, acc 1\n",
      "2017-09-26T14:51:07.811398: step 8018, loss 2.80373e-05, acc 1\n",
      "2017-09-26T14:51:08.040972: step 8019, loss 3.00656e-05, acc 1\n",
      "2017-09-26T14:51:08.273989: step 8020, loss 4.42284e-05, acc 1\n",
      "2017-09-26T14:51:08.504272: step 8021, loss 2.7982e-05, acc 1\n",
      "2017-09-26T14:51:08.709694: step 8022, loss 5.11674e-06, acc 1\n",
      "2017-09-26T14:51:08.943448: step 8023, loss 3.54318e-05, acc 1\n",
      "2017-09-26T14:51:09.206097: step 8024, loss 0.000277014, acc 1\n",
      "2017-09-26T14:51:09.439134: step 8025, loss 0.00011447, acc 1\n",
      "2017-09-26T14:51:09.668997: step 8026, loss 4.75567e-05, acc 1\n",
      "2017-09-26T14:51:09.898977: step 8027, loss 1.45231e-05, acc 1\n",
      "2017-09-26T14:51:10.130584: step 8028, loss 9.71642e-06, acc 1\n",
      "2017-09-26T14:51:10.370341: step 8029, loss 2.43583e-05, acc 1\n",
      "2017-09-26T14:51:10.607062: step 8030, loss 0.000260878, acc 1\n",
      "2017-09-26T14:51:10.841949: step 8031, loss 3.85662e-05, acc 1\n",
      "2017-09-26T14:51:11.071288: step 8032, loss 1.63517e-05, acc 1\n",
      "2017-09-26T14:51:11.312641: step 8033, loss 0.000235399, acc 1\n",
      "2017-09-26T14:51:11.543965: step 8034, loss 1.89961e-05, acc 1\n",
      "2017-09-26T14:51:11.775626: step 8035, loss 2.81996e-05, acc 1\n",
      "2017-09-26T14:51:12.009087: step 8036, loss 6.89366e-05, acc 1\n",
      "2017-09-26T14:51:12.242652: step 8037, loss 0.000291409, acc 1\n",
      "2017-09-26T14:51:12.472449: step 8038, loss 0.000127479, acc 1\n",
      "2017-09-26T14:51:12.704085: step 8039, loss 3.0602e-05, acc 1\n",
      "2017-09-26T14:51:12.993832: step 8040, loss 0.000301573, acc 1\n",
      "2017-09-26T14:51:13.231824: step 8041, loss 3.98803e-05, acc 1\n",
      "2017-09-26T14:51:13.468603: step 8042, loss 0.00116702, acc 1\n",
      "2017-09-26T14:51:13.719110: step 8043, loss 3.96729e-06, acc 1\n",
      "2017-09-26T14:51:13.989966: step 8044, loss 2.13272e-05, acc 1\n",
      "2017-09-26T14:51:14.221415: step 8045, loss 0.00138431, acc 1\n",
      "2017-09-26T14:51:14.450939: step 8046, loss 0.000111569, acc 1\n",
      "2017-09-26T14:51:14.712294: step 8047, loss 0.000249508, acc 1\n",
      "2017-09-26T14:51:14.968085: step 8048, loss 0.000412982, acc 1\n",
      "2017-09-26T14:51:15.199305: step 8049, loss 1.59356e-05, acc 1\n",
      "2017-09-26T14:51:15.427673: step 8050, loss 1.24865e-05, acc 1\n",
      "2017-09-26T14:51:15.665179: step 8051, loss 3.42796e-05, acc 1\n",
      "2017-09-26T14:51:15.897420: step 8052, loss 0.000230701, acc 1\n",
      "2017-09-26T14:51:16.129215: step 8053, loss 5.04954e-05, acc 1\n",
      "2017-09-26T14:51:16.370520: step 8054, loss 3.89386e-05, acc 1\n",
      "2017-09-26T14:51:16.598791: step 8055, loss 1.10169e-05, acc 1\n",
      "2017-09-26T14:51:16.827431: step 8056, loss 0.0384379, acc 0.984375\n",
      "2017-09-26T14:51:17.060243: step 8057, loss 0.00023994, acc 1\n",
      "2017-09-26T14:51:17.290908: step 8058, loss 0.00021382, acc 1\n",
      "2017-09-26T14:51:17.523203: step 8059, loss 0.000690791, acc 1\n",
      "2017-09-26T14:51:17.753436: step 8060, loss 0.000102032, acc 1\n",
      "2017-09-26T14:51:17.985845: step 8061, loss 0.00349275, acc 1\n",
      "2017-09-26T14:51:18.243029: step 8062, loss 0.0104438, acc 1\n",
      "2017-09-26T14:51:18.471377: step 8063, loss 5.78332e-06, acc 1\n",
      "2017-09-26T14:51:18.673500: step 8064, loss 2.86999e-05, acc 1\n",
      "2017-09-26T14:51:18.912128: step 8065, loss 7.51216e-05, acc 1\n",
      "2017-09-26T14:51:19.140672: step 8066, loss 7.31835e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:51:19.376014: step 8067, loss 0.000137602, acc 1\n",
      "2017-09-26T14:51:19.610774: step 8068, loss 9.10409e-06, acc 1\n",
      "2017-09-26T14:51:19.846564: step 8069, loss 0.0171553, acc 0.984375\n",
      "2017-09-26T14:51:20.083978: step 8070, loss 0.000482757, acc 1\n",
      "2017-09-26T14:51:20.332512: step 8071, loss 0.000378739, acc 1\n",
      "2017-09-26T14:51:20.565105: step 8072, loss 0.000357254, acc 1\n",
      "2017-09-26T14:51:20.796592: step 8073, loss 0.00647089, acc 1\n",
      "2017-09-26T14:51:21.032080: step 8074, loss 0.000216868, acc 1\n",
      "2017-09-26T14:51:21.262476: step 8075, loss 0.000542721, acc 1\n",
      "2017-09-26T14:51:21.499933: step 8076, loss 0.00238117, acc 1\n",
      "2017-09-26T14:51:21.730460: step 8077, loss 0.000115655, acc 1\n",
      "2017-09-26T14:51:21.957009: step 8078, loss 5.54133e-05, acc 1\n",
      "2017-09-26T14:51:22.186425: step 8079, loss 0.0337159, acc 0.984375\n",
      "2017-09-26T14:51:22.421919: step 8080, loss 0.000175232, acc 1\n",
      "2017-09-26T14:51:22.650459: step 8081, loss 2.79648e-05, acc 1\n",
      "2017-09-26T14:51:22.882487: step 8082, loss 6.53586e-05, acc 1\n",
      "2017-09-26T14:51:23.115619: step 8083, loss 5.66667e-05, acc 1\n",
      "2017-09-26T14:51:23.347923: step 8084, loss 0.0002443, acc 1\n",
      "2017-09-26T14:51:23.574657: step 8085, loss 0.00101081, acc 1\n",
      "2017-09-26T14:51:23.801696: step 8086, loss 0.000218273, acc 1\n",
      "2017-09-26T14:51:24.031277: step 8087, loss 0.000223784, acc 1\n",
      "2017-09-26T14:51:24.265839: step 8088, loss 0.000170826, acc 1\n",
      "2017-09-26T14:51:24.494601: step 8089, loss 0.00753913, acc 1\n",
      "2017-09-26T14:51:24.724377: step 8090, loss 1.98199e-05, acc 1\n",
      "2017-09-26T14:51:24.951762: step 8091, loss 0.0550103, acc 0.984375\n",
      "2017-09-26T14:51:25.180832: step 8092, loss 3.46321e-05, acc 1\n",
      "2017-09-26T14:51:25.409167: step 8093, loss 0.000135026, acc 1\n",
      "2017-09-26T14:51:25.639088: step 8094, loss 0.00618386, acc 1\n",
      "2017-09-26T14:51:25.866499: step 8095, loss 1.98008e-05, acc 1\n",
      "2017-09-26T14:51:26.093984: step 8096, loss 0.000180619, acc 1\n",
      "2017-09-26T14:51:26.327011: step 8097, loss 0.00650553, acc 1\n",
      "2017-09-26T14:51:26.555643: step 8098, loss 0.000381578, acc 1\n",
      "2017-09-26T14:51:26.785189: step 8099, loss 1.63575e-05, acc 1\n",
      "2017-09-26T14:51:27.014855: step 8100, loss 0.00331838, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:51:27.241302: step 8100, loss 0.785145, acc 0.878788\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-8100\n",
      "\n",
      "2017-09-26T14:51:27.726077: step 8101, loss 8.02667e-05, acc 1\n",
      "2017-09-26T14:51:27.954593: step 8102, loss 0.000102278, acc 1\n",
      "2017-09-26T14:51:28.183387: step 8103, loss 5.26898e-05, acc 1\n",
      "2017-09-26T14:51:28.410801: step 8104, loss 0.000123083, acc 1\n",
      "2017-09-26T14:51:28.636938: step 8105, loss 1.87471e-05, acc 1\n",
      "2017-09-26T14:51:28.840963: step 8106, loss 9.16705e-06, acc 1\n",
      "2017-09-26T14:51:29.070900: step 8107, loss 4.44795e-05, acc 1\n",
      "2017-09-26T14:51:29.307790: step 8108, loss 0.00789735, acc 1\n",
      "2017-09-26T14:51:29.537681: step 8109, loss 2.58535e-05, acc 1\n",
      "2017-09-26T14:51:29.774774: step 8110, loss 0.0144049, acc 0.984375\n",
      "2017-09-26T14:51:30.006345: step 8111, loss 0.000119656, acc 1\n",
      "2017-09-26T14:51:30.250211: step 8112, loss 6.79509e-05, acc 1\n",
      "2017-09-26T14:51:30.480581: step 8113, loss 4.62459e-05, acc 1\n",
      "2017-09-26T14:51:30.734236: step 8114, loss 6.44711e-05, acc 1\n",
      "2017-09-26T14:51:30.973045: step 8115, loss 0.000222574, acc 1\n",
      "2017-09-26T14:51:31.228555: step 8116, loss 5.43375e-05, acc 1\n",
      "2017-09-26T14:51:31.467084: step 8117, loss 2.74855e-05, acc 1\n",
      "2017-09-26T14:51:31.710048: step 8118, loss 3.58074e-05, acc 1\n",
      "2017-09-26T14:51:31.949727: step 8119, loss 6.79505e-05, acc 1\n",
      "2017-09-26T14:51:32.216272: step 8120, loss 4.21978e-05, acc 1\n",
      "2017-09-26T14:51:32.473329: step 8121, loss 0.00039729, acc 1\n",
      "2017-09-26T14:51:32.710828: step 8122, loss 3.02433e-05, acc 1\n",
      "2017-09-26T14:51:32.944219: step 8123, loss 8.61106e-05, acc 1\n",
      "2017-09-26T14:51:33.179832: step 8124, loss 2.87707e-05, acc 1\n",
      "2017-09-26T14:51:33.414215: step 8125, loss 0.000793642, acc 1\n",
      "2017-09-26T14:51:33.650500: step 8126, loss 0.00130858, acc 1\n",
      "2017-09-26T14:51:33.886108: step 8127, loss 0.000316713, acc 1\n",
      "2017-09-26T14:51:34.145127: step 8128, loss 4.69366e-05, acc 1\n",
      "2017-09-26T14:51:34.395107: step 8129, loss 0.000141963, acc 1\n",
      "2017-09-26T14:51:34.635077: step 8130, loss 1.9152e-05, acc 1\n",
      "2017-09-26T14:51:34.870072: step 8131, loss 1.23473e-05, acc 1\n",
      "2017-09-26T14:51:35.100368: step 8132, loss 0.000346714, acc 1\n",
      "2017-09-26T14:51:35.335041: step 8133, loss 0.00519872, acc 1\n",
      "2017-09-26T14:51:35.568301: step 8134, loss 0.000177724, acc 1\n",
      "2017-09-26T14:51:35.814025: step 8135, loss 0.000207046, acc 1\n",
      "2017-09-26T14:51:36.047276: step 8136, loss 1.29071e-05, acc 1\n",
      "2017-09-26T14:51:36.293686: step 8137, loss 7.43835e-05, acc 1\n",
      "2017-09-26T14:51:36.524451: step 8138, loss 7.11408e-05, acc 1\n",
      "2017-09-26T14:51:36.772709: step 8139, loss 1.93637e-05, acc 1\n",
      "2017-09-26T14:51:37.011836: step 8140, loss 0.000841484, acc 1\n",
      "2017-09-26T14:51:37.255635: step 8141, loss 1.66296e-05, acc 1\n",
      "2017-09-26T14:51:37.484049: step 8142, loss 1.39202e-05, acc 1\n",
      "2017-09-26T14:51:37.732631: step 8143, loss 0.000300487, acc 1\n",
      "2017-09-26T14:51:37.965456: step 8144, loss 1.34125e-05, acc 1\n",
      "2017-09-26T14:51:38.201520: step 8145, loss 0.00125449, acc 1\n",
      "2017-09-26T14:51:38.477494: step 8146, loss 3.15065e-05, acc 1\n",
      "2017-09-26T14:51:38.759843: step 8147, loss 0.000134287, acc 1\n",
      "2017-09-26T14:51:39.011353: step 8148, loss 0.000101857, acc 1\n",
      "2017-09-26T14:51:39.280810: step 8149, loss 0.000163668, acc 1\n",
      "2017-09-26T14:51:39.525240: step 8150, loss 4.04459e-05, acc 1\n",
      "2017-09-26T14:51:39.797773: step 8151, loss 1.96018e-05, acc 1\n",
      "2017-09-26T14:51:40.053169: step 8152, loss 3.68664e-05, acc 1\n",
      "2017-09-26T14:51:40.342952: step 8153, loss 0.000121016, acc 1\n",
      "2017-09-26T14:51:40.594652: step 8154, loss 0.0105819, acc 1\n",
      "2017-09-26T14:51:40.875835: step 8155, loss 4.86881e-06, acc 1\n",
      "2017-09-26T14:51:41.167335: step 8156, loss 0.000102838, acc 1\n",
      "2017-09-26T14:51:41.462799: step 8157, loss 0.0490664, acc 0.984375\n",
      "2017-09-26T14:51:41.741367: step 8158, loss 0.0291294, acc 0.984375\n",
      "2017-09-26T14:51:42.017404: step 8159, loss 3.75818e-05, acc 1\n",
      "2017-09-26T14:51:42.278672: step 8160, loss 1.20964e-05, acc 1\n",
      "2017-09-26T14:51:42.577468: step 8161, loss 2.66536e-05, acc 1\n",
      "2017-09-26T14:51:42.878155: step 8162, loss 3.65377e-05, acc 1\n",
      "2017-09-26T14:51:43.164682: step 8163, loss 0.000147293, acc 1\n",
      "2017-09-26T14:51:43.458774: step 8164, loss 7.28644e-06, acc 1\n",
      "2017-09-26T14:51:43.765224: step 8165, loss 9.22179e-05, acc 1\n",
      "2017-09-26T14:51:44.093928: step 8166, loss 5.23692e-05, acc 1\n",
      "2017-09-26T14:51:44.375933: step 8167, loss 0.000143186, acc 1\n",
      "2017-09-26T14:51:44.688306: step 8168, loss 1.70299e-05, acc 1\n",
      "2017-09-26T14:51:44.949928: step 8169, loss 2.59354e-05, acc 1\n",
      "2017-09-26T14:51:45.186852: step 8170, loss 3.57744e-05, acc 1\n",
      "2017-09-26T14:51:45.456563: step 8171, loss 9.81468e-05, acc 1\n",
      "2017-09-26T14:51:45.701648: step 8172, loss 0.000308073, acc 1\n",
      "2017-09-26T14:51:45.953130: step 8173, loss 0.000366513, acc 1\n",
      "2017-09-26T14:51:46.216393: step 8174, loss 2.26786e-05, acc 1\n",
      "2017-09-26T14:51:46.451332: step 8175, loss 0.000279722, acc 1\n",
      "2017-09-26T14:51:46.685443: step 8176, loss 0.00543025, acc 1\n",
      "2017-09-26T14:51:46.919206: step 8177, loss 7.98731e-05, acc 1\n",
      "2017-09-26T14:51:47.153814: step 8178, loss 5.22082e-05, acc 1\n",
      "2017-09-26T14:51:47.402209: step 8179, loss 1.00597e-05, acc 1\n",
      "2017-09-26T14:51:47.658593: step 8180, loss 9.46556e-05, acc 1\n",
      "2017-09-26T14:51:47.903246: step 8181, loss 5.27016e-05, acc 1\n",
      "2017-09-26T14:51:48.155149: step 8182, loss 0.000276851, acc 1\n",
      "2017-09-26T14:51:48.398939: step 8183, loss 3.58335e-05, acc 1\n",
      "2017-09-26T14:51:48.666515: step 8184, loss 0.000272705, acc 1\n",
      "2017-09-26T14:51:48.931954: step 8185, loss 6.64529e-05, acc 1\n",
      "2017-09-26T14:51:49.179789: step 8186, loss 4.39037e-05, acc 1\n",
      "2017-09-26T14:51:49.438092: step 8187, loss 2.33013e-06, acc 1\n",
      "2017-09-26T14:51:49.681053: step 8188, loss 6.65101e-06, acc 1\n",
      "2017-09-26T14:51:49.994449: step 8189, loss 5.84876e-05, acc 1\n",
      "2017-09-26T14:51:50.262925: step 8190, loss 0.000361226, acc 1\n",
      "2017-09-26T14:51:50.568914: step 8191, loss 0.0012362, acc 1\n",
      "2017-09-26T14:51:50.863957: step 8192, loss 0.000377413, acc 1\n",
      "2017-09-26T14:51:51.219508: step 8193, loss 3.37846e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-26T14:51:51.523824: step 8194, loss 4.25458e-05, acc 1\n",
      "2017-09-26T14:51:51.815527: step 8195, loss 0.000222252, acc 1\n",
      "2017-09-26T14:51:52.047526: step 8196, loss 0.00807807, acc 1\n",
      "2017-09-26T14:51:52.298033: step 8197, loss 2.67413e-05, acc 1\n",
      "2017-09-26T14:51:52.614798: step 8198, loss 3.12484e-05, acc 1\n",
      "2017-09-26T14:51:52.864887: step 8199, loss 5.45415e-05, acc 1\n",
      "2017-09-26T14:51:53.104373: step 8200, loss 5.36257e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-09-26T14:51:53.330420: step 8200, loss 0.894182, acc 0.875421\n",
      "\n",
      "Saved model checkpoint to /home/eduardo/UFRGS/Question Answering/qa-chave/runs/1506446087/checkpoints/model-8200\n",
      "\n",
      "2017-09-26T14:51:53.813601: step 8201, loss 2.43029e-05, acc 1\n",
      "2017-09-26T14:51:54.043694: step 8202, loss 6.74424e-05, acc 1\n",
      "2017-09-26T14:51:54.298030: step 8203, loss 0.0140064, acc 0.984375\n",
      "2017-09-26T14:51:54.553632: step 8204, loss 3.08683e-05, acc 1\n",
      "2017-09-26T14:51:54.845902: step 8205, loss 0.000396007, acc 1\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "import os\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
